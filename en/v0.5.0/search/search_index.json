{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"","title":"Home"},{"location":"basics/01_quickstart.html","text":"QUICKSTART \u00b6 This section will take the training process of MNIST as an example to briefly show how OneFlow can be used to accomplish common tasks in deep learning. Refer to the links in each section to the presentation on each subtask. Let\u2019s start by importing the necessary libraries: import oneflow as flow import oneflow.nn as nn import oneflow.utils.vision.transforms as transforms BATCH_SIZE = 128 Working with Data \u00b6 OneFlow has two primitives to work with data, which are Dataset and Dataloader. The oneflow.utils.vision.datasets module contains a number of real data sets (such as MNIST, CIFAR 10, FashionMNIST). We can use oneflow.utils.vision.datasets.MNIST to get the training set and test set data of MNIST. mnist_train = flow . utils . vision . datasets . MNIST ( root = \"data\" , train = True , transform = transforms . ToTensor (), download = True , source_url = \"https://oneflow-public.oss-cn-beijing.aliyuncs.com/datasets/mnist/MNIST/\" , ) mnist_test = flow . utils . vision . datasets . MNIST ( root = \"data\" , train = False , transform = transforms . ToTensor (), download = True , source_url = \"https://oneflow-public.oss-cn-beijing.aliyuncs.com/datasets/mnist/MNIST/\" , ) Out: Downloading https://oneflow-public.oss-cn-beijing.aliyuncs.com/datasets/mnist/MNIST/train-images-idx3-ubyte.gz Downloading https://oneflow-public.oss-cn-beijing.aliyuncs.com/datasets/mnist/MNIST/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz 9913344it [00:00, 36066177.85it/s] Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw ... The data will be downloaded and extracted to ./data directory. The oneflow.utils.data.DataLoader wraps an iterable around the dataset . train_iter = flow . utils . data . DataLoader ( mnist_train , BATCH_SIZE , shuffle = True ) test_iter = flow . utils . data . DataLoader ( mnist_test , BATCH_SIZE , shuffle = False ) for x , y in train_iter : print ( \"x.shape:\" , x . shape ) print ( \"y.shape:\" , y . shape ) break Out: x.shape: flow.Size([128, 1, 28, 28]) y.shape: flow.Size([128]) Dataset and Dataloader Building Networks \u00b6 To define a neural network in OneFlow, we create a class that inherits from nn.Module . We define the layers of the network in the __init__ function and specify how data will pass through the network in the forward function. class NeuralNetwork ( nn . Module ): def __init__ ( self ): super ( NeuralNetwork , self ) . __init__ () self . flatten = nn . Flatten () self . linear_relu_stack = nn . Sequential ( nn . Linear ( 28 * 28 , 512 ), nn . ReLU (), nn . Linear ( 512 , 512 ), nn . ReLU (), nn . Linear ( 512 , 10 ), nn . ReLU () ) def forward ( self , x ): x = self . flatten ( x ) logits = self . linear_relu_stack ( x ) return logits model = NeuralNetwork () print ( model ) Out: NeuralNetwork( (flatten): Flatten(start_dim=1, end_dim=-1) (linear_relu_stack): Sequential( (0): Linear(in_features=784, out_features=512, bias=True) (1): ReLU() (2): Linear(in_features=512, out_features=512, bias=True) (3): ReLU() (4): Linear(in_features=512, out_features=10, bias=True) (5): ReLU() ) ) Build Network Training Models \u00b6 To train a model, we need a loss function ( loss_fn ) and an optimizer ( optimizer ). The loss function is used to evaluate the difference between the prediction of the neural network and the real label. The optimizer adjusts the parameters of the neural network to make the prediction closer to the real label (expected answer). Here, we use oneflow.optim.SGD to be our optimizer. This process is called back propagation. loss_fn = nn . CrossEntropyLoss () optimizer = flow . optim . SGD ( model . parameters (), lr = 1e-3 ) The train function is defined for training. In a single training loop, the model makes forward propagation, calculates loss, and backpropagates to update the model's parameters. def train ( iter , model , loss_fn , optimizer ): size = len ( iter . dataset ) for batch , ( x , y ) in enumerate ( iter ): # Compute prediction error pred = model ( x ) loss = loss_fn ( pred , y ) # Backpropagation optimizer . zero_grad () loss . backward () optimizer . step () current = batch * BATCH_SIZE if batch % 100 == 0 : print ( f \"loss: { loss : >7f } [ { current : >5d } / { size : >5d } ]\" ) We also define a test function to verify the accuracy of the model: def test ( iter , model , loss_fn ): size = len ( iter . dataset ) num_batches = len ( iter ) model . eval () test_loss , correct = 0 , 0 with flow . no_grad (): for x , y in iter : pred = model ( x ) test_loss += loss_fn ( pred , y ) bool_value = ( pred . argmax ( 1 ) . to ( dtype = flow . int64 ) == y ) correct += float ( bool_value . sum () . numpy ()) test_loss /= num_batches print ( \"test_loss\" , test_loss , \"num_batches \" , num_batches ) correct /= size print ( f \"Test Error: \\n Accuracy: { ( 100 * correct ) : >0.1f } , Avg loss: { test_loss : >8f } \" ) We use the train function to begin the train process for several epochs and use the test function to assess the accuracy of the network at the end of each epoch: epochs = 5 for t in range ( epochs ): print ( f \"Epoch { t + 1 } \\n -------------------------------\" ) train ( train_iter , model , loss_fn , optimizer ) test ( test_iter , model , loss_fn ) print ( \"Done!\" ) Out: loss: 2.299633 loss: 2.303208 loss: 2.298017 loss: 2.297773 loss: 2.294673 loss: 2.295637 Test Error: Accuracy: 22.1%, Avg loss: 2.292105 Epoch 2 ------------------------------- loss: 2.288640 loss: 2.286367 ... Autograd Backpropagation and Optimizer Saving and Loading Models \u00b6 Use oneflow.save to save the model. The saved model can be then loaded by oneflow.load to make predictions. flow . save ( model . state_dict (), \"./model\" ) Model Load and Save QQ Group \u00b6 Any problems encountered during the installation or usage, welcome to join the QQ Group to discuss with OneFlow developers and enthusiasts: Add QQ group by 331883 or scan the QR code below:","title":"Quickstart"},{"location":"basics/01_quickstart.html#quickstart","text":"This section will take the training process of MNIST as an example to briefly show how OneFlow can be used to accomplish common tasks in deep learning. Refer to the links in each section to the presentation on each subtask. Let\u2019s start by importing the necessary libraries: import oneflow as flow import oneflow.nn as nn import oneflow.utils.vision.transforms as transforms BATCH_SIZE = 128","title":"QUICKSTART"},{"location":"basics/01_quickstart.html#working-with-data","text":"OneFlow has two primitives to work with data, which are Dataset and Dataloader. The oneflow.utils.vision.datasets module contains a number of real data sets (such as MNIST, CIFAR 10, FashionMNIST). We can use oneflow.utils.vision.datasets.MNIST to get the training set and test set data of MNIST. mnist_train = flow . utils . vision . datasets . MNIST ( root = \"data\" , train = True , transform = transforms . ToTensor (), download = True , source_url = \"https://oneflow-public.oss-cn-beijing.aliyuncs.com/datasets/mnist/MNIST/\" , ) mnist_test = flow . utils . vision . datasets . MNIST ( root = \"data\" , train = False , transform = transforms . ToTensor (), download = True , source_url = \"https://oneflow-public.oss-cn-beijing.aliyuncs.com/datasets/mnist/MNIST/\" , ) Out: Downloading https://oneflow-public.oss-cn-beijing.aliyuncs.com/datasets/mnist/MNIST/train-images-idx3-ubyte.gz Downloading https://oneflow-public.oss-cn-beijing.aliyuncs.com/datasets/mnist/MNIST/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz 9913344it [00:00, 36066177.85it/s] Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw ... The data will be downloaded and extracted to ./data directory. The oneflow.utils.data.DataLoader wraps an iterable around the dataset . train_iter = flow . utils . data . DataLoader ( mnist_train , BATCH_SIZE , shuffle = True ) test_iter = flow . utils . data . DataLoader ( mnist_test , BATCH_SIZE , shuffle = False ) for x , y in train_iter : print ( \"x.shape:\" , x . shape ) print ( \"y.shape:\" , y . shape ) break Out: x.shape: flow.Size([128, 1, 28, 28]) y.shape: flow.Size([128]) Dataset and Dataloader","title":"Working with Data"},{"location":"basics/01_quickstart.html#building-networks","text":"To define a neural network in OneFlow, we create a class that inherits from nn.Module . We define the layers of the network in the __init__ function and specify how data will pass through the network in the forward function. class NeuralNetwork ( nn . Module ): def __init__ ( self ): super ( NeuralNetwork , self ) . __init__ () self . flatten = nn . Flatten () self . linear_relu_stack = nn . Sequential ( nn . Linear ( 28 * 28 , 512 ), nn . ReLU (), nn . Linear ( 512 , 512 ), nn . ReLU (), nn . Linear ( 512 , 10 ), nn . ReLU () ) def forward ( self , x ): x = self . flatten ( x ) logits = self . linear_relu_stack ( x ) return logits model = NeuralNetwork () print ( model ) Out: NeuralNetwork( (flatten): Flatten(start_dim=1, end_dim=-1) (linear_relu_stack): Sequential( (0): Linear(in_features=784, out_features=512, bias=True) (1): ReLU() (2): Linear(in_features=512, out_features=512, bias=True) (3): ReLU() (4): Linear(in_features=512, out_features=10, bias=True) (5): ReLU() ) ) Build Network","title":"Building Networks"},{"location":"basics/01_quickstart.html#training-models","text":"To train a model, we need a loss function ( loss_fn ) and an optimizer ( optimizer ). The loss function is used to evaluate the difference between the prediction of the neural network and the real label. The optimizer adjusts the parameters of the neural network to make the prediction closer to the real label (expected answer). Here, we use oneflow.optim.SGD to be our optimizer. This process is called back propagation. loss_fn = nn . CrossEntropyLoss () optimizer = flow . optim . SGD ( model . parameters (), lr = 1e-3 ) The train function is defined for training. In a single training loop, the model makes forward propagation, calculates loss, and backpropagates to update the model's parameters. def train ( iter , model , loss_fn , optimizer ): size = len ( iter . dataset ) for batch , ( x , y ) in enumerate ( iter ): # Compute prediction error pred = model ( x ) loss = loss_fn ( pred , y ) # Backpropagation optimizer . zero_grad () loss . backward () optimizer . step () current = batch * BATCH_SIZE if batch % 100 == 0 : print ( f \"loss: { loss : >7f } [ { current : >5d } / { size : >5d } ]\" ) We also define a test function to verify the accuracy of the model: def test ( iter , model , loss_fn ): size = len ( iter . dataset ) num_batches = len ( iter ) model . eval () test_loss , correct = 0 , 0 with flow . no_grad (): for x , y in iter : pred = model ( x ) test_loss += loss_fn ( pred , y ) bool_value = ( pred . argmax ( 1 ) . to ( dtype = flow . int64 ) == y ) correct += float ( bool_value . sum () . numpy ()) test_loss /= num_batches print ( \"test_loss\" , test_loss , \"num_batches \" , num_batches ) correct /= size print ( f \"Test Error: \\n Accuracy: { ( 100 * correct ) : >0.1f } , Avg loss: { test_loss : >8f } \" ) We use the train function to begin the train process for several epochs and use the test function to assess the accuracy of the network at the end of each epoch: epochs = 5 for t in range ( epochs ): print ( f \"Epoch { t + 1 } \\n -------------------------------\" ) train ( train_iter , model , loss_fn , optimizer ) test ( test_iter , model , loss_fn ) print ( \"Done!\" ) Out: loss: 2.299633 loss: 2.303208 loss: 2.298017 loss: 2.297773 loss: 2.294673 loss: 2.295637 Test Error: Accuracy: 22.1%, Avg loss: 2.292105 Epoch 2 ------------------------------- loss: 2.288640 loss: 2.286367 ... Autograd Backpropagation and Optimizer","title":"Training Models"},{"location":"basics/01_quickstart.html#saving-and-loading-models","text":"Use oneflow.save to save the model. The saved model can be then loaded by oneflow.load to make predictions. flow . save ( model . state_dict (), \"./model\" ) Model Load and Save","title":"Saving and Loading Models"},{"location":"basics/01_quickstart.html#qq-group","text":"Any problems encountered during the installation or usage, welcome to join the QQ Group to discuss with OneFlow developers and enthusiasts: Add QQ group by 331883 or scan the QR code below:","title":"QQ Group"},{"location":"basics/02_tensor.html","text":"TENSORS \u00b6 The data in the neural network is stored in tensors, which are similar to arrays and mathematical matrices. OneFlow provides a series of operators on tensors. Tensors, together with operators, build up a neural network. Tensors differ from regular multidimensional arrays in that they can run on AI chips (such as the Nvidia GPU) and CPU, thus increasing computing speed. In addition, OneFlow provides Autograd function, which supports automatic differentiation. import oneflow as flow import numpy as np Creating Tensors \u00b6 There are several ways to create tensors, including: Directly from data From a NumPy array By an operator Directly from data \u00b6 Tensors can be created directly from data: x1 = flow . tensor ([[ 1 , 2 ], [ 3 , 4 ]]) x2 = flow . tensor ([[ 1.0 , 2.0 ], [ 3.0 , 4.0 ]]) print ( x1 ) print ( x2 ) Out: tensor([[1, 2], [3, 4]], dtype=oneflow.int64) tensor([[1., 2.], [3., 4.]], dtype=oneflow.float32) We can see that the tensor x1 and x2 are created, whose data types are int64 and float32 , respectively. From a NumPy array \u00b6 Tensors can be created from NumPy arrays by passing the NumPy array as a parameter when the tensor object is constructed. x3 = flow . tensor ( np . ones (( 2 , 3 ))) x4 = flow . tensor ( np . random . rand ( 2 , 3 )) print ( x3 ) print ( x4 ) Out: tensor([[1., 1., 1.], [1., 1., 1.]], dtype=oneflow.float64) tensor([[0.6213, 0.6142, 0.1592], [0.5539, 0.8453, 0.8576]], dtype=oneflow.float64) By an operator \u00b6 There are also many operators available in OneFlow that can be used to create tensors. For example, ones , zeros and eye , which create the all-ones tensor, zero tensor, and identity tensor, respectively. x5 = flow . ones ( 2 , 3 ) x6 = flow . zeros ( 2 , 3 ) x7 = flow . eye ( 3 ) print ( x5 ) print ( x6 ) print ( x7 ) Out: tensor([[1., 1., 1.], [1., 1., 1.]], dtype=oneflow.float32) tensor([[0., 0., 0.], [0., 0., 0.]], dtype=oneflow.float32) tensor([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]], dtype=oneflow.float32) The randn method creates a random tensor: x8 = flow . randn ( 2 , 3 ) Difference Between Tensor and tensor \u00b6 There are two interfaces ( oneflow.Tensor and oneflow.tensor ) in OneFlow, both of which can be used to create tensors. What\u2019s the difference between them? Briefly speaking, the data type of oneflow.Tensor is limited to float32 by default, while the data type of oneflow.tensor can be changed when the data is created. The following code illustrates the difference: print ( flow . Tensor ([ 1 , 2 , 3 ])) print ( flow . tensor ([ 1 , 2 , 3 ])) print ( flow . tensor ([ 1.0 , 2.0 , 3.0 ])) Out: tensor([1., 2., 3.], dtype=oneflow.float32) tensor([1, 2, 3], dtype=oneflow.int64) tensor([1., 2., 3.], dtype=oneflow.float32) Besides, oneflow.Tensor can be created without specific data: x9 = flow . Tensor ( 2 , 3 ) print ( x9 . shape ) Out: flow.Size([2, 3]) Therefore, use oneflow.Tensor to create a tensor if you do not want to specify an explicit value, otherwise, you should use oneflow.tensor . Attributes of a Tensor \u00b6 The shape , dtype , and device attributes of a tensor describe its shape, data type, and device type respectively. x9 = flow . randn ( 1 , 4 ) print ( x9 . shape ) print ( x9 . dtype ) print ( x9 . device ) Out: flow.Size([1, 4]) oneflow.float32 cpu:0 The output shows the shape, the data type, and the device (on CPU No. 0, CPUs were numbered because OneFlow naturally supports distribution, see Consistent Tensor ). The shape of the tensor can be changed by the reshape method, and the data type and device of the tensor can be changed by the to method: x10 = x9.reshape(2, 2) x11 = x10.to(dtype=flow.int32, device=flow.device(\"cuda\")) print(x10.shape) print(x11.dtype, x11.device) Out: flow.Size([2, 2]) oneflow.int32 cuda:0 Operations on Tensors \u00b6 A large number of operators are provided in OneFlow, most of which are in the namespaces of oneflow , oneflow.nn , and oneflow.nn.functional . Tensors in OneFlow are as easy to use as the NumPy arrays. For example, slicing in NumPy style is supported: tensor = flow . ones ( 4 , 4 ) print ( 'First row: ' , tensor [ 0 ]) print ( 'First column: ' , tensor [:, 0 ]) print ( 'Last column:' , tensor [ ... , - 1 ]) tensor [:, 1 ] = 0 print ( tensor ) Out: First row: tensor([1., 1., 1., 1.], dtype=oneflow.float32) First column: tensor([1., 1., 1., 1.], dtype=oneflow.float32) Last column: tensor([1., 1., 1., 1.], dtype=oneflow.float32) tensor([[1., 0., 1., 1.], [1., 0., 1., 1.], [1., 0., 1., 1.], [1., 0., 1., 1.]], dtype=oneflow.float32) In addition, there are many other operations in OneFlow, such as add , sub , mul , div for arithmetic operations; scatter , gather , gather_nd for positional operations; and activation functions ( relu ), convolution functions ( conv2d ), etc. Click on their links to see detailed API description and find out more about other operators.","title":"Tensor"},{"location":"basics/02_tensor.html#tensors","text":"The data in the neural network is stored in tensors, which are similar to arrays and mathematical matrices. OneFlow provides a series of operators on tensors. Tensors, together with operators, build up a neural network. Tensors differ from regular multidimensional arrays in that they can run on AI chips (such as the Nvidia GPU) and CPU, thus increasing computing speed. In addition, OneFlow provides Autograd function, which supports automatic differentiation. import oneflow as flow import numpy as np","title":"TENSORS"},{"location":"basics/02_tensor.html#creating-tensors","text":"There are several ways to create tensors, including: Directly from data From a NumPy array By an operator","title":"Creating Tensors"},{"location":"basics/02_tensor.html#directly-from-data","text":"Tensors can be created directly from data: x1 = flow . tensor ([[ 1 , 2 ], [ 3 , 4 ]]) x2 = flow . tensor ([[ 1.0 , 2.0 ], [ 3.0 , 4.0 ]]) print ( x1 ) print ( x2 ) Out: tensor([[1, 2], [3, 4]], dtype=oneflow.int64) tensor([[1., 2.], [3., 4.]], dtype=oneflow.float32) We can see that the tensor x1 and x2 are created, whose data types are int64 and float32 , respectively.","title":"Directly from data"},{"location":"basics/02_tensor.html#from-a-numpy-array","text":"Tensors can be created from NumPy arrays by passing the NumPy array as a parameter when the tensor object is constructed. x3 = flow . tensor ( np . ones (( 2 , 3 ))) x4 = flow . tensor ( np . random . rand ( 2 , 3 )) print ( x3 ) print ( x4 ) Out: tensor([[1., 1., 1.], [1., 1., 1.]], dtype=oneflow.float64) tensor([[0.6213, 0.6142, 0.1592], [0.5539, 0.8453, 0.8576]], dtype=oneflow.float64)","title":"From a NumPy array"},{"location":"basics/02_tensor.html#by-an-operator","text":"There are also many operators available in OneFlow that can be used to create tensors. For example, ones , zeros and eye , which create the all-ones tensor, zero tensor, and identity tensor, respectively. x5 = flow . ones ( 2 , 3 ) x6 = flow . zeros ( 2 , 3 ) x7 = flow . eye ( 3 ) print ( x5 ) print ( x6 ) print ( x7 ) Out: tensor([[1., 1., 1.], [1., 1., 1.]], dtype=oneflow.float32) tensor([[0., 0., 0.], [0., 0., 0.]], dtype=oneflow.float32) tensor([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]], dtype=oneflow.float32) The randn method creates a random tensor: x8 = flow . randn ( 2 , 3 )","title":"By an operator"},{"location":"basics/02_tensor.html#difference-between-tensor-and-tensor","text":"There are two interfaces ( oneflow.Tensor and oneflow.tensor ) in OneFlow, both of which can be used to create tensors. What\u2019s the difference between them? Briefly speaking, the data type of oneflow.Tensor is limited to float32 by default, while the data type of oneflow.tensor can be changed when the data is created. The following code illustrates the difference: print ( flow . Tensor ([ 1 , 2 , 3 ])) print ( flow . tensor ([ 1 , 2 , 3 ])) print ( flow . tensor ([ 1.0 , 2.0 , 3.0 ])) Out: tensor([1., 2., 3.], dtype=oneflow.float32) tensor([1, 2, 3], dtype=oneflow.int64) tensor([1., 2., 3.], dtype=oneflow.float32) Besides, oneflow.Tensor can be created without specific data: x9 = flow . Tensor ( 2 , 3 ) print ( x9 . shape ) Out: flow.Size([2, 3]) Therefore, use oneflow.Tensor to create a tensor if you do not want to specify an explicit value, otherwise, you should use oneflow.tensor .","title":"Difference Between Tensor and tensor"},{"location":"basics/02_tensor.html#attributes-of-a-tensor","text":"The shape , dtype , and device attributes of a tensor describe its shape, data type, and device type respectively. x9 = flow . randn ( 1 , 4 ) print ( x9 . shape ) print ( x9 . dtype ) print ( x9 . device ) Out: flow.Size([1, 4]) oneflow.float32 cpu:0 The output shows the shape, the data type, and the device (on CPU No. 0, CPUs were numbered because OneFlow naturally supports distribution, see Consistent Tensor ). The shape of the tensor can be changed by the reshape method, and the data type and device of the tensor can be changed by the to method: x10 = x9.reshape(2, 2) x11 = x10.to(dtype=flow.int32, device=flow.device(\"cuda\")) print(x10.shape) print(x11.dtype, x11.device) Out: flow.Size([2, 2]) oneflow.int32 cuda:0","title":"Attributes of a Tensor"},{"location":"basics/02_tensor.html#operations-on-tensors","text":"A large number of operators are provided in OneFlow, most of which are in the namespaces of oneflow , oneflow.nn , and oneflow.nn.functional . Tensors in OneFlow are as easy to use as the NumPy arrays. For example, slicing in NumPy style is supported: tensor = flow . ones ( 4 , 4 ) print ( 'First row: ' , tensor [ 0 ]) print ( 'First column: ' , tensor [:, 0 ]) print ( 'Last column:' , tensor [ ... , - 1 ]) tensor [:, 1 ] = 0 print ( tensor ) Out: First row: tensor([1., 1., 1., 1.], dtype=oneflow.float32) First column: tensor([1., 1., 1., 1.], dtype=oneflow.float32) Last column: tensor([1., 1., 1., 1.], dtype=oneflow.float32) tensor([[1., 0., 1., 1.], [1., 0., 1., 1.], [1., 0., 1., 1.], [1., 0., 1., 1.]], dtype=oneflow.float32) In addition, there are many other operations in OneFlow, such as add , sub , mul , div for arithmetic operations; scatter , gather , gather_nd for positional operations; and activation functions ( relu ), convolution functions ( conv2d ), etc. Click on their links to see detailed API description and find out more about other operators.","title":"Operations on Tensors"},{"location":"basics/03_dataset_dataloader.html","text":"DATASETS & DATALOADERS \u00b6 The behavior of OneFlow's Dataset and DataLoader is the same as PyTorch . Both Dataset and DataLoader are for designed for making dataset management decoupling with model training. oneflow.utils.vision.datasets provides us a number of classes that can automatically download and load prevailing datasets (such as fashionmnist). DataLoader wraps data into an iterator, for easy iterating and access to samples during training. import matplotlib.pyplot as plt import oneflow as flow import oneflow.nn as nn from oneflow.utils.vision.transforms import ToTensor from oneflow.utils.data import Dataset import oneflow.utils.vision.datasets as datasets Loading a Dataset \u00b6 Here is an example of how to load by Dataset . root : the path where the train/test data is stored; train : True for training dataset, False for test dataset; download=True : downloads the data from the internet if it\u2019s not available at root ; transforms : the feature and label transformations. training_data = datasets . FashionMNIST ( root = \"data\" , train = True , download = True , transform = ToTensor () ) test_data = datasets . FashionMNIST ( root = \"data\" , train = False , download = True , transform = ToTensor () ) The first time it runs, it will download the data set and output the following: Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz 26422272it [00:17, 1504123.86it/s] Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz 29696it [00:00, 98468.01it/s] Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz 4422656it [00:07, 620608.04it/s] Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz 6144it [00:00, 19231196.85it/s] Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw Iterating the Dataset \u00b6 We can index Dataset manually like a list : training_data[index] . The following example randomly accesses 9 pictures in training_data and visualizes them. labels_map = { 0 : \"T-Shirt\" , 1 : \"Trouser\" , 2 : \"Pullover\" , 3 : \"Dress\" , 4 : \"Coat\" , 5 : \"Sandal\" , 6 : \"Shirt\" , 7 : \"Sneaker\" , 8 : \"Bag\" , 9 : \"Ankle Boot\" , } figure = plt . figure ( figsize = ( 8 , 8 )) cols , rows = 3 , 3 from random import randint for i in range ( 1 , cols * rows + 1 ): sample_idx = randint ( 0 , len ( training_data )) img , label = training_data [ sample_idx ] figure . add_subplot ( rows , cols , i ) plt . title ( labels_map [ label ]) plt . axis ( \"off\" ) plt . imshow ( img . squeeze () . numpy (), cmap = \"gray\" ) plt . show () Creating a Custom Dataset for Your Files \u00b6 A custom dataset can be defined by inheriting oneflow.utils.data.Dataset . Custom Dataset can be used with Dataloader introduced in the next section to simplify data processing. Here is an example of how to create a custom Dataset , the key steps are: Inheriting oneflow.utils.data.Dataset Implements the __len__ method that returns the number of samples in our dataset. Implements the __getitem__ method that loads and returns a sample from the dataset when users call dataset_obj[idx] . import numpy as np class CustomDataset ( Dataset ): raw_data_x = np . array ([[ 1 , 2 ], [ 2 , 3 ], [ 4 , 6 ], [ 3 , 1 ]], dtype = np . float32 ) raw_label = np . array ([[ 8 ], [ 13 ], [ 26 ], [ 9 ]], dtype = np . float32 ) def __init__ ( self , transform = None , target_transform = None ): self . transform = transform self . target_transform = target_transform def __len__ ( self ): return len ( raw_label ) def __getitem__ ( self , idx ): x = CustomDataset . raw_data_x [ idx ] label = CustomDataset . raw_label [ idx ] if self . transform : x = self . transform ( x ) if self . target_transform : label = self . target_transform ( label ) return x , label custom_dataset = CustomDataset () print ( custom_dataset [ 0 ]) print ( custom_dataset [ 1 ]) Output\uff1a (array([1., 2.], dtype=float32), array([8.], dtype=float32)) (array([2., 3.], dtype=float32), array([13.], dtype=float32)) Using DataLoader \u00b6 The Dataset retrieves all features of our dataset and labels one sample at a time. While training a model, we typically want to pass samples in \"minibatches\", which means they will load a same amount of data as the batch size at the time, and reshuffle the data at every epoch to reduce model overfitting. At this time, we can use DataLoader . DataLoader can wrap Dataset into an iterator to access data during the training loop. Here is an example: batch_size=64 : the batch size at each iteration shuffle : whether the data is shuffled after we iterate over all batches from oneflow.utils.data import DataLoader train_dataloader = DataLoader ( training_data , batch_size = 64 , shuffle = True ) x , label = next ( iter ( train_dataloader )) print ( f \"shape of x: { x . shape } , shape of label: { label . shape } \" ) Output\uff1a shape of x:flow.Size([64, 1, 28, 28]), shape of label: flow.Size([64]) img = x [ 0 ] . squeeze () . numpy () label = label [ 0 ] plt . imshow ( img , cmap = \"gray\" ) plt . show () print ( label ) Output\uff1a(output a picture randomly) tensor(9, dtype=oneflow.int64) We can also use the Dataloader iterator during the training loop. for x , label in train_dataloader : print ( x . shape , label . shape ) # training...","title":"Datesets & Dataloaders"},{"location":"basics/03_dataset_dataloader.html#datasets-dataloaders","text":"The behavior of OneFlow's Dataset and DataLoader is the same as PyTorch . Both Dataset and DataLoader are for designed for making dataset management decoupling with model training. oneflow.utils.vision.datasets provides us a number of classes that can automatically download and load prevailing datasets (such as fashionmnist). DataLoader wraps data into an iterator, for easy iterating and access to samples during training. import matplotlib.pyplot as plt import oneflow as flow import oneflow.nn as nn from oneflow.utils.vision.transforms import ToTensor from oneflow.utils.data import Dataset import oneflow.utils.vision.datasets as datasets","title":"DATASETS &amp; DATALOADERS"},{"location":"basics/03_dataset_dataloader.html#loading-a-dataset","text":"Here is an example of how to load by Dataset . root : the path where the train/test data is stored; train : True for training dataset, False for test dataset; download=True : downloads the data from the internet if it\u2019s not available at root ; transforms : the feature and label transformations. training_data = datasets . FashionMNIST ( root = \"data\" , train = True , download = True , transform = ToTensor () ) test_data = datasets . FashionMNIST ( root = \"data\" , train = False , download = True , transform = ToTensor () ) The first time it runs, it will download the data set and output the following: Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz 26422272it [00:17, 1504123.86it/s] Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz 29696it [00:00, 98468.01it/s] Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz 4422656it [00:07, 620608.04it/s] Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz 6144it [00:00, 19231196.85it/s] Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw","title":"Loading a Dataset"},{"location":"basics/03_dataset_dataloader.html#iterating-the-dataset","text":"We can index Dataset manually like a list : training_data[index] . The following example randomly accesses 9 pictures in training_data and visualizes them. labels_map = { 0 : \"T-Shirt\" , 1 : \"Trouser\" , 2 : \"Pullover\" , 3 : \"Dress\" , 4 : \"Coat\" , 5 : \"Sandal\" , 6 : \"Shirt\" , 7 : \"Sneaker\" , 8 : \"Bag\" , 9 : \"Ankle Boot\" , } figure = plt . figure ( figsize = ( 8 , 8 )) cols , rows = 3 , 3 from random import randint for i in range ( 1 , cols * rows + 1 ): sample_idx = randint ( 0 , len ( training_data )) img , label = training_data [ sample_idx ] figure . add_subplot ( rows , cols , i ) plt . title ( labels_map [ label ]) plt . axis ( \"off\" ) plt . imshow ( img . squeeze () . numpy (), cmap = \"gray\" ) plt . show ()","title":"Iterating the Dataset"},{"location":"basics/03_dataset_dataloader.html#creating-a-custom-dataset-for-your-files","text":"A custom dataset can be defined by inheriting oneflow.utils.data.Dataset . Custom Dataset can be used with Dataloader introduced in the next section to simplify data processing. Here is an example of how to create a custom Dataset , the key steps are: Inheriting oneflow.utils.data.Dataset Implements the __len__ method that returns the number of samples in our dataset. Implements the __getitem__ method that loads and returns a sample from the dataset when users call dataset_obj[idx] . import numpy as np class CustomDataset ( Dataset ): raw_data_x = np . array ([[ 1 , 2 ], [ 2 , 3 ], [ 4 , 6 ], [ 3 , 1 ]], dtype = np . float32 ) raw_label = np . array ([[ 8 ], [ 13 ], [ 26 ], [ 9 ]], dtype = np . float32 ) def __init__ ( self , transform = None , target_transform = None ): self . transform = transform self . target_transform = target_transform def __len__ ( self ): return len ( raw_label ) def __getitem__ ( self , idx ): x = CustomDataset . raw_data_x [ idx ] label = CustomDataset . raw_label [ idx ] if self . transform : x = self . transform ( x ) if self . target_transform : label = self . target_transform ( label ) return x , label custom_dataset = CustomDataset () print ( custom_dataset [ 0 ]) print ( custom_dataset [ 1 ]) Output\uff1a (array([1., 2.], dtype=float32), array([8.], dtype=float32)) (array([2., 3.], dtype=float32), array([13.], dtype=float32))","title":"Creating a Custom Dataset for Your Files"},{"location":"basics/03_dataset_dataloader.html#using-dataloader","text":"The Dataset retrieves all features of our dataset and labels one sample at a time. While training a model, we typically want to pass samples in \"minibatches\", which means they will load a same amount of data as the batch size at the time, and reshuffle the data at every epoch to reduce model overfitting. At this time, we can use DataLoader . DataLoader can wrap Dataset into an iterator to access data during the training loop. Here is an example: batch_size=64 : the batch size at each iteration shuffle : whether the data is shuffled after we iterate over all batches from oneflow.utils.data import DataLoader train_dataloader = DataLoader ( training_data , batch_size = 64 , shuffle = True ) x , label = next ( iter ( train_dataloader )) print ( f \"shape of x: { x . shape } , shape of label: { label . shape } \" ) Output\uff1a shape of x:flow.Size([64, 1, 28, 28]), shape of label: flow.Size([64]) img = x [ 0 ] . squeeze () . numpy () label = label [ 0 ] plt . imshow ( img , cmap = \"gray\" ) plt . show () print ( label ) Output\uff1a(output a picture randomly) tensor(9, dtype=oneflow.int64) We can also use the Dataloader iterator during the training loop. for x , label in train_dataloader : print ( x . shape , label . shape ) # training...","title":"Using DataLoader"},{"location":"basics/04_build_network.html","text":"BUILD NEURAL NETWORK \u00b6 The layers of a neural network can be built by API in namespace oneflow.nn , It provides common Module (such as oneflow.nn.Conv2d , oneflow.nn.ReLU ). All Module classes inherit from oneflow.nn.Module , and many simple Module can form more complex Module. In this way, users can easily build and manage complex neural networks. import oneflow as flow import oneflow.nn as nn Define Module class \u00b6 oneflow.nn provides common Module classes and we can use them easily. Or we can build a neural network by customizing the Module class on the basis. This method consists of three parts\uff1a Write a class that inherits from oneflow.nn.Module class Write the method of __init__ class, in which we construct the network structure Write the method of forward class, which calculates on the basis of the input of Module class NeuralNetwork ( nn . Module ): def __init__ ( self ): super ( NeuralNetwork , self ) . __init__ () self . flatten = nn . Flatten () self . linear_relu_stack = nn . Sequential ( nn . Linear ( 28 * 28 , 512 ), nn . ReLU (), nn . Linear ( 512 , 512 ), nn . ReLU (), nn . Linear ( 512 , 10 ), nn . ReLU () ) def forward ( self , x ): x = self . flatten ( x ) logits = self . linear_relu_stack ( x ) return logits net = NeuralNetwork () print ( net ) The above code will output the structure of the NeuralNetwork network\uff1a NeuralNetwork( (flatten): Flatten(start_dim=1, end_dim=-1) (linear_relu_stack): Sequential( (0): Linear(in_features=784, out_features=512, bias=True) (1): ReLU() (2): Linear(in_features=512, out_features=512, bias=True) (3): ReLU() (4): Linear(in_features=512, out_features=10, bias=True) (5): ReLU() ) ) Then, call net (notice\uff1aIt is not recommended to explicitly call forward ): X = flow . ones ( 1 , 28 , 28 ) logits = net ( X ) pred_probab = nn . Softmax ( dim = 1 )( logits ) y_pred = pred_probab . argmax ( 1 ) print ( f \"Predicted class: { y_pred } \" ) You will get output similar to the following: Predicted class: tensor([1], dtype=oneflow.int32) The above process of data input, network calculation and the output of reasoning is shown in the figure below. flow.nn.functional \u00b6 In addition to oneflow.nn , oneflow.nn.functional namespace also provides many API. It overlaps with oneflow.nn to some extent. For example, nn.functional.relu and nn.ReLU both can be used for activation in neural network. The main differences between them are: The API under nn is a class. It needs to be instantiated before being called; The API under nn.functional is a function. It is called directly. The API under nn manages network parameters automatically\uff1bBut for the function under NN. Functional , we need to define our own parameters and manually pass them in each call. In fact, most of the Module provided by OneFlow is the result of encapsulating the methods under nn.functional . nn.functional can manage the network more finely. The following example uses the methods in nn.functional to build a Module FunctionalNeuralNetwork equivalent to the NeuralNetwork class above. Readers can appreciate the similarities and differences between the two: class FunctionalNeuralNetwork ( nn . Module ): def __init__ ( self ): super ( FunctionalNeuralNetwork , self ) . __init__ () self . weight1 = nn . Parameter ( flow . randn ( 28 * 28 , 512 )) self . bias1 = nn . Parameter ( flow . randn ( 512 )) self . weight2 = nn . Parameter ( flow . randn ( 512 , 512 )) self . bias2 = nn . Parameter ( flow . randn ( 512 )) self . weight3 = nn . Parameter ( flow . randn ( 512 , 10 )) self . bias3 = nn . Parameter ( flow . randn ( 10 )) def forward ( self , x ): x = x . reshape ( 1 , 28 * 28 ) out = flow . matmul ( x , self . weight1 ) out = out + self . bias1 out = nn . functional . relu ( out ) out = flow . matmul ( out , self . weight2 ) out = out + self . bias2 out = nn . functional . relu ( out ) out = flow . matmul ( out , self . weight3 ) out = out + self . bias3 out = nn . functional . relu ( out ) return out net = FunctionalNeuralNetwork () X = flow . ones ( 1 , 28 , 28 ) logits = net ( X ) pred_probab = nn . Softmax ( dim = 1 )( logits ) y_pred = pred_probab . argmax ( 1 ) print ( f \"Predicted class: { y_pred } \" ) Module container \u00b6 Comparing the similarities and differences between the NeuralNetwork and FunctionalNeuralNetwork ,we can find that nn.Sequential plays an important role in simplifying the code. nn.Sequential is a special container. Any class inherited from nn.Module can be placed in it. Its specialty is that when Sequential propagates forward, Sequential automatically \"concatenates\" the layers contained in the container. Specifically, the output of the previous layer will be automatically transferred as the input of the next layer according to the sequence of Sequential added to each layer until the output of the last layer of the whole Moudle is obtained. The following is an example of building a network without Sequential (not recommended): class MyModel ( nn . Module ): def __init__ ( self ): super ( MyModel , self ) . __init__ () self . conv1 = nn . Conv2d ( 1 , 20 , 5 ) self . relu1 = nn . ReLU () self . conv2 = nn . Conv2d ( 20 , 64 , 5 ) self . relu2 = nn . ReLU () def forward ( self , x ): out = self . conv1 ( x ) out = self . relu1 ( out ) out = self . conv2 ( out ) out = self . relu2 ( out ) return out If sequential is used, it looks like this, which will be more concise. class MySeqModel ( nn . Module ): def __init__ ( self ): super ( MySeqModel , self ) . __init__ () self . seq = nn . Sequential ( nn . Conv2d ( 1 , 20 , 5 ), nn . ReLU (), nn . Conv2d ( 20 , 64 , 5 ), nn . ReLU () ) def forward ( self , x ): return self . seq ( x ) Besides Sequential, there are nn.Modulelist and nn.ModuleDict . They can automatically register parameters to the whole network. But their other behavior is similar to Python list and Python dict, which are just simple containers and do not automatically propagate forward. You need manually traverse to complete the calculation of each layer.","title":"Build Neural Network"},{"location":"basics/04_build_network.html#build-neural-network","text":"The layers of a neural network can be built by API in namespace oneflow.nn , It provides common Module (such as oneflow.nn.Conv2d , oneflow.nn.ReLU ). All Module classes inherit from oneflow.nn.Module , and many simple Module can form more complex Module. In this way, users can easily build and manage complex neural networks. import oneflow as flow import oneflow.nn as nn","title":"BUILD NEURAL NETWORK"},{"location":"basics/04_build_network.html#define-module-class","text":"oneflow.nn provides common Module classes and we can use them easily. Or we can build a neural network by customizing the Module class on the basis. This method consists of three parts\uff1a Write a class that inherits from oneflow.nn.Module class Write the method of __init__ class, in which we construct the network structure Write the method of forward class, which calculates on the basis of the input of Module class NeuralNetwork ( nn . Module ): def __init__ ( self ): super ( NeuralNetwork , self ) . __init__ () self . flatten = nn . Flatten () self . linear_relu_stack = nn . Sequential ( nn . Linear ( 28 * 28 , 512 ), nn . ReLU (), nn . Linear ( 512 , 512 ), nn . ReLU (), nn . Linear ( 512 , 10 ), nn . ReLU () ) def forward ( self , x ): x = self . flatten ( x ) logits = self . linear_relu_stack ( x ) return logits net = NeuralNetwork () print ( net ) The above code will output the structure of the NeuralNetwork network\uff1a NeuralNetwork( (flatten): Flatten(start_dim=1, end_dim=-1) (linear_relu_stack): Sequential( (0): Linear(in_features=784, out_features=512, bias=True) (1): ReLU() (2): Linear(in_features=512, out_features=512, bias=True) (3): ReLU() (4): Linear(in_features=512, out_features=10, bias=True) (5): ReLU() ) ) Then, call net (notice\uff1aIt is not recommended to explicitly call forward ): X = flow . ones ( 1 , 28 , 28 ) logits = net ( X ) pred_probab = nn . Softmax ( dim = 1 )( logits ) y_pred = pred_probab . argmax ( 1 ) print ( f \"Predicted class: { y_pred } \" ) You will get output similar to the following: Predicted class: tensor([1], dtype=oneflow.int32) The above process of data input, network calculation and the output of reasoning is shown in the figure below.","title":"Define Module class"},{"location":"basics/04_build_network.html#flownnfunctional","text":"In addition to oneflow.nn , oneflow.nn.functional namespace also provides many API. It overlaps with oneflow.nn to some extent. For example, nn.functional.relu and nn.ReLU both can be used for activation in neural network. The main differences between them are: The API under nn is a class. It needs to be instantiated before being called; The API under nn.functional is a function. It is called directly. The API under nn manages network parameters automatically\uff1bBut for the function under NN. Functional , we need to define our own parameters and manually pass them in each call. In fact, most of the Module provided by OneFlow is the result of encapsulating the methods under nn.functional . nn.functional can manage the network more finely. The following example uses the methods in nn.functional to build a Module FunctionalNeuralNetwork equivalent to the NeuralNetwork class above. Readers can appreciate the similarities and differences between the two: class FunctionalNeuralNetwork ( nn . Module ): def __init__ ( self ): super ( FunctionalNeuralNetwork , self ) . __init__ () self . weight1 = nn . Parameter ( flow . randn ( 28 * 28 , 512 )) self . bias1 = nn . Parameter ( flow . randn ( 512 )) self . weight2 = nn . Parameter ( flow . randn ( 512 , 512 )) self . bias2 = nn . Parameter ( flow . randn ( 512 )) self . weight3 = nn . Parameter ( flow . randn ( 512 , 10 )) self . bias3 = nn . Parameter ( flow . randn ( 10 )) def forward ( self , x ): x = x . reshape ( 1 , 28 * 28 ) out = flow . matmul ( x , self . weight1 ) out = out + self . bias1 out = nn . functional . relu ( out ) out = flow . matmul ( out , self . weight2 ) out = out + self . bias2 out = nn . functional . relu ( out ) out = flow . matmul ( out , self . weight3 ) out = out + self . bias3 out = nn . functional . relu ( out ) return out net = FunctionalNeuralNetwork () X = flow . ones ( 1 , 28 , 28 ) logits = net ( X ) pred_probab = nn . Softmax ( dim = 1 )( logits ) y_pred = pred_probab . argmax ( 1 ) print ( f \"Predicted class: { y_pred } \" )","title":"flow.nn.functional"},{"location":"basics/04_build_network.html#module-container","text":"Comparing the similarities and differences between the NeuralNetwork and FunctionalNeuralNetwork ,we can find that nn.Sequential plays an important role in simplifying the code. nn.Sequential is a special container. Any class inherited from nn.Module can be placed in it. Its specialty is that when Sequential propagates forward, Sequential automatically \"concatenates\" the layers contained in the container. Specifically, the output of the previous layer will be automatically transferred as the input of the next layer according to the sequence of Sequential added to each layer until the output of the last layer of the whole Moudle is obtained. The following is an example of building a network without Sequential (not recommended): class MyModel ( nn . Module ): def __init__ ( self ): super ( MyModel , self ) . __init__ () self . conv1 = nn . Conv2d ( 1 , 20 , 5 ) self . relu1 = nn . ReLU () self . conv2 = nn . Conv2d ( 20 , 64 , 5 ) self . relu2 = nn . ReLU () def forward ( self , x ): out = self . conv1 ( x ) out = self . relu1 ( out ) out = self . conv2 ( out ) out = self . relu2 ( out ) return out If sequential is used, it looks like this, which will be more concise. class MySeqModel ( nn . Module ): def __init__ ( self ): super ( MySeqModel , self ) . __init__ () self . seq = nn . Sequential ( nn . Conv2d ( 1 , 20 , 5 ), nn . ReLU (), nn . Conv2d ( 20 , 64 , 5 ), nn . ReLU () ) def forward ( self , x ): return self . seq ( x ) Besides Sequential, there are nn.Modulelist and nn.ModuleDict . They can automatically register parameters to the whole network. But their other behavior is similar to Python list and Python dict, which are just simple containers and do not automatically propagate forward. You need manually traverse to complete the calculation of each layer.","title":"Module container"},{"location":"basics/05_autograd.html","text":"AUTOGRAD \u00b6 The training process of a neural network is powered by backpropagation algorithm . In the backpropagation process, we update the parameters by obtaining the gradient of the loss function with respect to the parameters. OneFlow provides an autograd engine, which can calculate the gradient of the parameters in the neural network automatically. We will first introduce the basic concepts of the computation graph, which are conducive to understand the common settings and limitations of Oneflow's automatic differentiation. Then we will introduce OneFlow's common automatic differentiation interfaces. Computation Graph \u00b6 Computation graphs are composed of tensors and operators. We show this in code as below: import oneflow as flow def loss ( y_pred , y ): return flow . sum ( 1 / 2 * ( y_pred - y ) ** 2 ) x = flow . ones ( 1 , 5 ) # input\u3010\u4e0d\u786e\u5b9a\u3011 w = flow . randn ( 5 , 3 , requires_grad = True ) b = flow . randn ( 1 , 3 , requires_grad = True ) z = flow . matmul ( x , w ) + b y = flow . zeros ( 1 , 3 ) # label l = loss ( z , y ) Corresponding computation graph\uff1a In computation graph, the nodes only with output and with no input called leaf node, like x , w , b , and y , the nodes only with output and with no input called root node, like loss . During the backpropagation process, the gradient of l to w and b is required to update w and b . Therefore, we need to set requires_grad as True when creating them. Automatic Gradient \u00b6 backward() and Gradient \u00b6 During the backpropagation process, we need to get the gradients of l to w , b respectively, shown as \\(\\frac{\\partial l}{\\partial w}\\) and \\(\\frac{\\partial l}{\\partial b}\\) . We only need to call the 'backward()' method of l , and then OneFlow will automatically calculate the gradients and store them in the w.grad and b.grad . l . backward () print ( w . grad ) print ( b . grad ) tensor([[0.9397, 2.5428, 2.5377], [0.9397, 2.5428, 2.5377], [0.9397, 2.5428, 2.5377], [0.9397, 2.5428, 2.5377], [0.9397, 2.5428, 2.5377]], dtype=oneflow.float32) tensor([[0.9397, 2.5428, 2.5377]], dtype=oneflow.float32) Gradient for Non-leaf Nodes \u00b6 By default, only gradients of leaf nodes with requires_grad=True will be retained. The 'grad' of a non-leaf node is automatically freed during the calling of 'backward' and cannot be viewed. Tensor.retain_grad() can be called to retain and view the 'grad' of a non-leaf node. from math import pi n1 = flow . tensor ( pi / 2 , requires_grad = True ) n2 = flow . sin ( n1 ) n2 . retain_grad () n3 = flow . pow ( n2 , 2 ) n3 . backward () print ( n1 . grad ) print ( n2 . grad ) we get \\(\\frac{\\partial n_3}{\\partial n_1}\\) and \\(\\frac{\\partial n_3}{\\partial n_2}\\) using the code above. Output: tensor(-8.7423e-08, dtype=oneflow.float32) tensor(2., dtype=oneflow.float32) Call backward() Multiple Times on a Computation Graph \u00b6 By default, we can only call backward() once for each computation graph. For example, the following code will raise an error: n1 = flow . tensor ( 10. , requires_grad = True ) n2 = flow . pow ( n1 , 2 ) n2 . backward () n2 . backward () Error message: Maybe you try to backward through the node a second time. Specify retain_graph=True when calling .backward() or autograd.grad() the first time. If we need to call backward() multiple times on the same computation graph, retain_graph needs to be True . n1 = flow . tensor ( 10. , requires_grad = True ) n2 = flow . pow ( n1 , 2 ) n2 . backward ( retain_graph = True ) print ( n1 . grad ) n2 . backward () print ( n1 . grad ) Output\uff1a tensor(20., dtype=oneflow.float32) tensor(40., dtype=oneflow.float32) The above output shows that OneFlow will accumulate the gradient calculated by backward() multiple times. By calling the zeros_() , we can clear the gradient: n1 = flow . tensor ( 10. , requires_grad = True ) n2 = flow . pow ( n1 , 2 ) n2 . backward ( retain_graph = True ) print ( n1 . grad ) n1 . grad . zeros_ () n2 . backward () print ( n1 . grad ) Output\uff1a tensor(20., dtype=oneflow.float32) tensor(20., dtype=oneflow.float32) Disabled Gradient Calculation \u00b6 By default, OneFlow will trace and calculate gradients of Tensors with requires_grad = Ture . However, in some cases, we don't need OneFlow to keep tracing gradients such as just wanting the forward pass for inference. Then we can use oneflow.no_grad() or oneflow.Tensor.detach() to set. z = flow . matmul ( x , w ) + b print ( z . requires_grad ) with flow . no_grad (): z = flow . matmul ( x , w ) + b print ( z . requires_grad ) Output\uff1a True False z_det = z . detach () print ( z_det . requires_grad ) Output\uff1a False Gradients for Non-Scalar Outputs \u00b6 Usually, we call backward() on scalar loss . However, if loss is a tensor, an error will be raised when calling backward() on loss . x = flow . randn ( 1 , 2 , requires_grad = True ) y = 3 * x + 1 y . backward () Error message\uff1a Check failed: IsScalarTensor(*outputs.at(i)) Grad can be implicitly created only for scalar outputs We can get the gradient after y.sum() . x = flow . randn ( 1 , 2 , requires_grad = True ) y = 3 * x + 1 y = y . sum () y . backward () print ( x . grad ) Output\uff1a tensor([[3., 3.]], dtype=oneflow.float32) Please refer to the \"Further Reading\" section below for the analysis of the cause and solution of the error. Further Reading \u00b6 There are two elements \\(x_1\\) and \\(x_2\\) in Tensor x , and two elements \\(y_1\\) and \\(y_2\\) in Tensor y . The relationship between them is: \\[ \\mathbf{x} = [x_1, x_2] \\] \\[ \\mathbf{y} = [y_1, y_2] = [3x_1+1, 3x_2+1] \\] We want to get \\(\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}\\) \\[ \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} = \\frac{[3x_1+1, 3x_2+1]}{[x_1, x_2]} \\] It doesn't make sense in mathematics, so of course an error is reported. In fact, when the user calls y.backward() , the result desired is usually: \\[ [\\frac{\\partial y_1}{\\partial x_1}, \\frac{\\partial y_2}{\\partial x_2}] \\] After call sum() on y : \\[ y = y_1 + y_2 = 3x_1 + 3x_2 + 2 \\] At this time, when calling backward() , the gradients of \\(x_1\\) and \\(x_2\\) can be calculated: \\[ \\frac{\\partial y}{\\partial x_1} = \\frac{\\partial 3x_1 + 3x_2 + 2}{\\partial x_1} = 3 \\] \\[ \\frac{\\partial y}{\\partial x_2} = \\frac{\\partial 3x_1 + 3x_2 + 2}{\\partial x_2} = 3 \\] In addition to using sum() , Vector Jacobian Product(VJP) is a more general method to calculate the gradient of the non-scalar root node. Using the above example, OneFlow will generate the Jacobian matrix according to the computation graph during the backpropagation process: \\[ J = \\begin{pmatrix} \\frac{\\partial y_1}{\\partial x_1} & \\frac{\\partial y_1}{\\partial x_2}\\\\ \\frac{\\partial y_2}{\\partial x_1} & \\frac{\\partial y_2}{\\partial x_2} \\end{pmatrix}\\\\ = \\begin{pmatrix} \\frac{\\partial y_1}{\\partial x_1} & 0 \\\\ 0 & \\frac{\\partial y_2}{\\partial x_2} \\end{pmatrix} \\] To calculate VJP, a vector \\(\\mathbf{v}\\) with the same size as \\(\\mathbf{y}\\) needs to be provided: \\[ \\begin{bmatrix} v_1\\\\ v_2 \\end{bmatrix} \\times \\begin{pmatrix} \\frac{\\partial y_1}{\\partial x_1} & 0 \\\\ 0 & \\frac{\\partial y_2}{\\partial x_2} \\end{pmatrix}= \\begin{bmatrix} v_1 \\frac{\\partial y_1}{\\partial x_1}\\\\ v_2 \\frac{\\partial y_2}{\\partial x_2} \\end{bmatrix} \\] If the vector \\(\\mathbf{v}\\) is the gradient of the upper layer in the backpropagation, the result of VJP is exactly the gradient required by the current layer. backward() can accept a tensor as a parameter, when the parameter is \\(\\mathbf{v}\\) in VJP. We can also use the following methods to find the gradient of a tensor: x = flow . randn ( 1 , 2 , requires_grad = True ) y = 3 * x + 1 y . backward ( flow . ones_like ( y )) print ( x . grad ) Output\uff1a tensor([[3., 3.]], dtype=oneflow.float32) External links Automatic Differentiation","title":"Autograd"},{"location":"basics/05_autograd.html#autograd","text":"The training process of a neural network is powered by backpropagation algorithm . In the backpropagation process, we update the parameters by obtaining the gradient of the loss function with respect to the parameters. OneFlow provides an autograd engine, which can calculate the gradient of the parameters in the neural network automatically. We will first introduce the basic concepts of the computation graph, which are conducive to understand the common settings and limitations of Oneflow's automatic differentiation. Then we will introduce OneFlow's common automatic differentiation interfaces.","title":"AUTOGRAD"},{"location":"basics/05_autograd.html#computation-graph","text":"Computation graphs are composed of tensors and operators. We show this in code as below: import oneflow as flow def loss ( y_pred , y ): return flow . sum ( 1 / 2 * ( y_pred - y ) ** 2 ) x = flow . ones ( 1 , 5 ) # input\u3010\u4e0d\u786e\u5b9a\u3011 w = flow . randn ( 5 , 3 , requires_grad = True ) b = flow . randn ( 1 , 3 , requires_grad = True ) z = flow . matmul ( x , w ) + b y = flow . zeros ( 1 , 3 ) # label l = loss ( z , y ) Corresponding computation graph\uff1a In computation graph, the nodes only with output and with no input called leaf node, like x , w , b , and y , the nodes only with output and with no input called root node, like loss . During the backpropagation process, the gradient of l to w and b is required to update w and b . Therefore, we need to set requires_grad as True when creating them.","title":"Computation Graph"},{"location":"basics/05_autograd.html#automatic-gradient","text":"","title":"Automatic Gradient"},{"location":"basics/05_autograd.html#backward-and-gradient","text":"During the backpropagation process, we need to get the gradients of l to w , b respectively, shown as \\(\\frac{\\partial l}{\\partial w}\\) and \\(\\frac{\\partial l}{\\partial b}\\) . We only need to call the 'backward()' method of l , and then OneFlow will automatically calculate the gradients and store them in the w.grad and b.grad . l . backward () print ( w . grad ) print ( b . grad ) tensor([[0.9397, 2.5428, 2.5377], [0.9397, 2.5428, 2.5377], [0.9397, 2.5428, 2.5377], [0.9397, 2.5428, 2.5377], [0.9397, 2.5428, 2.5377]], dtype=oneflow.float32) tensor([[0.9397, 2.5428, 2.5377]], dtype=oneflow.float32)","title":"backward() and Gradient"},{"location":"basics/05_autograd.html#gradient-for-non-leaf-nodes","text":"By default, only gradients of leaf nodes with requires_grad=True will be retained. The 'grad' of a non-leaf node is automatically freed during the calling of 'backward' and cannot be viewed. Tensor.retain_grad() can be called to retain and view the 'grad' of a non-leaf node. from math import pi n1 = flow . tensor ( pi / 2 , requires_grad = True ) n2 = flow . sin ( n1 ) n2 . retain_grad () n3 = flow . pow ( n2 , 2 ) n3 . backward () print ( n1 . grad ) print ( n2 . grad ) we get \\(\\frac{\\partial n_3}{\\partial n_1}\\) and \\(\\frac{\\partial n_3}{\\partial n_2}\\) using the code above. Output: tensor(-8.7423e-08, dtype=oneflow.float32) tensor(2., dtype=oneflow.float32)","title":"Gradient for Non-leaf Nodes"},{"location":"basics/05_autograd.html#call-backward-multiple-times-on-a-computation-graph","text":"By default, we can only call backward() once for each computation graph. For example, the following code will raise an error: n1 = flow . tensor ( 10. , requires_grad = True ) n2 = flow . pow ( n1 , 2 ) n2 . backward () n2 . backward () Error message: Maybe you try to backward through the node a second time. Specify retain_graph=True when calling .backward() or autograd.grad() the first time. If we need to call backward() multiple times on the same computation graph, retain_graph needs to be True . n1 = flow . tensor ( 10. , requires_grad = True ) n2 = flow . pow ( n1 , 2 ) n2 . backward ( retain_graph = True ) print ( n1 . grad ) n2 . backward () print ( n1 . grad ) Output\uff1a tensor(20., dtype=oneflow.float32) tensor(40., dtype=oneflow.float32) The above output shows that OneFlow will accumulate the gradient calculated by backward() multiple times. By calling the zeros_() , we can clear the gradient: n1 = flow . tensor ( 10. , requires_grad = True ) n2 = flow . pow ( n1 , 2 ) n2 . backward ( retain_graph = True ) print ( n1 . grad ) n1 . grad . zeros_ () n2 . backward () print ( n1 . grad ) Output\uff1a tensor(20., dtype=oneflow.float32) tensor(20., dtype=oneflow.float32)","title":"Call backward() Multiple Times on a Computation Graph"},{"location":"basics/05_autograd.html#disabled-gradient-calculation","text":"By default, OneFlow will trace and calculate gradients of Tensors with requires_grad = Ture . However, in some cases, we don't need OneFlow to keep tracing gradients such as just wanting the forward pass for inference. Then we can use oneflow.no_grad() or oneflow.Tensor.detach() to set. z = flow . matmul ( x , w ) + b print ( z . requires_grad ) with flow . no_grad (): z = flow . matmul ( x , w ) + b print ( z . requires_grad ) Output\uff1a True False z_det = z . detach () print ( z_det . requires_grad ) Output\uff1a False","title":"Disabled Gradient Calculation"},{"location":"basics/05_autograd.html#gradients-for-non-scalar-outputs","text":"Usually, we call backward() on scalar loss . However, if loss is a tensor, an error will be raised when calling backward() on loss . x = flow . randn ( 1 , 2 , requires_grad = True ) y = 3 * x + 1 y . backward () Error message\uff1a Check failed: IsScalarTensor(*outputs.at(i)) Grad can be implicitly created only for scalar outputs We can get the gradient after y.sum() . x = flow . randn ( 1 , 2 , requires_grad = True ) y = 3 * x + 1 y = y . sum () y . backward () print ( x . grad ) Output\uff1a tensor([[3., 3.]], dtype=oneflow.float32) Please refer to the \"Further Reading\" section below for the analysis of the cause and solution of the error.","title":"Gradients for Non-Scalar Outputs"},{"location":"basics/05_autograd.html#further-reading","text":"There are two elements \\(x_1\\) and \\(x_2\\) in Tensor x , and two elements \\(y_1\\) and \\(y_2\\) in Tensor y . The relationship between them is: \\[ \\mathbf{x} = [x_1, x_2] \\] \\[ \\mathbf{y} = [y_1, y_2] = [3x_1+1, 3x_2+1] \\] We want to get \\(\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}\\) \\[ \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} = \\frac{[3x_1+1, 3x_2+1]}{[x_1, x_2]} \\] It doesn't make sense in mathematics, so of course an error is reported. In fact, when the user calls y.backward() , the result desired is usually: \\[ [\\frac{\\partial y_1}{\\partial x_1}, \\frac{\\partial y_2}{\\partial x_2}] \\] After call sum() on y : \\[ y = y_1 + y_2 = 3x_1 + 3x_2 + 2 \\] At this time, when calling backward() , the gradients of \\(x_1\\) and \\(x_2\\) can be calculated: \\[ \\frac{\\partial y}{\\partial x_1} = \\frac{\\partial 3x_1 + 3x_2 + 2}{\\partial x_1} = 3 \\] \\[ \\frac{\\partial y}{\\partial x_2} = \\frac{\\partial 3x_1 + 3x_2 + 2}{\\partial x_2} = 3 \\] In addition to using sum() , Vector Jacobian Product(VJP) is a more general method to calculate the gradient of the non-scalar root node. Using the above example, OneFlow will generate the Jacobian matrix according to the computation graph during the backpropagation process: \\[ J = \\begin{pmatrix} \\frac{\\partial y_1}{\\partial x_1} & \\frac{\\partial y_1}{\\partial x_2}\\\\ \\frac{\\partial y_2}{\\partial x_1} & \\frac{\\partial y_2}{\\partial x_2} \\end{pmatrix}\\\\ = \\begin{pmatrix} \\frac{\\partial y_1}{\\partial x_1} & 0 \\\\ 0 & \\frac{\\partial y_2}{\\partial x_2} \\end{pmatrix} \\] To calculate VJP, a vector \\(\\mathbf{v}\\) with the same size as \\(\\mathbf{y}\\) needs to be provided: \\[ \\begin{bmatrix} v_1\\\\ v_2 \\end{bmatrix} \\times \\begin{pmatrix} \\frac{\\partial y_1}{\\partial x_1} & 0 \\\\ 0 & \\frac{\\partial y_2}{\\partial x_2} \\end{pmatrix}= \\begin{bmatrix} v_1 \\frac{\\partial y_1}{\\partial x_1}\\\\ v_2 \\frac{\\partial y_2}{\\partial x_2} \\end{bmatrix} \\] If the vector \\(\\mathbf{v}\\) is the gradient of the upper layer in the backpropagation, the result of VJP is exactly the gradient required by the current layer. backward() can accept a tensor as a parameter, when the parameter is \\(\\mathbf{v}\\) in VJP. We can also use the following methods to find the gradient of a tensor: x = flow . randn ( 1 , 2 , requires_grad = True ) y = 3 * x + 1 y . backward ( flow . ones_like ( y )) print ( x . grad ) Output\uff1a tensor([[3., 3.]], dtype=oneflow.float32) External links Automatic Differentiation","title":"Further Reading"},{"location":"basics/06_optimization.html","text":"BACKPROPAGATION AND OPTIMIZER \u00b6 So far, we have learned how to use OneFlow to Dataset and DataLoader , Build Models , Autograd , and combine them so that we can train models by using backpropagation algorithms. In oneflow.optim , there are various optimizer s that simplify the code of back propagation. This article will first introduce the basic concepts of back propagation and then show you how to use the oneflow.optim class. Backpropagation by Numpy Code \u00b6 In order to make it easier for readers to understand the relationship between backpropagation and autograd, a training process of a simple model implemented with numpy is provided here: import numpy as np ITER_COUNT = 500 LR = 0.01 # Forward propagation def forward ( x , w ): return np . matmul ( x , w ) # Loss function def loss ( y_pred , y ): return (( y_pred - y ) ** 2 ) . sum () # Calculate gradient def gradient ( x , y , y_pred ): return np . matmul ( x . T , 2 * ( y_pred - y )) if __name__ == \"__main__\" : # Train: Y = 2*X1 + 3*X2 x = np . array ([[ 1 , 2 ], [ 2 , 3 ], [ 4 , 6 ], [ 3 , 1 ]], dtype = np . float32 ) y = np . array ([[ 8 ], [ 13 ], [ 26 ], [ 9 ]], dtype = np . float32 ) w = np . array ([[ 2 ], [ 1 ]], dtype = np . float32 ) # Training cycle for i in range ( 0 , ITER_COUNT ): y_pred = forward ( x , w ) l = loss ( y_pred , y ) if ( i + 1 ) % 50 == 0 : print ( f \" { i + 1 } / { 500 } loss: { l } \" ) grad = gradient ( x , y , y_pred ) w -= LR * grad print ( f \"w: { w } \" ) output\uff1a 50/500 loss:0.0034512376878410578 100/500 loss:1.965487399502308e-06 150/500 loss:1.05524122773204e-09 200/500 loss:3.865352482534945e-12 250/500 loss:3.865352482534945e-12 300/500 loss:3.865352482534945e-12 350/500 loss:3.865352482534945e-12 400/500 loss:3.865352482534945e-12 450/500 loss:3.865352482534945e-12 500/500 loss:3.865352482534945e-12 w:[[2.000001 ] [2.9999993]] Note that the loss function expression we selected is \\(\\sum (y_{p} - y)^2\\) , so the code for gradient of loss to parameter w is\uff1a def gradient ( x , y , y_pred ): return np . matmul ( x . T , 2 * ( y_pred - y )) SGD is used to update parameters\uff1a grad = gradient ( x , y , y_pred ) w -= LR * grad In summary, a complete iteration in the training includes the following steps: The model calculates the predicted value based on the input and parameters ( y_pred ) Calculate loss, which is the error between the predicted value and the label Calculate the gradient of loss to parameter Update parameter(s) 1 and 2 are forward propagation process; 3 and 4 are back propagation process. Hyperparameters \u00b6 Hyperparameters are parameters related to model training settings, which can affect the efficiency and results of model training.As in the above code ITER_COUNT , LR are hyperparameters. Using the optimizer class in oneflow.optim \u00b6 Using the optimizer class in oneflow.optim for back propagation will be more concise. First, prepare the data and model. The convenience of using Module is that you can place the hyperparameters in Module for management. import oneflow as flow x = flow . tensor ([[ 1 , 2 ], [ 2 , 3 ], [ 4 , 6 ], [ 3 , 1 ]], dtype = flow . float32 ) y = flow . tensor ([[ 8 ], [ 13 ], [ 26 ], [ 9 ]], dtype = flow . float32 ) class MyLrModule ( flow . nn . Module ): def __init__ ( self , lr , iter_count ): super () . __init__ () self . w = flow . nn . Parameter ( flow . tensor ([[ 2 ], [ 1 ]], dtype = flow . float32 )) self . lr = lr self . iter_count = iter_count def forward ( self , x ): return flow . matmul ( x , self . w ) model = MyLrModule ( 0.01 , 500 ) Loss function \u00b6 Then, select the loss function. OneFlow comes with a variety of loss functions. We choose MSELoss here\uff1a loss = flow . nn . MSELoss ( reduction = \"sum\" ) Construct Optimizer \u00b6 The logic of back propagation is wrapped in optimizer. We choose SGD here, You can choose other optimization algorithms as needed, such as Adam and AdamW . optimizer = flow . optim . SGD ( model . parameters (), model . lr ) When the optimizer is constructed, the model parameters and learning rate are given to SGD . Then the optimizer.step() is called, and it automatically completes the gradient of the model parameters and updates the model parameters according to the SGD algorithm. Train \u00b6 When the above preparations are completed, we can start training: for i in range ( 0 , model . iter_count ): y_pred = model ( x ) l = loss ( y_pred , y ) if ( i + 1 ) % 50 == 0 : print ( f \" { i + 1 } / { model . iter_count } loss: { l . numpy () } \" ) optimizer . zero_grad () l . backward () optimizer . step () print ( f \" \\n w: { model . w } \" ) output\uff1a 50/500 loss:0.003451163647696376 100/500 loss:1.965773662959691e-06 150/500 loss:1.103217073250562e-09 200/500 loss:3.865352482534945e-12 250/500 loss:3.865352482534945e-12 300/500 loss:3.865352482534945e-12 350/500 loss:3.865352482534945e-12 400/500 loss:3.865352482534945e-12 450/500 loss:3.865352482534945e-12 500/500 loss:3.865352482534945e-12 w: tensor([[2.], [3.]], dtype=oneflow.float32, grad_fn=<accumulate_grad>)","title":"Backpropagation and Optimizer"},{"location":"basics/06_optimization.html#backpropagation-and-optimizer","text":"So far, we have learned how to use OneFlow to Dataset and DataLoader , Build Models , Autograd , and combine them so that we can train models by using backpropagation algorithms. In oneflow.optim , there are various optimizer s that simplify the code of back propagation. This article will first introduce the basic concepts of back propagation and then show you how to use the oneflow.optim class.","title":"BACKPROPAGATION AND OPTIMIZER"},{"location":"basics/06_optimization.html#backpropagation-by-numpy-code","text":"In order to make it easier for readers to understand the relationship between backpropagation and autograd, a training process of a simple model implemented with numpy is provided here: import numpy as np ITER_COUNT = 500 LR = 0.01 # Forward propagation def forward ( x , w ): return np . matmul ( x , w ) # Loss function def loss ( y_pred , y ): return (( y_pred - y ) ** 2 ) . sum () # Calculate gradient def gradient ( x , y , y_pred ): return np . matmul ( x . T , 2 * ( y_pred - y )) if __name__ == \"__main__\" : # Train: Y = 2*X1 + 3*X2 x = np . array ([[ 1 , 2 ], [ 2 , 3 ], [ 4 , 6 ], [ 3 , 1 ]], dtype = np . float32 ) y = np . array ([[ 8 ], [ 13 ], [ 26 ], [ 9 ]], dtype = np . float32 ) w = np . array ([[ 2 ], [ 1 ]], dtype = np . float32 ) # Training cycle for i in range ( 0 , ITER_COUNT ): y_pred = forward ( x , w ) l = loss ( y_pred , y ) if ( i + 1 ) % 50 == 0 : print ( f \" { i + 1 } / { 500 } loss: { l } \" ) grad = gradient ( x , y , y_pred ) w -= LR * grad print ( f \"w: { w } \" ) output\uff1a 50/500 loss:0.0034512376878410578 100/500 loss:1.965487399502308e-06 150/500 loss:1.05524122773204e-09 200/500 loss:3.865352482534945e-12 250/500 loss:3.865352482534945e-12 300/500 loss:3.865352482534945e-12 350/500 loss:3.865352482534945e-12 400/500 loss:3.865352482534945e-12 450/500 loss:3.865352482534945e-12 500/500 loss:3.865352482534945e-12 w:[[2.000001 ] [2.9999993]] Note that the loss function expression we selected is \\(\\sum (y_{p} - y)^2\\) , so the code for gradient of loss to parameter w is\uff1a def gradient ( x , y , y_pred ): return np . matmul ( x . T , 2 * ( y_pred - y )) SGD is used to update parameters\uff1a grad = gradient ( x , y , y_pred ) w -= LR * grad In summary, a complete iteration in the training includes the following steps: The model calculates the predicted value based on the input and parameters ( y_pred ) Calculate loss, which is the error between the predicted value and the label Calculate the gradient of loss to parameter Update parameter(s) 1 and 2 are forward propagation process; 3 and 4 are back propagation process.","title":"Backpropagation by Numpy Code"},{"location":"basics/06_optimization.html#hyperparameters","text":"Hyperparameters are parameters related to model training settings, which can affect the efficiency and results of model training.As in the above code ITER_COUNT , LR are hyperparameters.","title":"Hyperparameters"},{"location":"basics/06_optimization.html#using-the-optimizer-class-in-oneflowoptim","text":"Using the optimizer class in oneflow.optim for back propagation will be more concise. First, prepare the data and model. The convenience of using Module is that you can place the hyperparameters in Module for management. import oneflow as flow x = flow . tensor ([[ 1 , 2 ], [ 2 , 3 ], [ 4 , 6 ], [ 3 , 1 ]], dtype = flow . float32 ) y = flow . tensor ([[ 8 ], [ 13 ], [ 26 ], [ 9 ]], dtype = flow . float32 ) class MyLrModule ( flow . nn . Module ): def __init__ ( self , lr , iter_count ): super () . __init__ () self . w = flow . nn . Parameter ( flow . tensor ([[ 2 ], [ 1 ]], dtype = flow . float32 )) self . lr = lr self . iter_count = iter_count def forward ( self , x ): return flow . matmul ( x , self . w ) model = MyLrModule ( 0.01 , 500 )","title":"Using the optimizer class in oneflow.optim"},{"location":"basics/06_optimization.html#loss-function","text":"Then, select the loss function. OneFlow comes with a variety of loss functions. We choose MSELoss here\uff1a loss = flow . nn . MSELoss ( reduction = \"sum\" )","title":"Loss function"},{"location":"basics/06_optimization.html#construct-optimizer","text":"The logic of back propagation is wrapped in optimizer. We choose SGD here, You can choose other optimization algorithms as needed, such as Adam and AdamW . optimizer = flow . optim . SGD ( model . parameters (), model . lr ) When the optimizer is constructed, the model parameters and learning rate are given to SGD . Then the optimizer.step() is called, and it automatically completes the gradient of the model parameters and updates the model parameters according to the SGD algorithm.","title":"Construct Optimizer"},{"location":"basics/06_optimization.html#train","text":"When the above preparations are completed, we can start training: for i in range ( 0 , model . iter_count ): y_pred = model ( x ) l = loss ( y_pred , y ) if ( i + 1 ) % 50 == 0 : print ( f \" { i + 1 } / { model . iter_count } loss: { l . numpy () } \" ) optimizer . zero_grad () l . backward () optimizer . step () print ( f \" \\n w: { model . w } \" ) output\uff1a 50/500 loss:0.003451163647696376 100/500 loss:1.965773662959691e-06 150/500 loss:1.103217073250562e-09 200/500 loss:3.865352482534945e-12 250/500 loss:3.865352482534945e-12 300/500 loss:3.865352482534945e-12 350/500 loss:3.865352482534945e-12 400/500 loss:3.865352482534945e-12 450/500 loss:3.865352482534945e-12 500/500 loss:3.865352482534945e-12 w: tensor([[2.], [3.]], dtype=oneflow.float32, grad_fn=<accumulate_grad>)","title":"Train"},{"location":"basics/07_model_load_save.html","text":"SAVE AND LOAD THE MODEL \u00b6 There are two common uses for loading and saving models: Save the model that has been trained to continue training next time. Save the trained model for direct prediction in the future. We will introduce how to use save and load to save and load models as follows. Also, we will show how to load a pre-trained model for inference. Saving and Loading Model Parameters \u00b6 Module provided by OneFlow and defined by users provides the state_dict method to obtain all the model parameters, which is stored in a dictionary with the format \"name-value\". import oneflow as flow m = flow . nn . Linear ( 2 , 3 ) print ( m . state_dict ()) The above code first constructs a Linear object, then prints its parameters. OrderedDict([('weight', tensor([[-0.4297, -0.3571], [ 0.6797, -0.5295], [ 0.4918, -0.3039]], dtype=oneflow.float32, requires_grad=True)), ('bias', tensor([ 0.0977, 0.1219, -0.5372], dtype=oneflow.float32, requires_grad=True))]) We can load parameters by calling load_state_dict method of Module , as the following code: myparams = { \"weight\" : flow . ones ( 3 , 2 ), \"bias\" : flow . zeros ( 3 )} m . load_state_dict ( myparams ) print ( m . state_dict ()) The tensor in the dictionary created by us has been loaded into m Module: OrderedDict([('weight', tensor([[1., 1.], [1., 1.], [1., 1.]], dtype=oneflow.float32, requires_grad=True)), ('bias', tensor([0., 0., 0.], dtype=oneflow.float32, requires_grad=True))]) Saving Models \u00b6 We can use oneflow.save to save models. flow . save ( m . state_dict (), \"./model\" ) The first parameter is the Module parameters, and the second is the saved path. The above code saves the parameters of the m Module object to the path ./model . Loading Models \u00b6 Using oneflow.load to load parameters from disk to memory with the specified path, and get the dictionary of the parameters. params = flow . load ( \"./model\" ) Then use load_state_dict to load the dictionary into the model. m2 = flow . nn . Linear ( 2 , 3 ) m2 . load_state_dict ( params ) print ( m2 . state_dict ()) We have created a new Linear Module object m2 , and loaded the parameters saved from the above to m2 . Then we get the output as below: OrderedDict([('weight', tensor([[1., 1.], [1., 1.], [1., 1.]], dtype=oneflow.float32, requires_grad=True)), ('bias', tensor([0., 0., 0.], dtype=oneflow.float32, requires_grad=True))]) Using a Pre-trained Model to Make Predictions \u00b6 OneFlow can directly load PyTorch's pre-trained model for prediction as long as the structure and parameter names of the model are aligned with the PyTorch model. Examples can be found in here . Run commands below for trying how to use the pre-trained model to make predictions: git clone https://github.com/Oneflow-Inc/models.git cd models/shufflenetv2 bash infer.sh","title":"Model saving and loading"},{"location":"basics/07_model_load_save.html#save-and-load-the-model","text":"There are two common uses for loading and saving models: Save the model that has been trained to continue training next time. Save the trained model for direct prediction in the future. We will introduce how to use save and load to save and load models as follows. Also, we will show how to load a pre-trained model for inference.","title":"SAVE AND LOAD THE MODEL"},{"location":"basics/07_model_load_save.html#saving-and-loading-model-parameters","text":"Module provided by OneFlow and defined by users provides the state_dict method to obtain all the model parameters, which is stored in a dictionary with the format \"name-value\". import oneflow as flow m = flow . nn . Linear ( 2 , 3 ) print ( m . state_dict ()) The above code first constructs a Linear object, then prints its parameters. OrderedDict([('weight', tensor([[-0.4297, -0.3571], [ 0.6797, -0.5295], [ 0.4918, -0.3039]], dtype=oneflow.float32, requires_grad=True)), ('bias', tensor([ 0.0977, 0.1219, -0.5372], dtype=oneflow.float32, requires_grad=True))]) We can load parameters by calling load_state_dict method of Module , as the following code: myparams = { \"weight\" : flow . ones ( 3 , 2 ), \"bias\" : flow . zeros ( 3 )} m . load_state_dict ( myparams ) print ( m . state_dict ()) The tensor in the dictionary created by us has been loaded into m Module: OrderedDict([('weight', tensor([[1., 1.], [1., 1.], [1., 1.]], dtype=oneflow.float32, requires_grad=True)), ('bias', tensor([0., 0., 0.], dtype=oneflow.float32, requires_grad=True))])","title":"Saving and Loading Model Parameters"},{"location":"basics/07_model_load_save.html#saving-models","text":"We can use oneflow.save to save models. flow . save ( m . state_dict (), \"./model\" ) The first parameter is the Module parameters, and the second is the saved path. The above code saves the parameters of the m Module object to the path ./model .","title":"Saving Models"},{"location":"basics/07_model_load_save.html#loading-models","text":"Using oneflow.load to load parameters from disk to memory with the specified path, and get the dictionary of the parameters. params = flow . load ( \"./model\" ) Then use load_state_dict to load the dictionary into the model. m2 = flow . nn . Linear ( 2 , 3 ) m2 . load_state_dict ( params ) print ( m2 . state_dict ()) We have created a new Linear Module object m2 , and loaded the parameters saved from the above to m2 . Then we get the output as below: OrderedDict([('weight', tensor([[1., 1.], [1., 1.], [1., 1.]], dtype=oneflow.float32, requires_grad=True)), ('bias', tensor([0., 0., 0.], dtype=oneflow.float32, requires_grad=True))])","title":"Loading Models"},{"location":"basics/07_model_load_save.html#using-a-pre-trained-model-to-make-predictions","text":"OneFlow can directly load PyTorch's pre-trained model for prediction as long as the structure and parameter names of the model are aligned with the PyTorch model. Examples can be found in here . Run commands below for trying how to use the pre-trained model to make predictions: git clone https://github.com/Oneflow-Inc/models.git cd models/shufflenetv2 bash infer.sh","title":"Using a Pre-trained Model to Make Predictions"},{"location":"basics/08_nn_graph.html","text":"STATIC GRAPH INTERFACE: NN.GRAPH \u00b6 At present, there are two ways to run models in deep learning framework, Dynamic Graph and Static Graph , which are also called Eager Mode and Graph Mode in OneFlow. There are pros and cons to both approaches, and OneFlow offers support for both, with the Eager Mode by default. If you are reading the tutorials for this basic topic in order, then all the code you have encountered so far is in Eager Mode. In general, dynamic graphs are easier to use and static graphs have better performance. OneFlow offers nn.Graph , so that users can use the eager-like programming style to build static graphs and train the models. Eager Mode in OneFlow \u00b6 OneFlow runs in Eager Mode by default. The following script, using polynomial \\(y=a+bx+cx^2+dx^3\\) to fit the sine function \\(y=sin(x)\\) , finds a set of approximate fitting parameters \\(a\\) , \\(b\\) , \\(c\\) , \\(d\\) . This example was introduced to show how Eager Mode and Graph Mode are related in OneFlow (most of the code is reusable). Readers may be very familiar with OneFlow's Eager Mode now, here we do not explain in detail, interested readers can click on \"Code\" to expand the Code. Note: This sample code is adapted from PyTorch official tutorial . Code import math import numpy as np import oneflow as flow device = flow . device ( \"cuda\" ) dtype = flow . float32 # Create Tensors to hold input and outputs. x = flow . tensor ( np . linspace ( - math . pi , math . pi , 2000 ), device = device , dtype = dtype ) y = flow . tensor ( np . sin ( x ), device = device , dtype = dtype ) # For this example, the output y is a linear function of (x, x^2, x^3), so # we can consider it as a linear layer neural network. Let's prepare the # tensor (x, x^2, x^3). xx = flow . cat ( [ x . unsqueeze ( - 1 ) . pow ( 1 ), x . unsqueeze ( - 1 ) . pow ( 2 ), x . unsqueeze ( - 1 ) . pow ( 3 )], dim = 1 ) # The Linear Module model = flow . nn . Sequential ( flow . nn . Linear ( 3 , 1 ), flow . nn . Flatten ( 0 , 1 )) model . to ( device ) # Loss Function loss_fn = flow . nn . MSELoss ( reduction = \"sum\" ) loss_fn . to ( device ) # Optimizer optimizer = flow . optim . SGD ( model . parameters (), lr = 1e-6 ) for t in range ( 2000 ): # Forward pass: compute predicted y by passing x to the model. y_pred = model ( xx ) # Compute and print loss. loss = loss_fn ( y_pred , y ) if t % 100 == 99 : print ( t , loss . numpy ()) # Use the optimizer object to zero all of the gradients for the variables # it will update (which are the learnable weights of the model). optimizer . zero_grad () # Backward pass: compute gradient of the loss with respect to model # parameters. loss . backward () # Calling the step function on an Optimizer makes an update to its # parameters. optimizer . step () linear_layer = model [ 0 ] print ( f \"Result: y = { linear_layer . bias . numpy ()[ 0 ] } + { linear_layer . weight [:, 0 ] . numpy ()[ 0 ] } *x + { linear_layer . weight [:, 1 ] . numpy ()[ 0 ] } *x^2 + { linear_layer . weight [:, 2 ] . numpy ()[ 0 ] } *x^3\" ) Out: 99 582.7045 ... 1799 9.326502 1899 9.154123 1999 9.040091 Result: y = -0.0013652867637574673 + 0.8422811627388*x + 0.0002355352626182139*x^2 + -0.09127362817525864*x^3 Graph Mode in OneFlow \u00b6 Customize a Graph \u00b6 OneFlow provide the base class nn.Graph , which can be inherited to create a customized Graph class. import oneflow as flow import oneflow.nn as nn class MyLinear ( nn . Graph ): def __init__ ( self , in_features , out_features ): super () . __init__ () self . weight = nn . Parameter ( flow . randn ( in_features , out_features )) self . bias = nn . Parameter ( flow . randn ( out_features )) def build ( self , input ): return flow . matmul ( input , self . weight ) + self . bias The simple example above contains the important steps needed to customize a Graph: Inherits nn.Graph . Call super().__init__() at the begining of __init__ method to get OneFlow to do the necessary initialization for the Graph. Defines the structure and state of a neural network in __init__ method. Describes the computational process in build method. You can then instantiate and call the Graph: mygraph = MyLinear ( 4 , 3 ) input = flow . randn ( 1 , 4 ) out = mygraph ( input ) print ( out ) Out: tensor([[ 4.0638, -1.4453, 3.9640]], dtype=oneflow.float32) Note that Graph is similar to Module in that the object itself is callable and it is not recommended to explicitly call the build method. The definition of a Graph is very similar to the use of a Module, in fact, Graph can directly reuse a defined Module. Users can refer the content in Build Network directly about how to build a neural network in Graph Mode. For example, use the model above as the network structure: class ModelGraph ( flow . nn . Graph ): def __init__ ( self ): super () . __init__ () self . model = model def build ( self , x , y ): y_pred = self . model ( x ) return loss model_graph = ModelGraph () The major difference between Module and Graph is that Graph uses build method rather than forward method to describe the computation process, because the build method can contain not only forward computation, but also setting loss , optimizer, etc. You will see an example of using Graph for training later. Inference in Graph Mode \u00b6 The following example for inference in Graph Mode directly using the model, which we have already trained in Eager Mode at the beginning of this article. class LinearPredictGraph ( flow . nn . Graph ): def __init__ ( self ): super () . __init__ () self . model = model def build ( self , x ): return self . model ( x ) linear_graph = LinearPredictGraph () y_fit = linear_graph ( xx ) Draw the differences between the original function outputs and the fitting results: import matplotlib.pyplot as plt plt . plot ( x . numpy (), y . numpy ()) plt . plot ( x . numpy (), y_fit . numpy ()) Training in Graph Mode \u00b6 The Graph can be used for training. Click on the \"Code\" below to see the detailed code. Code import math import numpy as np import oneflow as flow device = flow . device ( \"cuda\" ) dtype = flow . float32 # Create Tensors to hold input and outputs. x = flow . tensor ( np . linspace ( - math . pi , math . pi , 2000 ), device = device , dtype = dtype ) y = flow . tensor ( np . sin ( x ), device = device , dtype = dtype ) # For this example, the output y is a linear function of (x, x^2, x^3), so # we can consider it as a linear layer neural network. Let's prepare the # tensor (x, x^2, x^3). xx = flow . cat ( [ x . unsqueeze ( - 1 ) . pow ( 1 ), x . unsqueeze ( - 1 ) . pow ( 2 ), x . unsqueeze ( - 1 ) . pow ( 3 )], dim = 1 ) # The Linear Module model = flow . nn . Sequential ( flow . nn . Linear ( 3 , 1 ), flow . nn . Flatten ( 0 , 1 )) model . to ( device ) # Loss Function loss_fn = flow . nn . MSELoss ( reduction = \"sum\" ) loss_fn . to ( device ) # Optimizer optimizer = flow . optim . SGD ( model . parameters (), lr = 1e-6 ) # The Linear Train Graph class LinearTrainGraph ( flow . nn . Graph ): def __init__ ( self ): super () . __init__ () self . model = model self . loss_fn = loss_fn self . add_optimizer ( optimizer ) def build ( self , x , y ): y_pred = self . model ( x ) loss = self . loss_fn ( y_pred , y ) loss . backward () return loss linear_graph = LinearTrainGraph () # linear_graph.debug() for t in range ( 2000 ): # Print loss. loss = linear_graph ( xx , y ) if t % 100 == 99 : print ( t , loss . numpy ()) linear_layer = model [ 0 ] print ( f \"Result: y = { linear_layer . bias . numpy () } + { linear_layer . weight [:, 0 ] . numpy () } x + { linear_layer . weight [:, 1 ] . numpy () } x^2 + { linear_layer . weight [:, 2 ] . numpy () } x^3\" ) Comparing to inference, there are only a few things that are unique to training: # Optimizer optimizer = flow . optim . SGD ( model . parameters (), lr = 1e-6 ) # (1) # The Linear Train Graph class LinearTrainGraph ( flow . nn . Graph ): def __init__ ( self ): #... self . add_optimizer ( optimizer ) # (2) def build ( self , x , y ): #... loss . backward () # (3) #... Constructing the optimizer object, which is same to the training in Eager Mode introduced in Backpropagation and Optimizer . Call self.add_optimizer in Graph's __init__ method to add the optimizer object constructed in the previous step to the Graph. Call backward in Graph's build to trigger back propagation. Debugging in Graph Mode \u00b6 You can call print to show information about the Graph object. print ( linear_graph ) The output is slightly different depending on whether the Graph object is called: If you use print before the Graph object is called, the output is information about the network structure. The output for print used before linear_graph is called is like this: (GRAPH:LinearTrainGraph_0:LinearTrainGraph): ( (MODULE:model:Sequential()): ( (MODULE:model.0:Linear(in_features=3, out_features=1, bias=True)): ( (PARAMETER:model.0.weight:tensor(..., device='cuda:0', size=(1, 3), dtype=oneflow.float32, requires_grad=True)): () (PARAMETER:model.0.bias:tensor(..., device='cuda:0', size=(1,), dtype=oneflow.float32, requires_grad=True)): () ) (MODULE:model.1:Flatten(start_dim=0, end_dim=1)): () ) (MODULE:loss_fn:MSELoss()): () ) If you use print after the Graph object is called, in addition to the structure of the network, it will print inputs and outputs of the tensors, the output on the console is like this: (GRAPH:LinearTrainGraph_0:LinearTrainGraph): ( (INPUT:_LinearTrainGraph_0-input_0:tensor(..., device='cuda:0', size=(2000, 3), dtype=oneflow.float32)) (INPUT:_LinearTrainGraph_0-input_1:tensor(..., device='cuda:0', size=(2000,), dtype=oneflow.float32)) (MODULE:model:Sequential()): ( (INPUT:_model-input_0:tensor(..., device='cuda:0', is_lazy='True', size=(2000, 3), dtype=oneflow.float32)) (MODULE:model.0:Linear(in_features=3, out_features=1, bias=True)): ( (INPUT:_model.0-input_0:tensor(..., device='cuda:0', is_lazy='True', size=(2000, 3), dtype=oneflow.float32)) (PARAMETER:model.0.weight:tensor(..., device='cuda:0', size=(1, 3), dtype=oneflow.float32, requires_grad=True)): () (PARAMETER:model.0.bias:tensor(..., device='cuda:0', size=(1,), dtype=oneflow.float32, requires_grad=True)): () (OUTPUT:_model.0-output_0:tensor(..., device='cuda:0', is_lazy='True', size=(2000, 1), dtype=oneflow.float32)) ) (MODULE:model.1:Flatten(start_dim=0, end_dim=1)): ( (INPUT:_model.1-input_0:tensor(..., device='cuda:0', is_lazy='True', size=(2000, 1), dtype=oneflow.float32)) (OUTPUT:_model.1-output_0:tensor(..., device='cuda:0', is_lazy='True', size=(2000,), dtype=oneflow.float32)) ) (OUTPUT:_model-output_0:tensor(..., device='cuda:0', is_lazy='True', size=(2000,), dtype=oneflow.float32)) ) (MODULE:loss_fn:MSELoss()): ( (INPUT:_loss_fn-input_0:tensor(..., device='cuda:0', is_lazy='True', size=(2000,), dtype=oneflow.float32)) (INPUT:_loss_fn-input_1:tensor(..., device='cuda:0', is_lazy='True', size=(2000,), dtype=oneflow.float32)) (OUTPUT:_loss_fn-output_0:tensor(..., device='cuda:0', is_lazy='True', size=(), dtype=oneflow.float32)) ) (OUTPUT:_LinearTrainGraph_0-output_0:tensor(..., device='cuda:0', is_lazy='True', size=(), dtype=oneflow.float32)) ) In addition, by calling the debug method of Graph objects, Graph\u2019s debug mode is turned on. OneFlow prints debug information when it compiles the computation graph. If the linear_graph.debug() is removed from the example code above, the output on the console is like this: Note that nn.Graph.debug() only print debug info on rank 0. (GRAPH:LinearTrainGraph_0:LinearTrainGraph) start building forward graph. (INPUT:_LinearTrainGraph_0-input_0:tensor(..., device='cuda:0', size=(20, 3), dtype=oneflow.float32)) (INPUT:_LinearTrainGraph_0-input_1:tensor(..., device='cuda:0', size=(20,), dtype=oneflow.float32)) (MODULE:model:Sequential()) (INPUT:_model-input_0:tensor(..., device='cuda:0', is_lazy='True', size=(20, 3), dtype=oneflow.float32)) (MODULE:model.0:Linear(in_features=3, out_features=1, bias=True)) (INPUT:_model.0-input_0:tensor(..., device='cuda:0', is_lazy='True', size=(20, 3), dtype=oneflow.float32)) (PARAMETER:model.0.weight:tensor(..., device='cuda:0', size=(1, 3), dtype=oneflow.float32, requires_grad=True)) (PARAMETER:model.0.bias:tensor(..., device='cuda:0', size=(1,), dtype=oneflow.float32, requires_grad=True)) (OUTPUT:_model.0-output_0:tensor(..., device='cuda:0', is_lazy='True', size=(20, 1), dtype=oneflow.float32)) (MODULE:model.1:Flatten(start_dim=0, end_dim=1)) (INPUT:_model.1-input_0:tensor(..., device='cuda:0', is_lazy='True', size=(20, 1), dtype=oneflow.float32)) (OUTPUT:_model.1-output_0:tensor(..., device='cuda:0', is_lazy='True', size=(20,), dtype=oneflow.float32)) (OUTPUT:_model-output_0:tensor(..., device='cuda:0', is_lazy='True', size=(20,), dtype=oneflow.float32)) (MODULE:loss_fn:MSELoss()) (INPUT:_loss_fn-input_0:tensor(..., device='cuda:0', is_lazy='True', size=(20,), dtype=oneflow.float32)) (INPUT:_loss_fn-input_1:tensor(..., device='cuda:0', is_lazy='True', size=(20,), dtype=oneflow.float32)) (OUTPUT:_loss_fn-output_0:tensor(..., device='cuda:0', is_lazy='True', size=(), dtype=oneflow.float32)) (OUTPUT:_LinearTrainGraph_0-output_0:tensor(..., device='cuda:0', is_lazy='True', size=(), dtype=oneflow.float32)) (GRAPH:LinearTrainGraph_0:LinearTrainGraph) end building forward graph. (GRAPH:LinearTrainGraph_0:LinearTrainGraph) start compiling and init graph runtime. (GRAPH:LinearTrainGraph_0:LinearTrainGraph) end compiling and init graph rumtime. It displays the names of the layers in the computation graph and input/output tensor information, including shape, device information, data type, and so on. The advantage of using debug is that the debug information is composed and printed at the same time, which makes it easy to find the problem if there is any error in the graph building process. In addition to the methods described above, getting the parameters of the gradient during the training process, accessing to the learning rate and other functions are also under development and will come up soon. Further Reading: Dynamic Graph vs. Static Graph \u00b6 User-defined neural networks, are transformed by deep learning frameworks into computation graphs, like the example in Autograd : def loss ( y_pred , y ): return flow . sum ( 1 / 2 * ( y_pred - y ) ** 2 ) x = flow . ones ( 1 , 5 ) # \u8f93\u5165 w = flow . randn ( 5 , 3 , requires_grad = True ) b = flow . randn ( 1 , 3 , requires_grad = True ) z = flow . matmul ( x , w ) + b y = flow . zeros ( 1 , 3 ) # label l = loss ( z , y ) The corresponding computation graph is: Dynamic Graph The characteristic of dynamic graph is that it is defined by run. The code above is run like this (Note: the figure below merges simple statements): Because the dynamic graph is defined by run, it is very flexible and easy to debug. You can modify the graph structure at any time and get results immediately. However, the deep learning framework can not get the complete graph information(which can be changed at any time and can never be considered as finished), it can not make full global optimization, so its performance is relatively poor. Static Graph Unlike a dynamic graph, a static graph defines a complete computation graph. It requires the user to declare all compute nodes before the framework starts running. This can be understood as the framework acting as a compiler between the user code and the computation graph that ultimately runs. In the case of the OneFlow, the user\u2019s code is first converted to a full computation graph and then run by the OneFlow Runtime module. Static graph, which get the complete network first, then compile and run, can be optimized in a way that dynamic graph can not, so they have an advantage in performance. It is also easier to deploy across platforms after compiling the computation graph. However, when the actual computation takes place in a static graph, it is no longer directly related to the user\u2019s code, so debugging the static graph is not convenient. The two approaches can be summarized as follows: Dynamic Graph Static Graph Computation Mode Eager Mode Graph Mode Pros The code is flexible and easy to debug. Good performance, easy to optimize and deploy. Cons Poor performance and portability. Not easy to debug. The Eager Mode in OneFlow is aligned with the PyTorch, which allows users familiar with the PyTorch to get their hands on easily with no more effert. The Graph Mode in OneFlow is based on the object-oriented programming style, which allows developers familiar with eager programming style to benefit from static graph with minimal code changes. Related Links \u00b6 Building neural network in OneFlow Eager Mode: Build Network PyTorch version of polynomial fitting example: PyTorch: nn","title":"Static Graph Interface"},{"location":"basics/08_nn_graph.html#static-graph-interface-nngraph","text":"At present, there are two ways to run models in deep learning framework, Dynamic Graph and Static Graph , which are also called Eager Mode and Graph Mode in OneFlow. There are pros and cons to both approaches, and OneFlow offers support for both, with the Eager Mode by default. If you are reading the tutorials for this basic topic in order, then all the code you have encountered so far is in Eager Mode. In general, dynamic graphs are easier to use and static graphs have better performance. OneFlow offers nn.Graph , so that users can use the eager-like programming style to build static graphs and train the models.","title":"STATIC GRAPH INTERFACE: NN.GRAPH"},{"location":"basics/08_nn_graph.html#eager-mode-in-oneflow","text":"OneFlow runs in Eager Mode by default. The following script, using polynomial \\(y=a+bx+cx^2+dx^3\\) to fit the sine function \\(y=sin(x)\\) , finds a set of approximate fitting parameters \\(a\\) , \\(b\\) , \\(c\\) , \\(d\\) . This example was introduced to show how Eager Mode and Graph Mode are related in OneFlow (most of the code is reusable). Readers may be very familiar with OneFlow's Eager Mode now, here we do not explain in detail, interested readers can click on \"Code\" to expand the Code. Note: This sample code is adapted from PyTorch official tutorial . Code import math import numpy as np import oneflow as flow device = flow . device ( \"cuda\" ) dtype = flow . float32 # Create Tensors to hold input and outputs. x = flow . tensor ( np . linspace ( - math . pi , math . pi , 2000 ), device = device , dtype = dtype ) y = flow . tensor ( np . sin ( x ), device = device , dtype = dtype ) # For this example, the output y is a linear function of (x, x^2, x^3), so # we can consider it as a linear layer neural network. Let's prepare the # tensor (x, x^2, x^3). xx = flow . cat ( [ x . unsqueeze ( - 1 ) . pow ( 1 ), x . unsqueeze ( - 1 ) . pow ( 2 ), x . unsqueeze ( - 1 ) . pow ( 3 )], dim = 1 ) # The Linear Module model = flow . nn . Sequential ( flow . nn . Linear ( 3 , 1 ), flow . nn . Flatten ( 0 , 1 )) model . to ( device ) # Loss Function loss_fn = flow . nn . MSELoss ( reduction = \"sum\" ) loss_fn . to ( device ) # Optimizer optimizer = flow . optim . SGD ( model . parameters (), lr = 1e-6 ) for t in range ( 2000 ): # Forward pass: compute predicted y by passing x to the model. y_pred = model ( xx ) # Compute and print loss. loss = loss_fn ( y_pred , y ) if t % 100 == 99 : print ( t , loss . numpy ()) # Use the optimizer object to zero all of the gradients for the variables # it will update (which are the learnable weights of the model). optimizer . zero_grad () # Backward pass: compute gradient of the loss with respect to model # parameters. loss . backward () # Calling the step function on an Optimizer makes an update to its # parameters. optimizer . step () linear_layer = model [ 0 ] print ( f \"Result: y = { linear_layer . bias . numpy ()[ 0 ] } + { linear_layer . weight [:, 0 ] . numpy ()[ 0 ] } *x + { linear_layer . weight [:, 1 ] . numpy ()[ 0 ] } *x^2 + { linear_layer . weight [:, 2 ] . numpy ()[ 0 ] } *x^3\" ) Out: 99 582.7045 ... 1799 9.326502 1899 9.154123 1999 9.040091 Result: y = -0.0013652867637574673 + 0.8422811627388*x + 0.0002355352626182139*x^2 + -0.09127362817525864*x^3","title":"Eager Mode in OneFlow"},{"location":"basics/08_nn_graph.html#graph-mode-in-oneflow","text":"","title":"Graph Mode in OneFlow"},{"location":"basics/08_nn_graph.html#customize-a-graph","text":"OneFlow provide the base class nn.Graph , which can be inherited to create a customized Graph class. import oneflow as flow import oneflow.nn as nn class MyLinear ( nn . Graph ): def __init__ ( self , in_features , out_features ): super () . __init__ () self . weight = nn . Parameter ( flow . randn ( in_features , out_features )) self . bias = nn . Parameter ( flow . randn ( out_features )) def build ( self , input ): return flow . matmul ( input , self . weight ) + self . bias The simple example above contains the important steps needed to customize a Graph: Inherits nn.Graph . Call super().__init__() at the begining of __init__ method to get OneFlow to do the necessary initialization for the Graph. Defines the structure and state of a neural network in __init__ method. Describes the computational process in build method. You can then instantiate and call the Graph: mygraph = MyLinear ( 4 , 3 ) input = flow . randn ( 1 , 4 ) out = mygraph ( input ) print ( out ) Out: tensor([[ 4.0638, -1.4453, 3.9640]], dtype=oneflow.float32) Note that Graph is similar to Module in that the object itself is callable and it is not recommended to explicitly call the build method. The definition of a Graph is very similar to the use of a Module, in fact, Graph can directly reuse a defined Module. Users can refer the content in Build Network directly about how to build a neural network in Graph Mode. For example, use the model above as the network structure: class ModelGraph ( flow . nn . Graph ): def __init__ ( self ): super () . __init__ () self . model = model def build ( self , x , y ): y_pred = self . model ( x ) return loss model_graph = ModelGraph () The major difference between Module and Graph is that Graph uses build method rather than forward method to describe the computation process, because the build method can contain not only forward computation, but also setting loss , optimizer, etc. You will see an example of using Graph for training later.","title":"Customize a Graph"},{"location":"basics/08_nn_graph.html#inference-in-graph-mode","text":"The following example for inference in Graph Mode directly using the model, which we have already trained in Eager Mode at the beginning of this article. class LinearPredictGraph ( flow . nn . Graph ): def __init__ ( self ): super () . __init__ () self . model = model def build ( self , x ): return self . model ( x ) linear_graph = LinearPredictGraph () y_fit = linear_graph ( xx ) Draw the differences between the original function outputs and the fitting results: import matplotlib.pyplot as plt plt . plot ( x . numpy (), y . numpy ()) plt . plot ( x . numpy (), y_fit . numpy ())","title":"Inference in Graph Mode"},{"location":"basics/08_nn_graph.html#training-in-graph-mode","text":"The Graph can be used for training. Click on the \"Code\" below to see the detailed code. Code import math import numpy as np import oneflow as flow device = flow . device ( \"cuda\" ) dtype = flow . float32 # Create Tensors to hold input and outputs. x = flow . tensor ( np . linspace ( - math . pi , math . pi , 2000 ), device = device , dtype = dtype ) y = flow . tensor ( np . sin ( x ), device = device , dtype = dtype ) # For this example, the output y is a linear function of (x, x^2, x^3), so # we can consider it as a linear layer neural network. Let's prepare the # tensor (x, x^2, x^3). xx = flow . cat ( [ x . unsqueeze ( - 1 ) . pow ( 1 ), x . unsqueeze ( - 1 ) . pow ( 2 ), x . unsqueeze ( - 1 ) . pow ( 3 )], dim = 1 ) # The Linear Module model = flow . nn . Sequential ( flow . nn . Linear ( 3 , 1 ), flow . nn . Flatten ( 0 , 1 )) model . to ( device ) # Loss Function loss_fn = flow . nn . MSELoss ( reduction = \"sum\" ) loss_fn . to ( device ) # Optimizer optimizer = flow . optim . SGD ( model . parameters (), lr = 1e-6 ) # The Linear Train Graph class LinearTrainGraph ( flow . nn . Graph ): def __init__ ( self ): super () . __init__ () self . model = model self . loss_fn = loss_fn self . add_optimizer ( optimizer ) def build ( self , x , y ): y_pred = self . model ( x ) loss = self . loss_fn ( y_pred , y ) loss . backward () return loss linear_graph = LinearTrainGraph () # linear_graph.debug() for t in range ( 2000 ): # Print loss. loss = linear_graph ( xx , y ) if t % 100 == 99 : print ( t , loss . numpy ()) linear_layer = model [ 0 ] print ( f \"Result: y = { linear_layer . bias . numpy () } + { linear_layer . weight [:, 0 ] . numpy () } x + { linear_layer . weight [:, 1 ] . numpy () } x^2 + { linear_layer . weight [:, 2 ] . numpy () } x^3\" ) Comparing to inference, there are only a few things that are unique to training: # Optimizer optimizer = flow . optim . SGD ( model . parameters (), lr = 1e-6 ) # (1) # The Linear Train Graph class LinearTrainGraph ( flow . nn . Graph ): def __init__ ( self ): #... self . add_optimizer ( optimizer ) # (2) def build ( self , x , y ): #... loss . backward () # (3) #... Constructing the optimizer object, which is same to the training in Eager Mode introduced in Backpropagation and Optimizer . Call self.add_optimizer in Graph's __init__ method to add the optimizer object constructed in the previous step to the Graph. Call backward in Graph's build to trigger back propagation.","title":"Training in Graph Mode"},{"location":"basics/08_nn_graph.html#debugging-in-graph-mode","text":"You can call print to show information about the Graph object. print ( linear_graph ) The output is slightly different depending on whether the Graph object is called: If you use print before the Graph object is called, the output is information about the network structure. The output for print used before linear_graph is called is like this: (GRAPH:LinearTrainGraph_0:LinearTrainGraph): ( (MODULE:model:Sequential()): ( (MODULE:model.0:Linear(in_features=3, out_features=1, bias=True)): ( (PARAMETER:model.0.weight:tensor(..., device='cuda:0', size=(1, 3), dtype=oneflow.float32, requires_grad=True)): () (PARAMETER:model.0.bias:tensor(..., device='cuda:0', size=(1,), dtype=oneflow.float32, requires_grad=True)): () ) (MODULE:model.1:Flatten(start_dim=0, end_dim=1)): () ) (MODULE:loss_fn:MSELoss()): () ) If you use print after the Graph object is called, in addition to the structure of the network, it will print inputs and outputs of the tensors, the output on the console is like this: (GRAPH:LinearTrainGraph_0:LinearTrainGraph): ( (INPUT:_LinearTrainGraph_0-input_0:tensor(..., device='cuda:0', size=(2000, 3), dtype=oneflow.float32)) (INPUT:_LinearTrainGraph_0-input_1:tensor(..., device='cuda:0', size=(2000,), dtype=oneflow.float32)) (MODULE:model:Sequential()): ( (INPUT:_model-input_0:tensor(..., device='cuda:0', is_lazy='True', size=(2000, 3), dtype=oneflow.float32)) (MODULE:model.0:Linear(in_features=3, out_features=1, bias=True)): ( (INPUT:_model.0-input_0:tensor(..., device='cuda:0', is_lazy='True', size=(2000, 3), dtype=oneflow.float32)) (PARAMETER:model.0.weight:tensor(..., device='cuda:0', size=(1, 3), dtype=oneflow.float32, requires_grad=True)): () (PARAMETER:model.0.bias:tensor(..., device='cuda:0', size=(1,), dtype=oneflow.float32, requires_grad=True)): () (OUTPUT:_model.0-output_0:tensor(..., device='cuda:0', is_lazy='True', size=(2000, 1), dtype=oneflow.float32)) ) (MODULE:model.1:Flatten(start_dim=0, end_dim=1)): ( (INPUT:_model.1-input_0:tensor(..., device='cuda:0', is_lazy='True', size=(2000, 1), dtype=oneflow.float32)) (OUTPUT:_model.1-output_0:tensor(..., device='cuda:0', is_lazy='True', size=(2000,), dtype=oneflow.float32)) ) (OUTPUT:_model-output_0:tensor(..., device='cuda:0', is_lazy='True', size=(2000,), dtype=oneflow.float32)) ) (MODULE:loss_fn:MSELoss()): ( (INPUT:_loss_fn-input_0:tensor(..., device='cuda:0', is_lazy='True', size=(2000,), dtype=oneflow.float32)) (INPUT:_loss_fn-input_1:tensor(..., device='cuda:0', is_lazy='True', size=(2000,), dtype=oneflow.float32)) (OUTPUT:_loss_fn-output_0:tensor(..., device='cuda:0', is_lazy='True', size=(), dtype=oneflow.float32)) ) (OUTPUT:_LinearTrainGraph_0-output_0:tensor(..., device='cuda:0', is_lazy='True', size=(), dtype=oneflow.float32)) ) In addition, by calling the debug method of Graph objects, Graph\u2019s debug mode is turned on. OneFlow prints debug information when it compiles the computation graph. If the linear_graph.debug() is removed from the example code above, the output on the console is like this: Note that nn.Graph.debug() only print debug info on rank 0. (GRAPH:LinearTrainGraph_0:LinearTrainGraph) start building forward graph. (INPUT:_LinearTrainGraph_0-input_0:tensor(..., device='cuda:0', size=(20, 3), dtype=oneflow.float32)) (INPUT:_LinearTrainGraph_0-input_1:tensor(..., device='cuda:0', size=(20,), dtype=oneflow.float32)) (MODULE:model:Sequential()) (INPUT:_model-input_0:tensor(..., device='cuda:0', is_lazy='True', size=(20, 3), dtype=oneflow.float32)) (MODULE:model.0:Linear(in_features=3, out_features=1, bias=True)) (INPUT:_model.0-input_0:tensor(..., device='cuda:0', is_lazy='True', size=(20, 3), dtype=oneflow.float32)) (PARAMETER:model.0.weight:tensor(..., device='cuda:0', size=(1, 3), dtype=oneflow.float32, requires_grad=True)) (PARAMETER:model.0.bias:tensor(..., device='cuda:0', size=(1,), dtype=oneflow.float32, requires_grad=True)) (OUTPUT:_model.0-output_0:tensor(..., device='cuda:0', is_lazy='True', size=(20, 1), dtype=oneflow.float32)) (MODULE:model.1:Flatten(start_dim=0, end_dim=1)) (INPUT:_model.1-input_0:tensor(..., device='cuda:0', is_lazy='True', size=(20, 1), dtype=oneflow.float32)) (OUTPUT:_model.1-output_0:tensor(..., device='cuda:0', is_lazy='True', size=(20,), dtype=oneflow.float32)) (OUTPUT:_model-output_0:tensor(..., device='cuda:0', is_lazy='True', size=(20,), dtype=oneflow.float32)) (MODULE:loss_fn:MSELoss()) (INPUT:_loss_fn-input_0:tensor(..., device='cuda:0', is_lazy='True', size=(20,), dtype=oneflow.float32)) (INPUT:_loss_fn-input_1:tensor(..., device='cuda:0', is_lazy='True', size=(20,), dtype=oneflow.float32)) (OUTPUT:_loss_fn-output_0:tensor(..., device='cuda:0', is_lazy='True', size=(), dtype=oneflow.float32)) (OUTPUT:_LinearTrainGraph_0-output_0:tensor(..., device='cuda:0', is_lazy='True', size=(), dtype=oneflow.float32)) (GRAPH:LinearTrainGraph_0:LinearTrainGraph) end building forward graph. (GRAPH:LinearTrainGraph_0:LinearTrainGraph) start compiling and init graph runtime. (GRAPH:LinearTrainGraph_0:LinearTrainGraph) end compiling and init graph rumtime. It displays the names of the layers in the computation graph and input/output tensor information, including shape, device information, data type, and so on. The advantage of using debug is that the debug information is composed and printed at the same time, which makes it easy to find the problem if there is any error in the graph building process. In addition to the methods described above, getting the parameters of the gradient during the training process, accessing to the learning rate and other functions are also under development and will come up soon.","title":"Debugging in Graph Mode"},{"location":"basics/08_nn_graph.html#further-reading-dynamic-graph-vs-static-graph","text":"User-defined neural networks, are transformed by deep learning frameworks into computation graphs, like the example in Autograd : def loss ( y_pred , y ): return flow . sum ( 1 / 2 * ( y_pred - y ) ** 2 ) x = flow . ones ( 1 , 5 ) # \u8f93\u5165 w = flow . randn ( 5 , 3 , requires_grad = True ) b = flow . randn ( 1 , 3 , requires_grad = True ) z = flow . matmul ( x , w ) + b y = flow . zeros ( 1 , 3 ) # label l = loss ( z , y ) The corresponding computation graph is: Dynamic Graph The characteristic of dynamic graph is that it is defined by run. The code above is run like this (Note: the figure below merges simple statements): Because the dynamic graph is defined by run, it is very flexible and easy to debug. You can modify the graph structure at any time and get results immediately. However, the deep learning framework can not get the complete graph information(which can be changed at any time and can never be considered as finished), it can not make full global optimization, so its performance is relatively poor. Static Graph Unlike a dynamic graph, a static graph defines a complete computation graph. It requires the user to declare all compute nodes before the framework starts running. This can be understood as the framework acting as a compiler between the user code and the computation graph that ultimately runs. In the case of the OneFlow, the user\u2019s code is first converted to a full computation graph and then run by the OneFlow Runtime module. Static graph, which get the complete network first, then compile and run, can be optimized in a way that dynamic graph can not, so they have an advantage in performance. It is also easier to deploy across platforms after compiling the computation graph. However, when the actual computation takes place in a static graph, it is no longer directly related to the user\u2019s code, so debugging the static graph is not convenient. The two approaches can be summarized as follows: Dynamic Graph Static Graph Computation Mode Eager Mode Graph Mode Pros The code is flexible and easy to debug. Good performance, easy to optimize and deploy. Cons Poor performance and portability. Not easy to debug. The Eager Mode in OneFlow is aligned with the PyTorch, which allows users familiar with the PyTorch to get their hands on easily with no more effert. The Graph Mode in OneFlow is based on the object-oriented programming style, which allows developers familiar with eager programming style to benefit from static graph with minimal code changes.","title":"Further Reading: Dynamic Graph vs. Static Graph"},{"location":"basics/08_nn_graph.html#related-links","text":"Building neural network in OneFlow Eager Mode: Build Network PyTorch version of polynomial fitting example: PyTorch: nn","title":"Related Links"},{"location":"parallelism/01_introduction.html","text":"COMMON DISTRIBUTED PARALLEL STRATIGES \u00b6 Why Distributed Training is Prevailing \u00b6 In recent years, deep learning has been widely used in various fields, including computer vision, language understanding, speech recognition, advertising recommendation and so on. A common feature in these different areas is the growing size of models, such as the GPT-3 model, which has 175 billion parameters. Even with 1,024 of 80 GB A100 cards, the full GPT-3 training time would take a month. The enlargement of model scale requires the development of hardware (computing power, memory). However, because of the existence of memory walls, the computational power and capacity of a single device, limited by the laws of physics, it is increasingly difficult to continuously improve the integration of chips and to keep up with the demands of model expansion. In order to solve the problem of insufficient increase speed of computing power, it is necessary to use multi-node cluster for distributed training in order to improve computing speed. Common Parallel Strategies \u00b6 Simply stacking machines does not increase computing power necessarily. Because the training of neural networks can not be simply \"divide the work done by one device to multiple ones\". It requires not only multiple devices to perform computing, but also data transmission between devices, only by coordinating the computing and communication in the cluster, can we do efficient distributed training. We will explain the difference between data parallelism and model parallelism with an example of matrix multiplication. Let's look at the following logical matrix multiplication examples: If a layer in the neural network is doing matrix multiplication, where the shape of the input \\(x\\) is \\(4\\times5\\) and the shape of the model parameter \\(w\\) is \\(5\\times8\\) , then the matrix multiplication output shape is \\(4\\times8\\) . The schematic diagram is as follows: In the single machine single card training situaiton, the above matrix multiplication first calculates \\(out\\) , passes \\(out\\) to the next layer, and finally calculates \\(loss\\) , and then in the backpropagation process, gets \\(\\frac{\\partial loss}{\\partial w}\\) , which then be used to update \\(w\\) . In distributed training, there are \" Data Parallelism \" and \" Model Parallelism \" strategies depending on whether \\(x\\) or \\(w\\) is partitioned. In the next section, we will introduce common strategies for parallelism. Data Paralelism \u00b6 Data parallelism slices \\(x\\) , while the model parameter \\(w\\) on each device is complete and consistent. As shown in the figure below, \\(x\\) is split evenly into two devices by dimension 0, each with a full \\(w\\) . In this way, the output on each device is only half the logical output, which shape is \\(2\\times8\\) . The output on both devices combind together to produce the logically complete output. Note that because the data is distributed to two devices, the backpropagation process will get different values for \\(\\frac{\\partial loss}{\\partial w}\\) , if the models are updated directly using the gradients on each device, it would cause the models on the two devices to be inconsistent, and the training would be meaningless(Which model should be used?). Therefore, in the process of backpropagation under data parallelism strategy, the gradients on each device should do AllReduce before use, which ensures the model on each device is always consistent. When the dataset is large and the model is small, and the communication cost for the gradients synchronization is small in the backpropagation process, so it is more advantageous to choose data parallelism in this situation. For example, the common vision classification model, such as ResNet50, is more suitable to use data parallelism strategy. Model Paralelism \u00b6 When the neural network is very large, the cost of gradients synchronization will be very high, moreover, the network may be too large to be stored in a single computing device, then the model parallelism strategy can be used. The so called model parallelism is that the data on each device is complete and consistent, while the model \\(w\\) is split into different devices, each device only has a part of the model, all the parts of model on the computing device put together forms the complete model. As shown in the figure below, \\(w\\) is split evenly into two devices by the first dimension, each with a full \\(x\\) . The output on both devices also needs to be combind together to get the logical output. The benefit of model parallelism is that it eliminates the gradient AllReduce between multiple devices. However, since each device requires complete input data, the input is broadcasted among multiple devices with some communication cost. For example, the \\(out~(4\\times8)\\) shown above needs to be broadcast to both devices if it is the input of the next layer. Language models, such as BERT, often use model parallelism. Pipelining Paralelism \u00b6 When the neural network is too large to be stored on a single device, in addition to the above parallel strategies, we can also choose pipelining parallel strategy. Pipelining paralelism divides the network into stages and places it to different computing devices, each of which completes the training in a \"relay\" manner. The figure below shows how to run with pipelining parallelism with a logical four-layer network ( T1 to T4 ). The four-layer network is divided into two computing devices, so that the T1 and T2 are placed to GPU0 and T3 and T4 are placed to GPU1 . After computing the first two layers on GPU0 , its output is treated as the input of GPU1 to continue computation of the next two layers. Hybrid Paralelism \u00b6 You can also mix with a variety of parallelism strategies when training a network, take GPT-3 as an example, the parallelism strategy for training could be like this: This large GPT network is partitioned into 64 stages, with each stage running on 6 DGX-A100s. The workload among the 6 machines is trained with data parallelism, while the workload among GPUs inside each machine is trained with model parallelism. The 3072 A100s in the entire cluster are divided into a matrix of \\(6\\times8\\times64\\) , and then train the model using data parallelism, model parallelism and pipeline parallelism simultaneously. The choice of parallelism strategy affects the efficiency of training. Whether the interface of framework supports parallelism well also determines the efficiency of algorithm engineer. OneFlow's system-level design and innovation for distributed training will help users to get comfortable well with distributed training. The related examples will be shown in other articles on this topic.","title":"Common Parallel Strategies"},{"location":"parallelism/01_introduction.html#common-distributed-parallel-stratiges","text":"","title":"COMMON DISTRIBUTED PARALLEL STRATIGES"},{"location":"parallelism/01_introduction.html#why-distributed-training-is-prevailing","text":"In recent years, deep learning has been widely used in various fields, including computer vision, language understanding, speech recognition, advertising recommendation and so on. A common feature in these different areas is the growing size of models, such as the GPT-3 model, which has 175 billion parameters. Even with 1,024 of 80 GB A100 cards, the full GPT-3 training time would take a month. The enlargement of model scale requires the development of hardware (computing power, memory). However, because of the existence of memory walls, the computational power and capacity of a single device, limited by the laws of physics, it is increasingly difficult to continuously improve the integration of chips and to keep up with the demands of model expansion. In order to solve the problem of insufficient increase speed of computing power, it is necessary to use multi-node cluster for distributed training in order to improve computing speed.","title":"Why Distributed Training is Prevailing"},{"location":"parallelism/01_introduction.html#common-parallel-strategies","text":"Simply stacking machines does not increase computing power necessarily. Because the training of neural networks can not be simply \"divide the work done by one device to multiple ones\". It requires not only multiple devices to perform computing, but also data transmission between devices, only by coordinating the computing and communication in the cluster, can we do efficient distributed training. We will explain the difference between data parallelism and model parallelism with an example of matrix multiplication. Let's look at the following logical matrix multiplication examples: If a layer in the neural network is doing matrix multiplication, where the shape of the input \\(x\\) is \\(4\\times5\\) and the shape of the model parameter \\(w\\) is \\(5\\times8\\) , then the matrix multiplication output shape is \\(4\\times8\\) . The schematic diagram is as follows: In the single machine single card training situaiton, the above matrix multiplication first calculates \\(out\\) , passes \\(out\\) to the next layer, and finally calculates \\(loss\\) , and then in the backpropagation process, gets \\(\\frac{\\partial loss}{\\partial w}\\) , which then be used to update \\(w\\) . In distributed training, there are \" Data Parallelism \" and \" Model Parallelism \" strategies depending on whether \\(x\\) or \\(w\\) is partitioned. In the next section, we will introduce common strategies for parallelism.","title":"Common Parallel Strategies"},{"location":"parallelism/01_introduction.html#data-paralelism","text":"Data parallelism slices \\(x\\) , while the model parameter \\(w\\) on each device is complete and consistent. As shown in the figure below, \\(x\\) is split evenly into two devices by dimension 0, each with a full \\(w\\) . In this way, the output on each device is only half the logical output, which shape is \\(2\\times8\\) . The output on both devices combind together to produce the logically complete output. Note that because the data is distributed to two devices, the backpropagation process will get different values for \\(\\frac{\\partial loss}{\\partial w}\\) , if the models are updated directly using the gradients on each device, it would cause the models on the two devices to be inconsistent, and the training would be meaningless(Which model should be used?). Therefore, in the process of backpropagation under data parallelism strategy, the gradients on each device should do AllReduce before use, which ensures the model on each device is always consistent. When the dataset is large and the model is small, and the communication cost for the gradients synchronization is small in the backpropagation process, so it is more advantageous to choose data parallelism in this situation. For example, the common vision classification model, such as ResNet50, is more suitable to use data parallelism strategy.","title":"Data Paralelism"},{"location":"parallelism/01_introduction.html#model-paralelism","text":"When the neural network is very large, the cost of gradients synchronization will be very high, moreover, the network may be too large to be stored in a single computing device, then the model parallelism strategy can be used. The so called model parallelism is that the data on each device is complete and consistent, while the model \\(w\\) is split into different devices, each device only has a part of the model, all the parts of model on the computing device put together forms the complete model. As shown in the figure below, \\(w\\) is split evenly into two devices by the first dimension, each with a full \\(x\\) . The output on both devices also needs to be combind together to get the logical output. The benefit of model parallelism is that it eliminates the gradient AllReduce between multiple devices. However, since each device requires complete input data, the input is broadcasted among multiple devices with some communication cost. For example, the \\(out~(4\\times8)\\) shown above needs to be broadcast to both devices if it is the input of the next layer. Language models, such as BERT, often use model parallelism.","title":"Model Paralelism"},{"location":"parallelism/01_introduction.html#pipelining-paralelism","text":"When the neural network is too large to be stored on a single device, in addition to the above parallel strategies, we can also choose pipelining parallel strategy. Pipelining paralelism divides the network into stages and places it to different computing devices, each of which completes the training in a \"relay\" manner. The figure below shows how to run with pipelining parallelism with a logical four-layer network ( T1 to T4 ). The four-layer network is divided into two computing devices, so that the T1 and T2 are placed to GPU0 and T3 and T4 are placed to GPU1 . After computing the first two layers on GPU0 , its output is treated as the input of GPU1 to continue computation of the next two layers.","title":"Pipelining Paralelism"},{"location":"parallelism/01_introduction.html#hybrid-paralelism","text":"You can also mix with a variety of parallelism strategies when training a network, take GPT-3 as an example, the parallelism strategy for training could be like this: This large GPT network is partitioned into 64 stages, with each stage running on 6 DGX-A100s. The workload among the 6 machines is trained with data parallelism, while the workload among GPUs inside each machine is trained with model parallelism. The 3072 A100s in the entire cluster are divided into a matrix of \\(6\\times8\\times64\\) , and then train the model using data parallelism, model parallelism and pipeline parallelism simultaneously. The choice of parallelism strategy affects the efficiency of training. Whether the interface of framework supports parallelism well also determines the efficiency of algorithm engineer. OneFlow's system-level design and innovation for distributed training will help users to get comfortable well with distributed training. The related examples will be shown in other articles on this topic.","title":"Hybrid Paralelism"},{"location":"parallelism/02_sbp.html","text":"CONSISTENT VIEW \u00b6 The concept of consistent view in OneFlow is introduced to simplify distributed training. In short, the cluster is abstracted as a \"Super Computing Device\" under OneFlow consistent view. Instead of caring about the details of computing and communication in a cluster, users can program like on a single node, and OneFlow can train the model in a distributed way. OneFlow's consistent view relies on several important concepts: Placement , SBP and SBP Signature . Placement \u00b6 The Tensors of OneFlow has a placement attribute in consistent view; the placement specifies which physical device the Tensor is placed on. OneFlow will automatically number the devices in the cluster. For example, if there are four hosts in a cluster and each host has eight cards, then the four hosts correspond to ID: 0,1,2,3. The cards on each host correspond to numbers 0 to 7. To place a Tensor on the first four cards on machine 0, simply configure: placement(\"cuda\", {0: [0, 1, 2, 3]}) . Placement makes it easy for OneFlow to support pipelining parallelism, and we\u2019ll see examples of placement in other articles on this topic. SBP \u00b6 SBP is a unique concept in OneFlow, which describes the mapping of data from a \"Super Computing Device\" perspective to data on real physical devices in a cluster. It is a combination of the initials of three words: split , broadcast , partial . In detail: split means that the physical Tensor is obtained by splitting the logical Tensor along a certain dimension. An axis parameter is used to indicate the dimension of the split. If multiple physical Tensors are concatenated along the dimension of Split, the logical Tensor can be restored. broadcast indicates that each physical Tensor is exactly a copy of the logical Tensor. partial indicates that although the physical Tensor has the same shape as the logical Tensor, the value in the physical Tensor is a part of the value in the corresponding position in the logical Tensor, if you add multiple physical Tensors at the same positions, you can restore the logical Tensor. Besides sum , min , max and some other opreations are made available for partial . The figures below show some examples of SBP, including split(0) , split(1) , broadcast and partial sum . When you create a Consistent Tensor, you can specify the SBP of the Tensor. The example will be seen in the next article: Consistent Tensor . SPB Signature \u00b6 SBP describes the mapping relationship between the data under the consistent view and the data on the physical devices. When doing distributed training, OneFlow distributes the data to the physical devices, computes the results according to the SBP attributes of the data. For an isolated Tensor, we can set its SBP attributes at will. However, for an operator with input and output data, we can not arbitrarily set the SBP attributes of its input and output. This is because arbitrarily setting the SBP attributes of an operator\u2019s input and output may not conform to the algorithm of the operator under consistent view. Let us discuss this problem with the example of matrix multiplication. Look at how the input and output SBP of matrix multiplication are combined to be legal and illegal in a distributed system with tow devices. Suppose, from the consistent view, that a matrix \\(A\\) with the shape $ \\((m, k)\\) is multiplied by a matrix \\(B\\) with the shape \\((k, n)\\) to get $y $, the shape of \\(y\\) must be \\((m, n)\\) . According to the rule of matrix multiplication, we can divide the matrix \\(A\\) into two matrices \\(A_0\\) and \\(A_1\\) by dimension 0, with the shapes of \\((m_0, k)\\) , \\((m_1, k)\\) respectively: Device 1: \\[ \\begin{matrix} A_0 \\times B = Y_0 \\\\ (m_0, k) (k, n) (m_0, n) \\end{matrix} \\] Device 2: \\[ \\begin{matrix} A_1 \\times B = Y_1 \\\\ (m_1, k) (k, n) (m_1, n) \\end{matrix} \\] It\u2019s easy to configure the relationship among physical Tensors \\(A_0\\) , \\(A_1\\) and the Tensor \\(A\\) , which is under the consistent view. And also the relationship between \\(Y_0\\) , \\(Y_1\\) and the consistent view data \\(Y\\) : \\[ \\begin{matrix} A &= concat&(A_0 ,& A_1) \\\\ (m,k) & & (m_0, k) & (m_1, k) \\end{matrix} \\] \\[ \\begin{matrix} Y &= concat&(Y_0 ,& Y_1) \\\\ (m,n) & & (m_0, n) & (m_1, n) \\end{matrix} \\] Note: The concat above represents a concatenate operation. In this way, it is possible to execute the operation and get the correct result from the consistent view by distributing the data to each physical device. The long story we talked above, described in SBP, are surprisingly simple: \\(A\\) is split(0) , \\(B\\) is broadcast , and \\(Y\\) is split(0) . We can see that for matrix multiplication, the SBP of its input and output combined in the above way, is legal. For matrix multiplication, there are more than one valid SBP combinations, such as: \\(A\\) is broadcast , \\(B\\) is split(1) , and \\(Y\\) is split(1) . Or: \\(A\\) is split(1) , \\(B\\) is split(0) , and \\(Y\\) is partial sum . While we showed multiple valid SBP combinations above, not all SBP combinations are valid. For example, for matrix multiplication, if \\(A\\) , \\(B\\) are both split(0) , then: \\[ \\begin{matrix} A &= concat&(A_0 ,& A_1) \\\\ (m,k) & & (m_0, k) & (m_1, k) \\end{matrix} \\] \\[ \\begin{matrix} B &= concat&(B_0 ,& B_1) \\\\ (k,n) & & (k_0, n) & (k_1, n) \\end{matrix} \\] Because the shapes of \\(A_0\\) and \\(B_0\\) do not meet the requirements of matrix multiplication, it is impossible to compute the matrix multiplication on physical devices. We can say that the combination of \\(A\\) as split(0) and \\(B\\) as split(0) is illegal. We defines a specific, valid SBP combination of the inputs and outputs of an operator, as shown above, as a SBP Signature of this operator. All operators in OneFlow are presetting all possible SBP signatures according to the operator's Operation Rules. The user only needs to set the placement and SBP attributes of the data, the selection process is transparent to the user. Conclusion \u00b6 placement , SBP , and SBP Signature are the important guarantee of OneFlow distributed consistent view, which makes OneFlow distributed training as simple as on a single machine single card. In the next article Consistent Tensor , we\u2019ll show you an example of programming under the consistent view.","title":"Consistent View"},{"location":"parallelism/02_sbp.html#consistent-view","text":"The concept of consistent view in OneFlow is introduced to simplify distributed training. In short, the cluster is abstracted as a \"Super Computing Device\" under OneFlow consistent view. Instead of caring about the details of computing and communication in a cluster, users can program like on a single node, and OneFlow can train the model in a distributed way. OneFlow's consistent view relies on several important concepts: Placement , SBP and SBP Signature .","title":"CONSISTENT VIEW"},{"location":"parallelism/02_sbp.html#placement","text":"The Tensors of OneFlow has a placement attribute in consistent view; the placement specifies which physical device the Tensor is placed on. OneFlow will automatically number the devices in the cluster. For example, if there are four hosts in a cluster and each host has eight cards, then the four hosts correspond to ID: 0,1,2,3. The cards on each host correspond to numbers 0 to 7. To place a Tensor on the first four cards on machine 0, simply configure: placement(\"cuda\", {0: [0, 1, 2, 3]}) . Placement makes it easy for OneFlow to support pipelining parallelism, and we\u2019ll see examples of placement in other articles on this topic.","title":"Placement"},{"location":"parallelism/02_sbp.html#sbp","text":"SBP is a unique concept in OneFlow, which describes the mapping of data from a \"Super Computing Device\" perspective to data on real physical devices in a cluster. It is a combination of the initials of three words: split , broadcast , partial . In detail: split means that the physical Tensor is obtained by splitting the logical Tensor along a certain dimension. An axis parameter is used to indicate the dimension of the split. If multiple physical Tensors are concatenated along the dimension of Split, the logical Tensor can be restored. broadcast indicates that each physical Tensor is exactly a copy of the logical Tensor. partial indicates that although the physical Tensor has the same shape as the logical Tensor, the value in the physical Tensor is a part of the value in the corresponding position in the logical Tensor, if you add multiple physical Tensors at the same positions, you can restore the logical Tensor. Besides sum , min , max and some other opreations are made available for partial . The figures below show some examples of SBP, including split(0) , split(1) , broadcast and partial sum . When you create a Consistent Tensor, you can specify the SBP of the Tensor. The example will be seen in the next article: Consistent Tensor .","title":"SBP"},{"location":"parallelism/02_sbp.html#spb-signature","text":"SBP describes the mapping relationship between the data under the consistent view and the data on the physical devices. When doing distributed training, OneFlow distributes the data to the physical devices, computes the results according to the SBP attributes of the data. For an isolated Tensor, we can set its SBP attributes at will. However, for an operator with input and output data, we can not arbitrarily set the SBP attributes of its input and output. This is because arbitrarily setting the SBP attributes of an operator\u2019s input and output may not conform to the algorithm of the operator under consistent view. Let us discuss this problem with the example of matrix multiplication. Look at how the input and output SBP of matrix multiplication are combined to be legal and illegal in a distributed system with tow devices. Suppose, from the consistent view, that a matrix \\(A\\) with the shape $ \\((m, k)\\) is multiplied by a matrix \\(B\\) with the shape \\((k, n)\\) to get $y $, the shape of \\(y\\) must be \\((m, n)\\) . According to the rule of matrix multiplication, we can divide the matrix \\(A\\) into two matrices \\(A_0\\) and \\(A_1\\) by dimension 0, with the shapes of \\((m_0, k)\\) , \\((m_1, k)\\) respectively: Device 1: \\[ \\begin{matrix} A_0 \\times B = Y_0 \\\\ (m_0, k) (k, n) (m_0, n) \\end{matrix} \\] Device 2: \\[ \\begin{matrix} A_1 \\times B = Y_1 \\\\ (m_1, k) (k, n) (m_1, n) \\end{matrix} \\] It\u2019s easy to configure the relationship among physical Tensors \\(A_0\\) , \\(A_1\\) and the Tensor \\(A\\) , which is under the consistent view. And also the relationship between \\(Y_0\\) , \\(Y_1\\) and the consistent view data \\(Y\\) : \\[ \\begin{matrix} A &= concat&(A_0 ,& A_1) \\\\ (m,k) & & (m_0, k) & (m_1, k) \\end{matrix} \\] \\[ \\begin{matrix} Y &= concat&(Y_0 ,& Y_1) \\\\ (m,n) & & (m_0, n) & (m_1, n) \\end{matrix} \\] Note: The concat above represents a concatenate operation. In this way, it is possible to execute the operation and get the correct result from the consistent view by distributing the data to each physical device. The long story we talked above, described in SBP, are surprisingly simple: \\(A\\) is split(0) , \\(B\\) is broadcast , and \\(Y\\) is split(0) . We can see that for matrix multiplication, the SBP of its input and output combined in the above way, is legal. For matrix multiplication, there are more than one valid SBP combinations, such as: \\(A\\) is broadcast , \\(B\\) is split(1) , and \\(Y\\) is split(1) . Or: \\(A\\) is split(1) , \\(B\\) is split(0) , and \\(Y\\) is partial sum . While we showed multiple valid SBP combinations above, not all SBP combinations are valid. For example, for matrix multiplication, if \\(A\\) , \\(B\\) are both split(0) , then: \\[ \\begin{matrix} A &= concat&(A_0 ,& A_1) \\\\ (m,k) & & (m_0, k) & (m_1, k) \\end{matrix} \\] \\[ \\begin{matrix} B &= concat&(B_0 ,& B_1) \\\\ (k,n) & & (k_0, n) & (k_1, n) \\end{matrix} \\] Because the shapes of \\(A_0\\) and \\(B_0\\) do not meet the requirements of matrix multiplication, it is impossible to compute the matrix multiplication on physical devices. We can say that the combination of \\(A\\) as split(0) and \\(B\\) as split(0) is illegal. We defines a specific, valid SBP combination of the inputs and outputs of an operator, as shown above, as a SBP Signature of this operator. All operators in OneFlow are presetting all possible SBP signatures according to the operator's Operation Rules. The user only needs to set the placement and SBP attributes of the data, the selection process is transparent to the user.","title":"SPB Signature"},{"location":"parallelism/02_sbp.html#conclusion","text":"placement , SBP , and SBP Signature are the important guarantee of OneFlow distributed consistent view, which makes OneFlow distributed training as simple as on a single machine single card. In the next article Consistent Tensor , we\u2019ll show you an example of programming under the consistent view.","title":"Conclusion"},{"location":"parallelism/03_consistent_tensor.html","text":"CONSISTENT TENSOR \u00b6 The Mapping Between Consistent View and Physical View \u00b6 Create Consistent Tensor \u00b6 To interactively experience consistent tensor on a two-GPU machine, you can launch python separately in two consoles in the following way. Note Click the Terminal 0 or Terminal 1 label to check the commands/code Terminal 0 export MASTER_ADDR = 127 .0.0.1 MASTER_PORT = 17789 WORLD_SIZE = 2 RANK = 0 LOCAL_RANK = 0 python3 Terminal 1 export MASTER_ADDR = 127 .0.0.1 MASTER_PORT = 17789 WORLD_SIZE = 2 RANK = 1 LOCAL_RANK = 1 python3 Setting environment variables prepares the machines for distributed computing. Please refer to the Extended Reading section at the end of this article for a detailed explanation and ways to launch distributed computing using provided tools. Create Consistent Tensor Directly \u00b6 In each of the two consoles, import oneflow and create x . flow.placement(\"cuda\", {0:[0,1]}) specifies the device to place the consistent tensors. \"cuda\" means \"on GPU\". The second parameter of placement is a dictionary. Its key is the index of machine, and its value is the index of the graphic cards. Therefore, {0:[0,1]} means that the consistent tensor is on the 0 th , 1 st graphic cards of the 0 th machine. Terminal 0 import oneflow as flow placement = flow.placement(\"cuda\",{0:[0,1]}) sbp = flow.sbp.split(0) x = flow.randn(4,5,placement=placement, sbp=sbp) x.shape ``` Terminal 1 import oneflow as flow placement = flow . placement ( \"cuda\" ,{ 0 :[ 0 , 1 ]}) sbp = flow . sbp . split ( 0 ) x = flow . randn ( 4 , 5 , placement = placement , sbp = sbp ) x . shape Output: Terminal 0 flow.Size([4, 5]) Terminal 1 flow.Size([4, 5]) Get Local Tensor from Consistent Tensor \u00b6 Call to_local() to check the local tensor on a device. Terminal 0 x . to_local () tensor ([[ 2.9186e-01 , - 3.9442e-01 , 4.7072e-04 , - 3.2216e-01 , 1.7788e-01 ], [ - 4.5284e-01 , 1.2361e-01 , - 3.5962e-01 , 2.6651e-01 , 1.2951e+00 ]], device = 'cuda:0' , dtype = oneflow . float32 ) Terminal 1 x . to_local () tensor ([[ - 0.4363 , 0.9985 , - 2.5387 , 0.3003 , 0.3803 ], [ 0.0556 , - 0.8077 , 1.1191 , - 2.1278 , 0.1468 ]], device = 'cuda:1' , dtype = oneflow . float32 ) Convert Local Tensor to Consistent Tensor \u00b6 Developers can create local tensor first, then convert it to consistent tensor with Tensor.to_consistent . Two local tensors with the shape of (2,5) are created separately on two devices. While after the to_consistent method, the consistent tensor with a shape of (4,5) is obtained. The reason for this transformation lies in that by setting the sbp with sbp=flow.sbp.split(0) , the two local tensors with the shape of (2, 5) are concatenated on the 0 th dimension. Terminal 0 import oneflow as flow x = flow.randn(2,5) placement = flow.placement(\"cuda\",{0:[0,1]}) sbp = flow.sbp.split(0) x_consistent = x.to_consistent(placement=placement, sbp=sbp) x_consistent.shape ``` Terminal 1 import oneflow as flow x = flow . randn ( 2 , 5 ) placement = flow . placement ( \"cuda\" ,{ 0 :[ 0 , 1 ]}) sbp = flow . sbp . split ( 0 ) x_consistent = x . to_consistent ( placement = placement , sbp = sbp ) x_consistent . shape Practice with SBP Signature \u00b6 Data Parallelism \u00b6 The following code is an example of data parallelism of common distributed strategy Terminal 0 import oneflow as flow placement = flow . placement ( \"cuda\" ,{ 0 :[ 0 , 1 ]}) x = flow . randn ( 4 , 5 , placement = placement , sbp = flow . sbp . split ( 0 )) w = flow . randn ( 5 , 8 , placement = placement , sbp = flow . sbp . broadcast ) y = flow . matmul ( x , w ) y . sbp y . shape Terminal 1 import oneflow as flow placement = flow . placement ( \"cuda\" ,{ 0 :[ 0 , 1 ]}) x = flow . randn ( 4 , 5 , placement = placement , sbp = flow . sbp . split ( 0 )) w = flow . randn ( 5 , 8 , placement = placement , sbp = flow . sbp . broadcast ) y = flow . matmul ( x , w ) y . sbp y . shape flow.matmul supports many SBP signatures of inputs. When the SBP of x and w are split(0) and broadcast respectively, the SBP of output y is split(0) with logical shape of (4, 8) . Output: Terminal 0 (oneflow.sbp.split(axis=0),) flow.Size([4, 8]) Terminal 1 (oneflow.sbp.split(axis=0),) flow.Size([4, 8]) Model Parallelism \u00b6 The following code is an example of model parallelism of common distributed strategy . Terminal 0 import oneflow as flow placement = flow . placement ( \"cuda\" ,{ 0 :[ 0 , 1 ]}) x = flow . randn ( 4 , 5 , placement = placement , sbp = flow . sbp . broadcast ) w = flow . randn ( 5 , 8 , placement = placement , sbp = flow . sbp . split ( 1 )) y = flow . matmul ( x , w ) y . sbp y . shape Terminal 1 import oneflow as flow placement = flow . placement ( \"cuda\" ,{ 0 :[ 0 , 1 ]}) x = flow . randn ( 4 , 5 , placement = placement , sbp = flow . sbp . broadcast ) w = flow . randn ( 5 , 8 , placement = placement , sbp = flow . sbp . split ( 1 )) y = flow . matmul ( x , w ) y . sbp y . shape flow.matmul supports many SBP signatures of inputs. When the SBP of x and w are broadcast and split(0) respectively, the SBP of output y is split(1) with logical shape of (4, 8) . Output: Terminal 0 (oneflow.sbp.split(axis=1),) flow.Size([4, 8]) Terminal 1 (oneflow.sbp.split(axis=1),) flow.Size([4, 8]) Extended Reading \u00b6 Environment Variables in Multi-Machine Training \u00b6 As in the examples shown above, developers can manually launch the distributed training by setting the environment variables. In this way, developers can clearly see the effects and outputs in an interactive Python environment which is friendly for debugging. In production practice, developers can instead launch the distributed training with oneflow.distributed.launch . This module automatically sets necessary environment variables based on command-line arguments. MASTER_ADDR : The IP address of the 0 th machine in the cluster MASTER_PORT : The listening port of the 0 th machine in a multi-machine case. Note that this port should not be occupied by another process WORLD_SIZE : The number of computing devices in the whole cluster. Since there is currently no support for different numbers of GPUs on each machine, the number of WORLD_SIZE is actually \\(number\\:of\\:machines \\times number\\:of\\:GPUs\\:on\\:one\\:machine\\) . In our example, we have one machine and two GPUs on it, so WORLD_SIZE=2 RANK and LOCAL_RANK are indexes for processes. The difference is that RANK is a \"global perspective\" index, while LOCAL_RANK is a \"local perspective\" index. They are the same when only one machine is involved. In the above examples, we launch two processes on the same machine, so the RANK and LOCAL_RANK are the same. When launching the distributed training on multiple machines, the upper bound of LOCAL_RANK keeps the same with the number of computing devices on a single machine. The upper bound of RANK keeps the same with the sum of all computing devices in the cluster, with the indexing of processes starts from 0. (Both upper bounds are non-inclusive since indexing starts from 0) Assume that there are two machines and there are two graphic cards on each machine, we can sort out the correspondence between LOCAL_RANK and RANK RANK LOCAL_RANK GPU 0 on Machine 0 0 0 GPU 1 on Machine 0 1 1 GPU 0 on Machine 1 2 0 GPU 1 on Machine 1 3 1 Boxing\uff08Automatic Conversion of SBP\uff09 \u00b6 From the examples above, we learned that an operator can automatically set the SBP of the output tensor based on the SBP of the input tensor and the built-in SBP Signature of the operator. But what if the SBP of the output tensor does not satisfy the requirements of the next-layer operator? Assume that in model parallelism, there are two layers of matrix multiplication, and both layers use model parallelism. The SBP ( split(1) ) of the output from the first layer is not what the second layer expects ( broadcast ). In this case, OneFlow automatically inserts Boxing operation (AllGather) between the output of the first layer and the input of the second layer to perform necessary data movement. Converting split(1) to broadcast is equivalent to an AllGather operation, as shown in the figure below. Because of the Boxing mechanism, the developer only needs to set the SBP signature in a few key places (such as the source operator). The rest is all handled by the OneFlow framework and there is no need to insert the colletive communication operations manually.","title":"Consistent Tensor"},{"location":"parallelism/03_consistent_tensor.html#consistent-tensor","text":"","title":"CONSISTENT TENSOR"},{"location":"parallelism/03_consistent_tensor.html#the-mapping-between-consistent-view-and-physical-view","text":"","title":"The Mapping Between Consistent View and Physical View"},{"location":"parallelism/03_consistent_tensor.html#create-consistent-tensor","text":"To interactively experience consistent tensor on a two-GPU machine, you can launch python separately in two consoles in the following way. Note Click the Terminal 0 or Terminal 1 label to check the commands/code Terminal 0 export MASTER_ADDR = 127 .0.0.1 MASTER_PORT = 17789 WORLD_SIZE = 2 RANK = 0 LOCAL_RANK = 0 python3 Terminal 1 export MASTER_ADDR = 127 .0.0.1 MASTER_PORT = 17789 WORLD_SIZE = 2 RANK = 1 LOCAL_RANK = 1 python3 Setting environment variables prepares the machines for distributed computing. Please refer to the Extended Reading section at the end of this article for a detailed explanation and ways to launch distributed computing using provided tools.","title":"Create Consistent Tensor"},{"location":"parallelism/03_consistent_tensor.html#create-consistent-tensor-directly","text":"In each of the two consoles, import oneflow and create x . flow.placement(\"cuda\", {0:[0,1]}) specifies the device to place the consistent tensors. \"cuda\" means \"on GPU\". The second parameter of placement is a dictionary. Its key is the index of machine, and its value is the index of the graphic cards. Therefore, {0:[0,1]} means that the consistent tensor is on the 0 th , 1 st graphic cards of the 0 th machine. Terminal 0 import oneflow as flow placement = flow.placement(\"cuda\",{0:[0,1]}) sbp = flow.sbp.split(0) x = flow.randn(4,5,placement=placement, sbp=sbp) x.shape ``` Terminal 1 import oneflow as flow placement = flow . placement ( \"cuda\" ,{ 0 :[ 0 , 1 ]}) sbp = flow . sbp . split ( 0 ) x = flow . randn ( 4 , 5 , placement = placement , sbp = sbp ) x . shape Output: Terminal 0 flow.Size([4, 5]) Terminal 1 flow.Size([4, 5])","title":"Create Consistent Tensor Directly"},{"location":"parallelism/03_consistent_tensor.html#get-local-tensor-from-consistent-tensor","text":"Call to_local() to check the local tensor on a device. Terminal 0 x . to_local () tensor ([[ 2.9186e-01 , - 3.9442e-01 , 4.7072e-04 , - 3.2216e-01 , 1.7788e-01 ], [ - 4.5284e-01 , 1.2361e-01 , - 3.5962e-01 , 2.6651e-01 , 1.2951e+00 ]], device = 'cuda:0' , dtype = oneflow . float32 ) Terminal 1 x . to_local () tensor ([[ - 0.4363 , 0.9985 , - 2.5387 , 0.3003 , 0.3803 ], [ 0.0556 , - 0.8077 , 1.1191 , - 2.1278 , 0.1468 ]], device = 'cuda:1' , dtype = oneflow . float32 )","title":"Get Local Tensor from Consistent Tensor"},{"location":"parallelism/03_consistent_tensor.html#convert-local-tensor-to-consistent-tensor","text":"Developers can create local tensor first, then convert it to consistent tensor with Tensor.to_consistent . Two local tensors with the shape of (2,5) are created separately on two devices. While after the to_consistent method, the consistent tensor with a shape of (4,5) is obtained. The reason for this transformation lies in that by setting the sbp with sbp=flow.sbp.split(0) , the two local tensors with the shape of (2, 5) are concatenated on the 0 th dimension. Terminal 0 import oneflow as flow x = flow.randn(2,5) placement = flow.placement(\"cuda\",{0:[0,1]}) sbp = flow.sbp.split(0) x_consistent = x.to_consistent(placement=placement, sbp=sbp) x_consistent.shape ``` Terminal 1 import oneflow as flow x = flow . randn ( 2 , 5 ) placement = flow . placement ( \"cuda\" ,{ 0 :[ 0 , 1 ]}) sbp = flow . sbp . split ( 0 ) x_consistent = x . to_consistent ( placement = placement , sbp = sbp ) x_consistent . shape","title":"Convert Local Tensor to Consistent Tensor"},{"location":"parallelism/03_consistent_tensor.html#practice-with-sbp-signature","text":"","title":"Practice with SBP Signature"},{"location":"parallelism/03_consistent_tensor.html#data-parallelism","text":"The following code is an example of data parallelism of common distributed strategy Terminal 0 import oneflow as flow placement = flow . placement ( \"cuda\" ,{ 0 :[ 0 , 1 ]}) x = flow . randn ( 4 , 5 , placement = placement , sbp = flow . sbp . split ( 0 )) w = flow . randn ( 5 , 8 , placement = placement , sbp = flow . sbp . broadcast ) y = flow . matmul ( x , w ) y . sbp y . shape Terminal 1 import oneflow as flow placement = flow . placement ( \"cuda\" ,{ 0 :[ 0 , 1 ]}) x = flow . randn ( 4 , 5 , placement = placement , sbp = flow . sbp . split ( 0 )) w = flow . randn ( 5 , 8 , placement = placement , sbp = flow . sbp . broadcast ) y = flow . matmul ( x , w ) y . sbp y . shape flow.matmul supports many SBP signatures of inputs. When the SBP of x and w are split(0) and broadcast respectively, the SBP of output y is split(0) with logical shape of (4, 8) . Output: Terminal 0 (oneflow.sbp.split(axis=0),) flow.Size([4, 8]) Terminal 1 (oneflow.sbp.split(axis=0),) flow.Size([4, 8])","title":"Data Parallelism"},{"location":"parallelism/03_consistent_tensor.html#model-parallelism","text":"The following code is an example of model parallelism of common distributed strategy . Terminal 0 import oneflow as flow placement = flow . placement ( \"cuda\" ,{ 0 :[ 0 , 1 ]}) x = flow . randn ( 4 , 5 , placement = placement , sbp = flow . sbp . broadcast ) w = flow . randn ( 5 , 8 , placement = placement , sbp = flow . sbp . split ( 1 )) y = flow . matmul ( x , w ) y . sbp y . shape Terminal 1 import oneflow as flow placement = flow . placement ( \"cuda\" ,{ 0 :[ 0 , 1 ]}) x = flow . randn ( 4 , 5 , placement = placement , sbp = flow . sbp . broadcast ) w = flow . randn ( 5 , 8 , placement = placement , sbp = flow . sbp . split ( 1 )) y = flow . matmul ( x , w ) y . sbp y . shape flow.matmul supports many SBP signatures of inputs. When the SBP of x and w are broadcast and split(0) respectively, the SBP of output y is split(1) with logical shape of (4, 8) . Output: Terminal 0 (oneflow.sbp.split(axis=1),) flow.Size([4, 8]) Terminal 1 (oneflow.sbp.split(axis=1),) flow.Size([4, 8])","title":"Model Parallelism"},{"location":"parallelism/03_consistent_tensor.html#extended-reading","text":"","title":"Extended Reading"},{"location":"parallelism/03_consistent_tensor.html#environment-variables-in-multi-machine-training","text":"As in the examples shown above, developers can manually launch the distributed training by setting the environment variables. In this way, developers can clearly see the effects and outputs in an interactive Python environment which is friendly for debugging. In production practice, developers can instead launch the distributed training with oneflow.distributed.launch . This module automatically sets necessary environment variables based on command-line arguments. MASTER_ADDR : The IP address of the 0 th machine in the cluster MASTER_PORT : The listening port of the 0 th machine in a multi-machine case. Note that this port should not be occupied by another process WORLD_SIZE : The number of computing devices in the whole cluster. Since there is currently no support for different numbers of GPUs on each machine, the number of WORLD_SIZE is actually \\(number\\:of\\:machines \\times number\\:of\\:GPUs\\:on\\:one\\:machine\\) . In our example, we have one machine and two GPUs on it, so WORLD_SIZE=2 RANK and LOCAL_RANK are indexes for processes. The difference is that RANK is a \"global perspective\" index, while LOCAL_RANK is a \"local perspective\" index. They are the same when only one machine is involved. In the above examples, we launch two processes on the same machine, so the RANK and LOCAL_RANK are the same. When launching the distributed training on multiple machines, the upper bound of LOCAL_RANK keeps the same with the number of computing devices on a single machine. The upper bound of RANK keeps the same with the sum of all computing devices in the cluster, with the indexing of processes starts from 0. (Both upper bounds are non-inclusive since indexing starts from 0) Assume that there are two machines and there are two graphic cards on each machine, we can sort out the correspondence between LOCAL_RANK and RANK RANK LOCAL_RANK GPU 0 on Machine 0 0 0 GPU 1 on Machine 0 1 1 GPU 0 on Machine 1 2 0 GPU 1 on Machine 1 3 1","title":"Environment Variables in Multi-Machine Training"},{"location":"parallelism/03_consistent_tensor.html#boxingautomatic-conversion-of-sbp","text":"From the examples above, we learned that an operator can automatically set the SBP of the output tensor based on the SBP of the input tensor and the built-in SBP Signature of the operator. But what if the SBP of the output tensor does not satisfy the requirements of the next-layer operator? Assume that in model parallelism, there are two layers of matrix multiplication, and both layers use model parallelism. The SBP ( split(1) ) of the output from the first layer is not what the second layer expects ( broadcast ). In this case, OneFlow automatically inserts Boxing operation (AllGather) between the output of the first layer and the input of the second layer to perform necessary data movement. Converting split(1) to broadcast is equivalent to an AllGather operation, as shown in the figure below. Because of the Boxing mechanism, the developer only needs to set the SBP signature in a few key places (such as the source operator). The rest is all handled by the OneFlow framework and there is no need to insert the colletive communication operations manually.","title":"Boxing\uff08Automatic Conversion of SBP\uff09"},{"location":"parallelism/04_launch.html","text":"DISTRIBUTED TRAINING LAUNCHER \u00b6 OneFlow provides the oneflow.distributed.launch module to help users start distributed training more conveniently. Users can start distributed training by the following commands: python3 -m oneflow.distributed.launch [ Boot Option ] training_script.py For example, to start the training of a single machine with two graphics cards: python3 -m oneflow.distributed.launch --nproc_per_node 2 ./script.py For another example, start two machines, and each machine has two graphics cards for training. Run on machine 0: python3 -m oneflow.distributed.launch --nproc_per_node = 2 \\ --nnodes = 2 \\ --node_rank = 0 \\ --nproc_per_node = 2 \\ --master_addr = \"192.168.1.1\" \\ --master_port = 7788 \\ script.py Run on machine 1: python3 -m oneflow.distributed.launch --nproc_per_node = 2 \\ --nnodes = 2 \\ --node_rank = 1 \\ --nproc_per_node = 2 \\ --master_addr = \"192.168.1.1\" \\ --master_port = 7788 \\ script.py Description of Common Options \u00b6 We can view the description of the options of the launch module after running python3 -m oneflow.distributed.launch -h . The following are some common options: --nnodes : number of nodes --node_rank : the serial number of the machines, starting from 0 --nproc_per_node : The number of processes per node to be started on each machine, which is recommended to be consistent with the number of GPUs --logdir : The relative storage path of the child process log The Relationship between Launch Module and Parallel Strategy \u00b6 The main function of oneflow.distributed.launch is to allow users to start distributed training more conveniently after the user completes the distributed program. It saves the trouble of configuring environment variables in the cluster. But oneflow.distributed.launch does not determine Parallel Strategy . The Parallel Strategy is determined by the setup of the distribution method of data and the model, and the placement of those on the physical devices. OneFlow provides Consistent View and Consistent Tensor to flexibly configure parallel strategies. And for data parallelism, OneFlow provides the DistributedDataParallel module, which can change the stand-alone single-card script to the script of data parallel with minimal code modification.","title":"Distributed Training Launcher"},{"location":"parallelism/04_launch.html#distributed-training-launcher","text":"OneFlow provides the oneflow.distributed.launch module to help users start distributed training more conveniently. Users can start distributed training by the following commands: python3 -m oneflow.distributed.launch [ Boot Option ] training_script.py For example, to start the training of a single machine with two graphics cards: python3 -m oneflow.distributed.launch --nproc_per_node 2 ./script.py For another example, start two machines, and each machine has two graphics cards for training. Run on machine 0: python3 -m oneflow.distributed.launch --nproc_per_node = 2 \\ --nnodes = 2 \\ --node_rank = 0 \\ --nproc_per_node = 2 \\ --master_addr = \"192.168.1.1\" \\ --master_port = 7788 \\ script.py Run on machine 1: python3 -m oneflow.distributed.launch --nproc_per_node = 2 \\ --nnodes = 2 \\ --node_rank = 1 \\ --nproc_per_node = 2 \\ --master_addr = \"192.168.1.1\" \\ --master_port = 7788 \\ script.py","title":"DISTRIBUTED TRAINING LAUNCHER"},{"location":"parallelism/04_launch.html#description-of-common-options","text":"We can view the description of the options of the launch module after running python3 -m oneflow.distributed.launch -h . The following are some common options: --nnodes : number of nodes --node_rank : the serial number of the machines, starting from 0 --nproc_per_node : The number of processes per node to be started on each machine, which is recommended to be consistent with the number of GPUs --logdir : The relative storage path of the child process log","title":"Description of Common Options"},{"location":"parallelism/04_launch.html#the-relationship-between-launch-module-and-parallel-strategy","text":"The main function of oneflow.distributed.launch is to allow users to start distributed training more conveniently after the user completes the distributed program. It saves the trouble of configuring environment variables in the cluster. But oneflow.distributed.launch does not determine Parallel Strategy . The Parallel Strategy is determined by the setup of the distribution method of data and the model, and the placement of those on the physical devices. OneFlow provides Consistent View and Consistent Tensor to flexibly configure parallel strategies. And for data parallelism, OneFlow provides the DistributedDataParallel module, which can change the stand-alone single-card script to the script of data parallel with minimal code modification.","title":"The Relationship between Launch Module and Parallel Strategy"},{"location":"parallelism/05_ddp.html","text":"DATA PARALLELISM TRAINING \u00b6 In Common Distributed Parallel Strategies , we introduced the characteristics of data parallel. OneFlow provides oneflow.nn.parallel.DistributedDataParallel module and launcher , which allows users to run data parallel training almost without modifying the script of a single node. A quick start of OneFlow's data parallelsim training: wget https://docs.oneflow.org/master/code/parallelism/ddp_train.py #Download python3 -m oneflow.distributed.launch --nproc_per_node 2 ./ddp_train.py #data parallel training Out\uff1a 50/500 loss:0.004111831542104483 50/500 loss:0.00025336415274068713 ... 500/500 loss:6.184563972055912e-11 500/500 loss:4.547473508864641e-12 w:tensor([[2.0000], [3.0000]], device='cuda:1', dtype=oneflow.float32, grad_fn=<accumulate_grad>) w:tensor([[2.0000], [3.0000]], device='cuda:0', dtype=oneflow.float32, grad_fn=<accumulate_grad>) Codes \u00b6 Click \"Code\" below to expand the code of the above running script. Code import oneflow as flow from oneflow.nn.parallel import DistributedDataParallel as ddp train_x = [ flow . tensor ([[ 1 , 2 ], [ 2 , 3 ]], dtype = flow . float32 ), flow . tensor ([[ 4 , 6 ], [ 3 , 1 ]], dtype = flow . float32 ), ] train_y = [ flow . tensor ([[ 8 ], [ 13 ]], dtype = flow . float32 ), flow . tensor ([[ 26 ], [ 9 ]], dtype = flow . float32 ), ] class Model ( flow . nn . Module ): def __init__ ( self ): super () . __init__ () self . lr = 0.01 self . iter_count = 500 self . w = flow . nn . Parameter ( flow . tensor ([[ 0 ], [ 0 ]], dtype = flow . float32 )) def forward ( self , x ): x = flow . matmul ( x , self . w ) return x m = Model () . to ( \"cuda\" ) m = ddp ( m ) loss = flow . nn . MSELoss ( reduction = \"sum\" ) optimizer = flow . optim . SGD ( m . parameters (), m . lr ) for i in range ( 0 , m . iter_count ): rank = flow . env . get_rank () x = train_x [ rank ] . to ( \"cuda\" ) y = train_y [ rank ] . to ( \"cuda\" ) y_pred = m ( x ) l = loss ( y_pred , y ) if ( i + 1 ) % 50 == 0 : print ( f \" { i + 1 } / { m . iter_count } loss: { l } \" ) optimizer . zero_grad () l . backward () optimizer . step () print ( f \" \\n w: { m . w } \" ) There are only two differences between the data parallelism training code and the stand-alone single-card script: Use DistributedDataParallel to wrap the module object ( m = ddp(m) ) Use get_rank to get the current device number and distribute the data to the device. Then use launcher to run the script, leave everything else to OneFlow, which makes distributed training as simple as stand-alone single-card training: python3 -m oneflow.distributed.launch --nproc_per_node 2 ./ddp_train.py DistributedSampler \u00b6 The data used is manually distributed in this context to highlight DistributedDataParallel . However, in practical applications, you can directly use DistributedSampler with data parallel. DistributedSampler will instantiate the Dataloader in each process, and each Dataloader instance will load a part of the complete data to automatically complete the data distribution.","title":"Data Parallelism Training"},{"location":"parallelism/05_ddp.html#data-parallelism-training","text":"In Common Distributed Parallel Strategies , we introduced the characteristics of data parallel. OneFlow provides oneflow.nn.parallel.DistributedDataParallel module and launcher , which allows users to run data parallel training almost without modifying the script of a single node. A quick start of OneFlow's data parallelsim training: wget https://docs.oneflow.org/master/code/parallelism/ddp_train.py #Download python3 -m oneflow.distributed.launch --nproc_per_node 2 ./ddp_train.py #data parallel training Out\uff1a 50/500 loss:0.004111831542104483 50/500 loss:0.00025336415274068713 ... 500/500 loss:6.184563972055912e-11 500/500 loss:4.547473508864641e-12 w:tensor([[2.0000], [3.0000]], device='cuda:1', dtype=oneflow.float32, grad_fn=<accumulate_grad>) w:tensor([[2.0000], [3.0000]], device='cuda:0', dtype=oneflow.float32, grad_fn=<accumulate_grad>)","title":"DATA PARALLELISM TRAINING"},{"location":"parallelism/05_ddp.html#codes","text":"Click \"Code\" below to expand the code of the above running script. Code import oneflow as flow from oneflow.nn.parallel import DistributedDataParallel as ddp train_x = [ flow . tensor ([[ 1 , 2 ], [ 2 , 3 ]], dtype = flow . float32 ), flow . tensor ([[ 4 , 6 ], [ 3 , 1 ]], dtype = flow . float32 ), ] train_y = [ flow . tensor ([[ 8 ], [ 13 ]], dtype = flow . float32 ), flow . tensor ([[ 26 ], [ 9 ]], dtype = flow . float32 ), ] class Model ( flow . nn . Module ): def __init__ ( self ): super () . __init__ () self . lr = 0.01 self . iter_count = 500 self . w = flow . nn . Parameter ( flow . tensor ([[ 0 ], [ 0 ]], dtype = flow . float32 )) def forward ( self , x ): x = flow . matmul ( x , self . w ) return x m = Model () . to ( \"cuda\" ) m = ddp ( m ) loss = flow . nn . MSELoss ( reduction = \"sum\" ) optimizer = flow . optim . SGD ( m . parameters (), m . lr ) for i in range ( 0 , m . iter_count ): rank = flow . env . get_rank () x = train_x [ rank ] . to ( \"cuda\" ) y = train_y [ rank ] . to ( \"cuda\" ) y_pred = m ( x ) l = loss ( y_pred , y ) if ( i + 1 ) % 50 == 0 : print ( f \" { i + 1 } / { m . iter_count } loss: { l } \" ) optimizer . zero_grad () l . backward () optimizer . step () print ( f \" \\n w: { m . w } \" ) There are only two differences between the data parallelism training code and the stand-alone single-card script: Use DistributedDataParallel to wrap the module object ( m = ddp(m) ) Use get_rank to get the current device number and distribute the data to the device. Then use launcher to run the script, leave everything else to OneFlow, which makes distributed training as simple as stand-alone single-card training: python3 -m oneflow.distributed.launch --nproc_per_node 2 ./ddp_train.py","title":"Codes"},{"location":"parallelism/05_ddp.html#distributedsampler","text":"The data used is manually distributed in this context to highlight DistributedDataParallel . However, in practical applications, you can directly use DistributedSampler with data parallel. DistributedSampler will instantiate the Dataloader in each process, and each Dataloader instance will load a part of the complete data to automatically complete the data distribution.","title":"DistributedSampler"},{"location":"parallelism/06_pipeline.html","text":"PIPELINING PARALLELISM \u00b6 We have introduced the characteristics of pipelining parallelism in COMMON DISTRIBUTED PARALLEL STRATEGIES . From OneFlow's consistent view , pipelining can be achieved by simply setting the placement attribute of Tensor. The following code is a simple example that will run the network in QUICKSTART with pipelining parallelism. nn.Flatten , nn.Linear(28*28, 512) and nn.ReLU() run on GPU0, and the rest layers of the network run on GPU1. Code import oneflow as flow BATCH_SIZE = 16 BROADCAST = [ flow . sbp . broadcast ] P0 = flow . placement ( \"cuda\" , { 0 : [ 0 ]}) P1 = flow . placement ( \"cuda\" , { 0 : [ 1 ]}) class Stage0Module ( flow . nn . Module ): def __init__ ( self ): super () . __init__ () self . flatten = flow . nn . Flatten () self . linear0 = flow . nn . Linear ( 28 * 28 , 512 ) self . relu0 = flow . nn . ReLU () def forward ( self , x ): out = self . flatten ( x ) out = self . linear0 ( out ) out = self . relu0 ( out ) return out class Stage1Module ( flow . nn . Module ): def __init__ ( self ): super () . __init__ () self . linear1 = flow . nn . Linear ( 512 , 512 ) self . relu1 = flow . nn . ReLU () self . linear2 = flow . nn . Linear ( 512 , 10 ) self . relu2 = flow . nn . ReLU () def forward ( self , x ): out = self . linear1 ( x ) out = self . relu1 ( out ) out = self . linear2 ( out ) out = self . relu2 ( out ) return out class PipelineModule ( flow . nn . Module ): def __init__ ( self ): super () . __init__ () self . m_stage0 = Stage0Module () self . m_stage1 = Stage1Module () self . m_stage0 . to_consistent ( placement = P0 , sbp = BROADCAST ) self . m_stage1 . to_consistent ( placement = P1 , sbp = BROADCAST ) def forward ( self , x ): out_stage0 = self . m_stage0 ( x ) in_stage1 = out_stage0 . to_consistent ( placement = P1 , sbp = BROADCAST ) out_stage1 = self . m_stage1 ( in_stage1 ) return out_stage1 module_pipeline = PipelineModule () sgd = flow . optim . SGD ( module_pipeline . parameters (), lr = 0.001 ) class PipelineGraph ( flow . nn . Graph ): def __init__ ( self ): super () . __init__ () self . module_pipeline = module_pipeline self . module_pipeline . m_stage0 . config . stage_id = 0 self . module_pipeline . m_stage1 . config . stage_id = 1 self . loss_fn = flow . nn . CrossEntropyLoss () self . config . set_gradient_accumulation_steps ( 2 ) self . add_optimizer ( sgd ) def build ( self , x , y ): out = self . module_pipeline ( x ) loss = self . loss_fn ( out , y ) loss . backward () return loss graph_pipeline = PipelineGraph () x = flow . randn ( BATCH_SIZE , 1 , 28 , 28 ) x = x . to_consistent ( P0 , BROADCAST ) y = flow . randint ( 0 , 10 , ( BATCH_SIZE ,)) y = y . to_consistent ( P1 , BROADCAST ) for i in range ( 20 ): loss = graph_pipeline ( x , y ) print ( loss . to_local ()) When the code above is saved as a script ( pipeline.py ), it can be then launched by the launch module : python3 -m oneflow.distributed.launch --nproc_per_node 2 ./pipeline.py More Details \u00b6 Setting placement and SBP \u00b6 Setting up the placement and SBP at the begining: BROADCAST = [ flow . sbp . broadcast ] P0 = flow . placement ( \"cuda\" , { 0 : [ 0 ]}) P1 = flow . placement ( \"cuda\" , { 0 : [ 1 ]}) P0 and P1 represent the 0 th GPU and the 1 st GPU on the 0 th machine respectively. By calling nn.Module.to_consistent or Tensor.to_consistent , the model or tensor will be distributed to the devices specified before, breaking a network into stages. Here we define a PipelineModule that specifically sets the pipeline for each stage. class PipelineModule ( flow . nn . Module ): def __init__ ( self ): #... self . m_stage0 . to_consistent ( placement = P0 , sbp = BROADCAST ) self . m_stage1 . to_consistent ( placement = P1 , sbp = BROADCAST ) def forward ( self , x ): out_stage0 = self . m_stage0 ( x ) in_stage1 = out_stage0 . to_consistent ( placement = P1 , sbp = BROADCAST ) out_stage1 = self . m_stage1 ( in_stage1 ) return out_stage1 Transforming the Local Tensor to the Consistent Tensor \u00b6 The example uses randomly generated data as input. x = flow . randn ( BATCH_SIZE , 1 , 28 , 28 ) x = x . to_consistent ( P0 , BROADCAST ) The launch will start two processes when you launch the training by the launch module because the command-line parameter is --nproc_per_node 2 . Both processes will execute the code in the script. The statement x = flow.randn(BATCH_SIZE, 1, 28, 28) returns the Local Tensor (the local data only valid in current process). when running x = x.to_consistent(P0, BROADCAST) , OneFlow will automatically integrate the Local Tensor of all processes into the Consistent Tensor. In practice, each computing device can load data locally, and then convert the Local Tensor to the Consistent Tensor via to_consistent . Stage ID and Settings for Gradient Accumulation \u00b6 We can set Stage ID by setting the config.stage_id attribute of Module. The Stage ID is numbered starting from 0 and increasing by 1. Call self.config.set_gradient_accumulation_steps method to set the step size of gradient accumulation. The information needed to implement micro-batch in pipelining parallelism can be obtained by these two configurations. self . module_pipeline . m_stage0 . config . stage_id = 0 self . module_pipeline . m_stage1 . config . stage_id = 1 self . config . set_gradient_accumulation_steps ( 2 )","title":"Pipelining Parallelism"},{"location":"parallelism/06_pipeline.html#pipelining-parallelism","text":"We have introduced the characteristics of pipelining parallelism in COMMON DISTRIBUTED PARALLEL STRATEGIES . From OneFlow's consistent view , pipelining can be achieved by simply setting the placement attribute of Tensor. The following code is a simple example that will run the network in QUICKSTART with pipelining parallelism. nn.Flatten , nn.Linear(28*28, 512) and nn.ReLU() run on GPU0, and the rest layers of the network run on GPU1. Code import oneflow as flow BATCH_SIZE = 16 BROADCAST = [ flow . sbp . broadcast ] P0 = flow . placement ( \"cuda\" , { 0 : [ 0 ]}) P1 = flow . placement ( \"cuda\" , { 0 : [ 1 ]}) class Stage0Module ( flow . nn . Module ): def __init__ ( self ): super () . __init__ () self . flatten = flow . nn . Flatten () self . linear0 = flow . nn . Linear ( 28 * 28 , 512 ) self . relu0 = flow . nn . ReLU () def forward ( self , x ): out = self . flatten ( x ) out = self . linear0 ( out ) out = self . relu0 ( out ) return out class Stage1Module ( flow . nn . Module ): def __init__ ( self ): super () . __init__ () self . linear1 = flow . nn . Linear ( 512 , 512 ) self . relu1 = flow . nn . ReLU () self . linear2 = flow . nn . Linear ( 512 , 10 ) self . relu2 = flow . nn . ReLU () def forward ( self , x ): out = self . linear1 ( x ) out = self . relu1 ( out ) out = self . linear2 ( out ) out = self . relu2 ( out ) return out class PipelineModule ( flow . nn . Module ): def __init__ ( self ): super () . __init__ () self . m_stage0 = Stage0Module () self . m_stage1 = Stage1Module () self . m_stage0 . to_consistent ( placement = P0 , sbp = BROADCAST ) self . m_stage1 . to_consistent ( placement = P1 , sbp = BROADCAST ) def forward ( self , x ): out_stage0 = self . m_stage0 ( x ) in_stage1 = out_stage0 . to_consistent ( placement = P1 , sbp = BROADCAST ) out_stage1 = self . m_stage1 ( in_stage1 ) return out_stage1 module_pipeline = PipelineModule () sgd = flow . optim . SGD ( module_pipeline . parameters (), lr = 0.001 ) class PipelineGraph ( flow . nn . Graph ): def __init__ ( self ): super () . __init__ () self . module_pipeline = module_pipeline self . module_pipeline . m_stage0 . config . stage_id = 0 self . module_pipeline . m_stage1 . config . stage_id = 1 self . loss_fn = flow . nn . CrossEntropyLoss () self . config . set_gradient_accumulation_steps ( 2 ) self . add_optimizer ( sgd ) def build ( self , x , y ): out = self . module_pipeline ( x ) loss = self . loss_fn ( out , y ) loss . backward () return loss graph_pipeline = PipelineGraph () x = flow . randn ( BATCH_SIZE , 1 , 28 , 28 ) x = x . to_consistent ( P0 , BROADCAST ) y = flow . randint ( 0 , 10 , ( BATCH_SIZE ,)) y = y . to_consistent ( P1 , BROADCAST ) for i in range ( 20 ): loss = graph_pipeline ( x , y ) print ( loss . to_local ()) When the code above is saved as a script ( pipeline.py ), it can be then launched by the launch module : python3 -m oneflow.distributed.launch --nproc_per_node 2 ./pipeline.py","title":"PIPELINING PARALLELISM"},{"location":"parallelism/06_pipeline.html#more-details","text":"","title":"More Details"},{"location":"parallelism/06_pipeline.html#setting-placement-and-sbp","text":"Setting up the placement and SBP at the begining: BROADCAST = [ flow . sbp . broadcast ] P0 = flow . placement ( \"cuda\" , { 0 : [ 0 ]}) P1 = flow . placement ( \"cuda\" , { 0 : [ 1 ]}) P0 and P1 represent the 0 th GPU and the 1 st GPU on the 0 th machine respectively. By calling nn.Module.to_consistent or Tensor.to_consistent , the model or tensor will be distributed to the devices specified before, breaking a network into stages. Here we define a PipelineModule that specifically sets the pipeline for each stage. class PipelineModule ( flow . nn . Module ): def __init__ ( self ): #... self . m_stage0 . to_consistent ( placement = P0 , sbp = BROADCAST ) self . m_stage1 . to_consistent ( placement = P1 , sbp = BROADCAST ) def forward ( self , x ): out_stage0 = self . m_stage0 ( x ) in_stage1 = out_stage0 . to_consistent ( placement = P1 , sbp = BROADCAST ) out_stage1 = self . m_stage1 ( in_stage1 ) return out_stage1","title":"Setting placement and SBP"},{"location":"parallelism/06_pipeline.html#transforming-the-local-tensor-to-the-consistent-tensor","text":"The example uses randomly generated data as input. x = flow . randn ( BATCH_SIZE , 1 , 28 , 28 ) x = x . to_consistent ( P0 , BROADCAST ) The launch will start two processes when you launch the training by the launch module because the command-line parameter is --nproc_per_node 2 . Both processes will execute the code in the script. The statement x = flow.randn(BATCH_SIZE, 1, 28, 28) returns the Local Tensor (the local data only valid in current process). when running x = x.to_consistent(P0, BROADCAST) , OneFlow will automatically integrate the Local Tensor of all processes into the Consistent Tensor. In practice, each computing device can load data locally, and then convert the Local Tensor to the Consistent Tensor via to_consistent .","title":"Transforming the Local Tensor to the Consistent Tensor"},{"location":"parallelism/06_pipeline.html#stage-id-and-settings-for-gradient-accumulation","text":"We can set Stage ID by setting the config.stage_id attribute of Module. The Stage ID is numbered starting from 0 and increasing by 1. Call self.config.set_gradient_accumulation_steps method to set the step size of gradient accumulation. The information needed to implement micro-batch in pipelining parallelism can be obtained by these two configurations. self . module_pipeline . m_stage0 . config . stage_id = 0 self . module_pipeline . m_stage1 . config . stage_id = 1 self . config . set_gradient_accumulation_steps ( 2 )","title":"Stage ID and Settings for Gradient Accumulation"},{"location":"single_client/basics_topics/async_get.html","text":"Get Results from Job Function \u00b6 In this article, we will talk about getting the return value of a job function in OneFlow. It covers: How to get the return value from a job function synchronously. How to get the return value from a job function asynchronously. In OneFlow, a function decorated by @flow.global_function is called \"Job Function\". Job function can be implmented for training, evaluation and prediction. By specifing the return type of job function, we can get results from job function both synchronously and asynchronously. Difference Between Synchronous and Asynchronous \u00b6 Synchronization \u00b6 During synchronous training, the training of the next step cannot be started until the work of the previous step is completed. Asynchronization \u00b6 In asynchronous training, it is equivalent to turning on multi-threading mode. A step does not have to wait until previous step completes. For instance data preprocessing and loading task could be runned in advance. Through the comparison above, it can be seen that the use of asynchronous execution job function in OneFlow can effectively utilize computer resources, especially in the case of loading huge data, enabling asynchronous execution can effectively shorten the data loading and preprocessing time, and accelerate the model training. Next, we will explain how to get the results synchronously and asynchronously from job function, and how to write callback functions in asynchronous jobs. At the end of the article, we will provide a complete example. The main points are\uff1a Getting results synchronously or asynchronously is determined by the return value type of job function The data type of return value is selected in oneflow.typing When we call a job function, the form of getting results synchronously / asynchronously is slightly different Get Result Synchronously \u00b6 When we define a job function, if the annotation of the return type of the job function is oneflow.typing.Numpy , the job function will be called synchronously. For example, when we define a job function like this: @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : with flow . scope . placement ( \"gpu\" , \"0:0\" ): logits = lenet ( images , train = True ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , logits , name = \"softmax_loss\" ) lr_scheduler = flow . optimizer . PiecewiseConstantScheduler ([], [ 0.1 ]) flow . optimizer . SGD ( lr_scheduler , momentum = 0 ) . minimize ( loss ) return loss Through Python annotations, OneFlow knows the type of the job function's return is oneflow.typing.Numpy , which corresponds to ndarray in Numpy . Then when we call the job function, it will simply return the ndarray object: loss = train_job ( images , labels ) if i % 20 == 0 : print ( loss . mean ()) From the example above, it should be noted that: When we define the job function, the return value of job function (loss) is a placeholder for graph construction. When we specify the return value type as oneflow.typing.Numpy . OneFlow will know that when the job function is called, the real data type returned is Numpy.ndarray object. By calling the job function train_job(images, labels) , we can get the result from job function directly, and the retnred value is a ndarray object corresponding to oneflow.typing.Numpy . Data Types in oneflow.typing \u00b6 The oneflow.typing contains all the data types that can be returned by the job function. The oneflow.typing.Numpy shown above is only one of them. The commonly used types and their corresponding meanings are listed as follows: oneflow.typing.Numpy : corresponding to a numpy.ndarray flow.typing.ListNumpy : corresponding to a list container. Each element in it is a numpy.ndarray object. It is related to the view of OneFlow for distributed training. We will see details in The consistent and mirrored view in distributed training. oneflow.typing.ListListNumpy \uff1acorresponds to a list container where each element is a TensorList object and some interfaces to OneFlow need to process or return multiple TensorList . More information refer to Term & Concept in OneFlow and related API documentation oneflow.typing.Callback : corresponding to a callback function. It is used to call job function asynchronously. We will introduce it below. In addition, OneFlow also allows job functions to pass out data in dictionary form. For examples of ListNumpy , ListNumpy , ListListNumpy and how to pass out data in dictionary form please refer to OneFlow's Test Case . Get Result Asynchronously \u00b6 Generally speaking, the efficiency of asynchronous training is higher than that of synchronous training. The following describes how to call job function asynchronously and process the results. The basic steps include: Prepare a callback function: It is necessary to specify the parameters accepted by the callback function by annotations and implementing the logic for return value of the job function inside the callback function. Implementing a Job Function: Specify flow.typing.Callback as the return type of the job function by annotation. As we will see in the following example that we can specify the parameter type of the callback function with Callback . Call the job function: Register the callback function prepared in step 1 above. The first three steps are completed by users of OneFlow. When the program runs, the registered callback function is called by OneFlow and the return values of the job function are passed as parameters to the callback function. Prepare a Callback Function \u00b6 Suppose the prototype of the callback function is: def cb_func ( result : T ): #... Among them, the result needs to be annotated to specify its type T , that is, numpy, listnumpy, etc. mentioned above, or their composite type. We will have corresponding examples below. The result of callback function is actually the return value of job function. Therefore, it must be consistent with the annotation of the return value of the job function For example, when we define a job function below: @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Callback [ tp . Numpy ]: # mlp initializer = flow . truncated_normal ( 0.1 ) reshape = flow . reshape ( images , [ images . shape [ 0 ], - 1 ]) hidden = flow . layers . dense ( reshape , 512 , activation = flow . nn . relu , kernel_initializer = initializer , name = \"hidden\" , ) logits = flow . layers . dense ( hidden , 10 , kernel_initializer = initializer , name = \"output\" ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , logits , name = \"softmax_loss\" ) lr_scheduler = flow . optimizer . PiecewiseConstantScheduler ([], [ 0.1 ]) flow . optimizer . SGD ( lr_scheduler , momentum = 0 ) . minimize ( loss ) return loss Annotation -> tp.Callback[tp.Numpy] means that this job function returns a oneflow.typing.Numpy type, and need to be called asynchronously. Thus, the callback function we defined should accept a numpy parameter: def cb_print_loss ( result : tp . Numpy ): global g_i if g_i % 20 == 0 : print ( result . mean ()) g_i += 1 Let's take a look at another example. If the job function is defined as below: @flow . global_function ( type = \"predict\" ) def eval_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Callback [ Tuple [ tp . Numpy , tp . Numpy ]]: with flow . scope . placement ( \"cpu\" , \"0:0\" ): logits = mlp ( images ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , logits , name = \"softmax_loss\" ) return ( labels , logits ) Annotation -> tp.Callback[Tuple[tp.Numpy, tp.Numpy]] means that this job function returns a tuple and each element is tp.Numpy . The job function needs to be called asynchronously. Thus, the parameter annotation of the corresponding callback function should be: g_total = 0 g_correct = 0 def acc ( arguments : Tuple [ tp . Numpy , tp . Numpy ]): global g_total global g_correct labels = arguments [ 0 ] logits = arguments [ 1 ] predictions = np . argmax ( logits , 1 ) right_count = np . sum ( predictions == labels ) g_total += labels . shape [ 0 ] g_correct += right_count The arguments corresponds to the return type of the above job function. Registration of Callback Function \u00b6 When we call the job function asynchronously, the job function will return a 'callback' object, and we register the prepared callback function by passing it to that object. OneFlow will automatically call the registered callback function when it gets the training results. callbacker = train_job ( images , labels ) callbacker ( cb_print_loss ) But the above code is redundant, we recommend: train_job ( images , labels )( cb_print_loss ) Code \u00b6 Get single result synchronously \u00b6 We use LeNet as an example here to show how to get the return value loss synchronously and print the loss every 20 iterations. Code example: synchronize_single_job.py Run: wget https://docs.oneflow.org/master/code/basics_topics/synchronize_single_job.py python3 synchronize_single_job.py There will be outputs like: File mnist.npz already exist, path: ./mnist.npz 7.3258467 2.1435719 1.1712438 0.7531896 ... ... model saved \u00b6 Get Mutiple Results Synchronously \u00b6 In this case, the job function returns a tuple . We get the results labels and logits in tuple synchronously. Also, we evaluate the trained model in the above example, then output the accuracy rate. Code: synchronize_batch_job.py The trained model can be downloaded from lenet_models_1.zip Run: wget https://oneflow-public.oss-cn-beijing.aliyuncs.com/online_document/docs/quick_start/lenet_models_1.zip unzip lenet_models_1.zip wget https://docs.oneflow.org/master/code/basics_topics/synchronize_batch_job.py python3 synchronize_batch_job.py There will be outputs like: accuracy: 99.3% \u00b6 Get Single Result Asynchronously \u00b6 In this case, we take MLP as example to get the single return result loss from job function asynchronously, and the loss is printed every 20 rounds. Code: async_single_job.py Run: wget https://docs.oneflow.org/master/code/basics_topics/async_single_job.py python3 async_single_job.py There will be outputs like: File mnist.npz already exist, path: ./mnist.npz 3.0865736 0.8949808 0.47858357 0.3486296 ... Get Multiple Results Asynchronously \u00b6 In the following example, we will show how to get multiple return results of the job function asynchronously, evaluate the trained model in the above example, and get the accuracy. Code: async_batch_job.py The trained model can be downloaded from mlp_models_1.zip ]( https://oneflow-public.oss-cn-beijing.aliyuncs.com/online_document/docs/basics_topics/mlp_models_1.zip ) Run: wget https://oneflow-public.oss-cn-beijing.aliyuncs.com/online_document/docs/basics_topics/mlp_models_1.zip unzip mlp_models_1.zip wget https://docs.oneflow.org/master/code/basics_topics/async_batch_job.py python3 async_batch_job.py There would be outputs like: File mnist.npz already exist, path: ./mnist.npz accuracy: 97.6%","title":"Get results from job function"},{"location":"single_client/basics_topics/async_get.html#get-results-from-job-function","text":"In this article, we will talk about getting the return value of a job function in OneFlow. It covers: How to get the return value from a job function synchronously. How to get the return value from a job function asynchronously. In OneFlow, a function decorated by @flow.global_function is called \"Job Function\". Job function can be implmented for training, evaluation and prediction. By specifing the return type of job function, we can get results from job function both synchronously and asynchronously.","title":"Get Results from Job Function"},{"location":"single_client/basics_topics/async_get.html#difference-between-synchronous-and-asynchronous","text":"","title":"Difference Between Synchronous and Asynchronous"},{"location":"single_client/basics_topics/async_get.html#synchronization","text":"During synchronous training, the training of the next step cannot be started until the work of the previous step is completed.","title":"Synchronization"},{"location":"single_client/basics_topics/async_get.html#asynchronization","text":"In asynchronous training, it is equivalent to turning on multi-threading mode. A step does not have to wait until previous step completes. For instance data preprocessing and loading task could be runned in advance. Through the comparison above, it can be seen that the use of asynchronous execution job function in OneFlow can effectively utilize computer resources, especially in the case of loading huge data, enabling asynchronous execution can effectively shorten the data loading and preprocessing time, and accelerate the model training. Next, we will explain how to get the results synchronously and asynchronously from job function, and how to write callback functions in asynchronous jobs. At the end of the article, we will provide a complete example. The main points are\uff1a Getting results synchronously or asynchronously is determined by the return value type of job function The data type of return value is selected in oneflow.typing When we call a job function, the form of getting results synchronously / asynchronously is slightly different","title":"Asynchronization"},{"location":"single_client/basics_topics/async_get.html#get-result-synchronously","text":"When we define a job function, if the annotation of the return type of the job function is oneflow.typing.Numpy , the job function will be called synchronously. For example, when we define a job function like this: @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : with flow . scope . placement ( \"gpu\" , \"0:0\" ): logits = lenet ( images , train = True ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , logits , name = \"softmax_loss\" ) lr_scheduler = flow . optimizer . PiecewiseConstantScheduler ([], [ 0.1 ]) flow . optimizer . SGD ( lr_scheduler , momentum = 0 ) . minimize ( loss ) return loss Through Python annotations, OneFlow knows the type of the job function's return is oneflow.typing.Numpy , which corresponds to ndarray in Numpy . Then when we call the job function, it will simply return the ndarray object: loss = train_job ( images , labels ) if i % 20 == 0 : print ( loss . mean ()) From the example above, it should be noted that: When we define the job function, the return value of job function (loss) is a placeholder for graph construction. When we specify the return value type as oneflow.typing.Numpy . OneFlow will know that when the job function is called, the real data type returned is Numpy.ndarray object. By calling the job function train_job(images, labels) , we can get the result from job function directly, and the retnred value is a ndarray object corresponding to oneflow.typing.Numpy .","title":"Get Result Synchronously"},{"location":"single_client/basics_topics/async_get.html#data-types-in-oneflowtyping","text":"The oneflow.typing contains all the data types that can be returned by the job function. The oneflow.typing.Numpy shown above is only one of them. The commonly used types and their corresponding meanings are listed as follows: oneflow.typing.Numpy : corresponding to a numpy.ndarray flow.typing.ListNumpy : corresponding to a list container. Each element in it is a numpy.ndarray object. It is related to the view of OneFlow for distributed training. We will see details in The consistent and mirrored view in distributed training. oneflow.typing.ListListNumpy \uff1acorresponds to a list container where each element is a TensorList object and some interfaces to OneFlow need to process or return multiple TensorList . More information refer to Term & Concept in OneFlow and related API documentation oneflow.typing.Callback : corresponding to a callback function. It is used to call job function asynchronously. We will introduce it below. In addition, OneFlow also allows job functions to pass out data in dictionary form. For examples of ListNumpy , ListNumpy , ListListNumpy and how to pass out data in dictionary form please refer to OneFlow's Test Case .","title":"Data Types in oneflow.typing"},{"location":"single_client/basics_topics/async_get.html#get-result-asynchronously","text":"Generally speaking, the efficiency of asynchronous training is higher than that of synchronous training. The following describes how to call job function asynchronously and process the results. The basic steps include: Prepare a callback function: It is necessary to specify the parameters accepted by the callback function by annotations and implementing the logic for return value of the job function inside the callback function. Implementing a Job Function: Specify flow.typing.Callback as the return type of the job function by annotation. As we will see in the following example that we can specify the parameter type of the callback function with Callback . Call the job function: Register the callback function prepared in step 1 above. The first three steps are completed by users of OneFlow. When the program runs, the registered callback function is called by OneFlow and the return values of the job function are passed as parameters to the callback function.","title":"Get Result Asynchronously"},{"location":"single_client/basics_topics/async_get.html#prepare-a-callback-function","text":"Suppose the prototype of the callback function is: def cb_func ( result : T ): #... Among them, the result needs to be annotated to specify its type T , that is, numpy, listnumpy, etc. mentioned above, or their composite type. We will have corresponding examples below. The result of callback function is actually the return value of job function. Therefore, it must be consistent with the annotation of the return value of the job function For example, when we define a job function below: @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Callback [ tp . Numpy ]: # mlp initializer = flow . truncated_normal ( 0.1 ) reshape = flow . reshape ( images , [ images . shape [ 0 ], - 1 ]) hidden = flow . layers . dense ( reshape , 512 , activation = flow . nn . relu , kernel_initializer = initializer , name = \"hidden\" , ) logits = flow . layers . dense ( hidden , 10 , kernel_initializer = initializer , name = \"output\" ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , logits , name = \"softmax_loss\" ) lr_scheduler = flow . optimizer . PiecewiseConstantScheduler ([], [ 0.1 ]) flow . optimizer . SGD ( lr_scheduler , momentum = 0 ) . minimize ( loss ) return loss Annotation -> tp.Callback[tp.Numpy] means that this job function returns a oneflow.typing.Numpy type, and need to be called asynchronously. Thus, the callback function we defined should accept a numpy parameter: def cb_print_loss ( result : tp . Numpy ): global g_i if g_i % 20 == 0 : print ( result . mean ()) g_i += 1 Let's take a look at another example. If the job function is defined as below: @flow . global_function ( type = \"predict\" ) def eval_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Callback [ Tuple [ tp . Numpy , tp . Numpy ]]: with flow . scope . placement ( \"cpu\" , \"0:0\" ): logits = mlp ( images ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , logits , name = \"softmax_loss\" ) return ( labels , logits ) Annotation -> tp.Callback[Tuple[tp.Numpy, tp.Numpy]] means that this job function returns a tuple and each element is tp.Numpy . The job function needs to be called asynchronously. Thus, the parameter annotation of the corresponding callback function should be: g_total = 0 g_correct = 0 def acc ( arguments : Tuple [ tp . Numpy , tp . Numpy ]): global g_total global g_correct labels = arguments [ 0 ] logits = arguments [ 1 ] predictions = np . argmax ( logits , 1 ) right_count = np . sum ( predictions == labels ) g_total += labels . shape [ 0 ] g_correct += right_count The arguments corresponds to the return type of the above job function.","title":"Prepare a Callback Function"},{"location":"single_client/basics_topics/async_get.html#registration-of-callback-function","text":"When we call the job function asynchronously, the job function will return a 'callback' object, and we register the prepared callback function by passing it to that object. OneFlow will automatically call the registered callback function when it gets the training results. callbacker = train_job ( images , labels ) callbacker ( cb_print_loss ) But the above code is redundant, we recommend: train_job ( images , labels )( cb_print_loss )","title":"Registration of Callback Function"},{"location":"single_client/basics_topics/async_get.html#code","text":"","title":"Code"},{"location":"single_client/basics_topics/async_get.html#get-single-result-synchronously","text":"We use LeNet as an example here to show how to get the return value loss synchronously and print the loss every 20 iterations. Code example: synchronize_single_job.py Run: wget https://docs.oneflow.org/master/code/basics_topics/synchronize_single_job.py python3 synchronize_single_job.py There will be outputs like: File mnist.npz already exist, path: ./mnist.npz 7.3258467 2.1435719 1.1712438 0.7531896 ... ... model saved","title":"Get single result synchronously"},{"location":"single_client/basics_topics/async_get.html#_1","text":"","title":""},{"location":"single_client/basics_topics/async_get.html#get-mutiple-results-synchronously","text":"In this case, the job function returns a tuple . We get the results labels and logits in tuple synchronously. Also, we evaluate the trained model in the above example, then output the accuracy rate. Code: synchronize_batch_job.py The trained model can be downloaded from lenet_models_1.zip Run: wget https://oneflow-public.oss-cn-beijing.aliyuncs.com/online_document/docs/quick_start/lenet_models_1.zip unzip lenet_models_1.zip wget https://docs.oneflow.org/master/code/basics_topics/synchronize_batch_job.py python3 synchronize_batch_job.py There will be outputs like: accuracy: 99.3%","title":"Get Mutiple Results Synchronously"},{"location":"single_client/basics_topics/async_get.html#_2","text":"","title":""},{"location":"single_client/basics_topics/async_get.html#get-single-result-asynchronously","text":"In this case, we take MLP as example to get the single return result loss from job function asynchronously, and the loss is printed every 20 rounds. Code: async_single_job.py Run: wget https://docs.oneflow.org/master/code/basics_topics/async_single_job.py python3 async_single_job.py There will be outputs like: File mnist.npz already exist, path: ./mnist.npz 3.0865736 0.8949808 0.47858357 0.3486296 ...","title":"Get Single Result Asynchronously"},{"location":"single_client/basics_topics/async_get.html#get-multiple-results-asynchronously","text":"In the following example, we will show how to get multiple return results of the job function asynchronously, evaluate the trained model in the above example, and get the accuracy. Code: async_batch_job.py The trained model can be downloaded from mlp_models_1.zip ]( https://oneflow-public.oss-cn-beijing.aliyuncs.com/online_document/docs/basics_topics/mlp_models_1.zip ) Run: wget https://oneflow-public.oss-cn-beijing.aliyuncs.com/online_document/docs/basics_topics/mlp_models_1.zip unzip mlp_models_1.zip wget https://docs.oneflow.org/master/code/basics_topics/async_batch_job.py python3 async_batch_job.py There would be outputs like: File mnist.npz already exist, path: ./mnist.npz accuracy: 97.6%","title":"Get Multiple Results Asynchronously"},{"location":"single_client/basics_topics/build_nn_with_op_and_layer.html","text":"Build a Neural Network \u00b6 In the article Recognition of MNIST Handwritten Digits , we have used \"operator\" in oneflow.nn and \"layer\" in oneflow.layers to build a LeNet neural network. Now we will use this simple neural network to introduce the core element for network construction in OneFlow: operator and layer. LeNet is constructed by convolution layer, pooling layer and fully connected layer. There are two types of elements in the above diagram, one is the computing units represented by boxes which including Op and Layer such as conv2d , dense , max_pool2d and etc. The other is the data represented by arrows. It corresponds to the following code: def lenet ( data , train = False ): initializer = flow . truncated_normal ( 0.1 ) conv1 = flow . layers . conv2d ( data , 32 , 5 , padding = \"SAME\" , activation = flow . nn . relu , name = \"conv1\" , kernel_initializer = initializer , ) pool1 = flow . nn . max_pool2d ( conv1 , ksize = 2 , strides = 2 , padding = \"SAME\" , name = \"pool1\" , data_format = \"NCHW\" ) conv2 = flow . layers . conv2d ( pool1 , 64 , 5 , padding = \"SAME\" , activation = flow . nn . relu , name = \"conv2\" , kernel_initializer = initializer , ) pool2 = flow . nn . max_pool2d ( conv2 , ksize = 2 , strides = 2 , padding = \"SAME\" , name = \"pool2\" , data_format = \"NCHW\" ) reshape = flow . reshape ( pool2 , [ pool2 . shape [ 0 ], - 1 ]) hidden = flow . layers . dense ( reshape , 512 , activation = flow . nn . relu , kernel_initializer = initializer , name = \"dense1\" , ) if train : hidden = flow . nn . dropout ( hidden , rate = 0.5 , name = \"dropout\" ) return flow . layers . dense ( hidden , 10 , kernel_initializer = initializer , name = \"dense2\" ) When the job function is running, data's shape is 100x1\u00d728\u00d728 . Data is firstly used as input in conv2d to participate in the convolution calculation and the result from conv1 is passed to max_pool2d as input. Operator and Layer \u00b6 Operator is a common concept. It is the basic calculation unit in OneFlow. reshape and nn.max_pool2d used in LeNet code are two kinds of operators. In contrast, layers.conv2d and layers.dense are not operator. They are layers which constructed by specific operators. The existence of layers makes it easier to build neural networks. For more details please refer to oneflow.layers API By reading oneflow.layers source code , you can learn the details of building a layer of calculations from basic operators. Data block in neural network \u00b6 OneFlow's default mode is a static graph mechanism and the network is actually built and run separately. As a result, when defining the network, there is no real data in each variable, which means they are just placeholders. The computation of the real data occurs during the call of the job function. When building the network by defining job function, we only describe the attributes and shapes(such as shape , dtype ) of the nodes in network. There is no data in the node, we call the node as PlaceHolder , OneFlow can compile and infer according to these placeholders to get the computation graph. The placeholders are usually called Blob in OneFlow. There is a corresponding base class BlobDef in OneFlow. When we build network, we can print the information of Blob . For example, we can print data shape and dtype as follow. print ( conv1 . shape , conv1 . dtype ) Operator Overloading \u00b6 The BlobDef class implements operator overloading which means BlobDef supports math operators such as addition, subtraction, multiplication and division and so on. Like '+' in the following code: output = output + fc2_biases which is same as: output = flow.broadcast_add(output, fc2_biases) Summary \u00b6 Neural network builds by OneFlow require the operators or layers provided by OneFlow as computing units. The placeholder Blob serves as input and output for the operators and layers. Also operator reloading helps simplify some of the statements. The operators provided by OneFlow can be found in the API documentation: oneflow.nn \u3001 oneflow.math \u3001 oneflow.layers","title":"Build a Neural Network"},{"location":"single_client/basics_topics/build_nn_with_op_and_layer.html#build-a-neural-network","text":"In the article Recognition of MNIST Handwritten Digits , we have used \"operator\" in oneflow.nn and \"layer\" in oneflow.layers to build a LeNet neural network. Now we will use this simple neural network to introduce the core element for network construction in OneFlow: operator and layer. LeNet is constructed by convolution layer, pooling layer and fully connected layer. There are two types of elements in the above diagram, one is the computing units represented by boxes which including Op and Layer such as conv2d , dense , max_pool2d and etc. The other is the data represented by arrows. It corresponds to the following code: def lenet ( data , train = False ): initializer = flow . truncated_normal ( 0.1 ) conv1 = flow . layers . conv2d ( data , 32 , 5 , padding = \"SAME\" , activation = flow . nn . relu , name = \"conv1\" , kernel_initializer = initializer , ) pool1 = flow . nn . max_pool2d ( conv1 , ksize = 2 , strides = 2 , padding = \"SAME\" , name = \"pool1\" , data_format = \"NCHW\" ) conv2 = flow . layers . conv2d ( pool1 , 64 , 5 , padding = \"SAME\" , activation = flow . nn . relu , name = \"conv2\" , kernel_initializer = initializer , ) pool2 = flow . nn . max_pool2d ( conv2 , ksize = 2 , strides = 2 , padding = \"SAME\" , name = \"pool2\" , data_format = \"NCHW\" ) reshape = flow . reshape ( pool2 , [ pool2 . shape [ 0 ], - 1 ]) hidden = flow . layers . dense ( reshape , 512 , activation = flow . nn . relu , kernel_initializer = initializer , name = \"dense1\" , ) if train : hidden = flow . nn . dropout ( hidden , rate = 0.5 , name = \"dropout\" ) return flow . layers . dense ( hidden , 10 , kernel_initializer = initializer , name = \"dense2\" ) When the job function is running, data's shape is 100x1\u00d728\u00d728 . Data is firstly used as input in conv2d to participate in the convolution calculation and the result from conv1 is passed to max_pool2d as input.","title":"Build a Neural Network"},{"location":"single_client/basics_topics/build_nn_with_op_and_layer.html#operator-and-layer","text":"Operator is a common concept. It is the basic calculation unit in OneFlow. reshape and nn.max_pool2d used in LeNet code are two kinds of operators. In contrast, layers.conv2d and layers.dense are not operator. They are layers which constructed by specific operators. The existence of layers makes it easier to build neural networks. For more details please refer to oneflow.layers API By reading oneflow.layers source code , you can learn the details of building a layer of calculations from basic operators.","title":"Operator and Layer"},{"location":"single_client/basics_topics/build_nn_with_op_and_layer.html#data-block-in-neural-network","text":"OneFlow's default mode is a static graph mechanism and the network is actually built and run separately. As a result, when defining the network, there is no real data in each variable, which means they are just placeholders. The computation of the real data occurs during the call of the job function. When building the network by defining job function, we only describe the attributes and shapes(such as shape , dtype ) of the nodes in network. There is no data in the node, we call the node as PlaceHolder , OneFlow can compile and infer according to these placeholders to get the computation graph. The placeholders are usually called Blob in OneFlow. There is a corresponding base class BlobDef in OneFlow. When we build network, we can print the information of Blob . For example, we can print data shape and dtype as follow. print ( conv1 . shape , conv1 . dtype )","title":"Data block in neural network"},{"location":"single_client/basics_topics/build_nn_with_op_and_layer.html#operator-overloading","text":"The BlobDef class implements operator overloading which means BlobDef supports math operators such as addition, subtraction, multiplication and division and so on. Like '+' in the following code: output = output + fc2_biases which is same as: output = flow.broadcast_add(output, fc2_biases)","title":"Operator Overloading"},{"location":"single_client/basics_topics/build_nn_with_op_and_layer.html#summary","text":"Neural network builds by OneFlow require the operators or layers provided by OneFlow as computing units. The placeholder Blob serves as input and output for the operators and layers. Also operator reloading helps simplify some of the statements. The operators provided by OneFlow can be found in the API documentation: oneflow.nn \u3001 oneflow.math \u3001 oneflow.layers","title":"Summary"},{"location":"single_client/basics_topics/concept_explanation.html","text":"Term & Concept in OneFlow \u00b6 In this article, we will explain some common terms and concepts in OneFlow. The main content is divided for algorithm engineers and framework developers: Algorithm Development Framework Development In algorithms development part, we will explain some common terms and concepts used in the process of deep learning algorithms development. And in framework development part, we will focus on the inner design concepts of OneFlow and some relevant element concepts. Algorithms Development \u00b6 1.Placeholder \u00b6 Placeholder is data placeholder . This concept is used to store the shape of input or output data. There is no actual data in Placeholder. For example: import oneflow.typing as tp def test_job ( images : tp . Numpy . Placeholder (( 32 , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( 32 ,), dtype = flow . int32 ), ) -> Tuple [ tp . Numpy , tp . Numpy ]: # do something with images or labels return ( images , labels ) The code above shows that the images shape is (32, 1, 28, 28) and images data type is flow.float32 , the labels shape is (32,) and labels data type is flow.int32 . 2.Tensor and Blob \u00b6 Tensor is a common concept in other framework. In PyTorch, Tensor contains the data, data type, grad, storing device and other attributes. Tensor can be used to create and describe the computation graph in forward and backward process. OneFlow Tensor is basically the same, but there are some difference. In order to provide sufficient support for distributed system and parallelism, the Tensor in OneFlow is more complex and have more types and attributes (Such as logical, physical, devices and attributes of distribution). The Tensor at logical level could be divided to different devices. In order to simplify description, OneFlow hides the different types of Tensor, all the things are defined by a higher level concept named Blob. In OneFlow, Blob has a base class BlobDef . You can print the attributes of Blob when building network. As in the following code, we can print conv1 's shape and dtype : print ( conv1 . shape , conv1 . dtype ) Blob can be Placeholder at compile time, but can also contains values at running time. 3.Job Function \u00b6 In OneFlow, we call the training, evaluating, predicting and inference tasks as job function. Job function connects logic of user and computing resources that managed by OneFlow. In OneFlow, we can use decorator @oneflow.global_function to convert a function to a job function. By this decorator, we can not only define the type of job function(such as: type=\"train\" ), but also bind a FunctionConfig object to set the configuration of job function which can help OneFlow to manage memory and device resources. 4.Layer and Operator \u00b6 Layer \u00b6 The layer concept in OneFlow is basically the same as the layer in TensorFlow, PyTorch and other popular deep learning framework. It is used to describe a layer in neural network such as convolution layer, batch normalization layer, fully connected layer and normalization layer. Layer can simplify the process of building neural network. For example you can use just few lines of code to build LeNet: def lenet ( data , train = False ): initializer = flow . truncated_normal ( 0.1 ) conv1 = flow . layers . conv2d ( data , 32 , 5 , padding = \"SAME\" , activation = flow . nn . relu , name = \"conv1\" , kernel_initializer = initializer , ) pool1 = flow . nn . max_pool2d ( conv1 , ksize = 2 , strides = 2 , padding = \"SAME\" , name = \"pool1\" , data_format = \"NCHW\" ) conv2 = flow . layers . conv2d ( pool1 , 64 , 5 , padding = \"SAME\" , activation = flow . nn . relu , name = \"conv2\" , kernel_initializer = initializer , ) pool2 = flow . nn . max_pool2d ( conv2 , ksize = 2 , strides = 2 , padding = \"SAME\" , name = \"pool2\" , data_format = \"NCHW\" ) reshape = flow . reshape ( pool2 , [ pool2 . shape [ 0 ], - 1 ]) hidden = flow . layers . dense ( reshape , 512 , activation = flow . nn . relu , kernel_initializer = initializer , name = \"dense1\" , ) if train : hidden = flow . nn . dropout ( hidden , rate = 0.5 , name = \"dropout\" ) return flow . layers . dense ( hidden , 10 , kernel_initializer = initializer , name = \"dense2\" ) Layer is composed by operators. For example: layers.conv2d is composed by conv2d and variable . Operator \u00b6 Operator is the basic calculation unit in OneFlow. The calculation in above example is completed by operators. flow.nn.max_pool2d and flow.reshape are two kinds of operators. 5.Mirrored View and Consistent View \u00b6 OneFlow use two types of view: Mirrored View and Consistent View . They are used to describe the distribution of data and model under distributed system. Different view is corresponding to different parallelism view. Mirrored View comes from mirrors view of MPI. It is used to describe the mirrored model to multiple devices when using data parallelism. Consistent View regards multi hosts and devices as one in distributed environment. From this view, OneFlow will hide the detailed parallelism strategy and try to find the optimal parallelism plan(data/model/hybrid parallelism). Basically: When we set the mirrored view ( flow.scope.mirrored_view ), it means we can only use data parallelism . For example, when we set one host and four devices in job function, the model will be broadcasted to all devices, the data will be divided into four parts and send to each device. When set consistent view ( flow.scope.consistent_view ), OneFlow can choose data parallelism, model parallelism or hybrid parallelism by its compiler. Framework developing \u00b6 1.Boxing \u00b6 The module responsible for splitting or merging data blob according the parallelism strategy. We called it Boxing . Such as: When the op of upstream and downstream has different parallelism feature (such as different parallelism number), OneFlow will use Boxing to automatic process the data conversion and transmission. 2.SBP \u00b6 All the forward and backward operations in neural network can be calculated by matrix. In block matrix calculation, matrix needs split and broadcast operations at different axises. OneFlow operator has an attribute to describe this, we call it SBP. Of course, the SBP in OneFlow is not only matrix calculation. It also corresponding to divided data into different devices, broadcast and some other operations. SBP is abbreviated for Split, Broadcast, Partial sum. Split \u00b6 In parallelism operations, tensor is divided into many sub tensors. Different operators allow tensor to be divided on different axis. Boxing will automatically handle the splitting of tensor on different axis. Broadcast \u00b6 In parallelism operator calculation, the tensor will be broadcasted to many devices. This makes the tensor be same on each device. Partial Sum \u00b6 If an operator has distributive property, different part of tensor can be simply added. 3.TensorBuffer and TensorList \u00b6 Base on static map mechanism, OneFlow can infer the tensor shape of each operator and distribute the memory in advance when compiling. It can achieve zero copies of memory when running programs. But in some specific scenarios, OneFlow needs handle growing data. For example, the shape of the image loaded by DataLoader is unknown when compiling. In order to handle the growing data, OneFlow have two type of data format which is TensorBuffer and TensorList . TensorBuffer \u00b6 TensorBuffer is a flexible data format. When using TensorBuffer. We need to specify the dimension of the instance. OneFlow will generate a corresponding TensorBuffer object for each instance. TensorBuffer will indirectly references memory data and the memory section is dynamic and discontinuous . TensorList \u00b6 Similar with TensorBuffer, TensorList also can store the growing data. The main difference is that the data of TensorList is continuous in memory.","title":"Term & Concept Explanation"},{"location":"single_client/basics_topics/concept_explanation.html#term-concept-in-oneflow","text":"In this article, we will explain some common terms and concepts in OneFlow. The main content is divided for algorithm engineers and framework developers: Algorithm Development Framework Development In algorithms development part, we will explain some common terms and concepts used in the process of deep learning algorithms development. And in framework development part, we will focus on the inner design concepts of OneFlow and some relevant element concepts.","title":"Term &amp; Concept in OneFlow"},{"location":"single_client/basics_topics/concept_explanation.html#algorithms-development","text":"","title":"Algorithms Development"},{"location":"single_client/basics_topics/concept_explanation.html#1placeholder","text":"Placeholder is data placeholder . This concept is used to store the shape of input or output data. There is no actual data in Placeholder. For example: import oneflow.typing as tp def test_job ( images : tp . Numpy . Placeholder (( 32 , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( 32 ,), dtype = flow . int32 ), ) -> Tuple [ tp . Numpy , tp . Numpy ]: # do something with images or labels return ( images , labels ) The code above shows that the images shape is (32, 1, 28, 28) and images data type is flow.float32 , the labels shape is (32,) and labels data type is flow.int32 .","title":"1.Placeholder"},{"location":"single_client/basics_topics/concept_explanation.html#2tensor-and-blob","text":"Tensor is a common concept in other framework. In PyTorch, Tensor contains the data, data type, grad, storing device and other attributes. Tensor can be used to create and describe the computation graph in forward and backward process. OneFlow Tensor is basically the same, but there are some difference. In order to provide sufficient support for distributed system and parallelism, the Tensor in OneFlow is more complex and have more types and attributes (Such as logical, physical, devices and attributes of distribution). The Tensor at logical level could be divided to different devices. In order to simplify description, OneFlow hides the different types of Tensor, all the things are defined by a higher level concept named Blob. In OneFlow, Blob has a base class BlobDef . You can print the attributes of Blob when building network. As in the following code, we can print conv1 's shape and dtype : print ( conv1 . shape , conv1 . dtype ) Blob can be Placeholder at compile time, but can also contains values at running time.","title":"2.Tensor and Blob"},{"location":"single_client/basics_topics/concept_explanation.html#3job-function","text":"In OneFlow, we call the training, evaluating, predicting and inference tasks as job function. Job function connects logic of user and computing resources that managed by OneFlow. In OneFlow, we can use decorator @oneflow.global_function to convert a function to a job function. By this decorator, we can not only define the type of job function(such as: type=\"train\" ), but also bind a FunctionConfig object to set the configuration of job function which can help OneFlow to manage memory and device resources.","title":"3.Job Function"},{"location":"single_client/basics_topics/concept_explanation.html#4layer-and-operator","text":"","title":"4.Layer and Operator"},{"location":"single_client/basics_topics/concept_explanation.html#layer","text":"The layer concept in OneFlow is basically the same as the layer in TensorFlow, PyTorch and other popular deep learning framework. It is used to describe a layer in neural network such as convolution layer, batch normalization layer, fully connected layer and normalization layer. Layer can simplify the process of building neural network. For example you can use just few lines of code to build LeNet: def lenet ( data , train = False ): initializer = flow . truncated_normal ( 0.1 ) conv1 = flow . layers . conv2d ( data , 32 , 5 , padding = \"SAME\" , activation = flow . nn . relu , name = \"conv1\" , kernel_initializer = initializer , ) pool1 = flow . nn . max_pool2d ( conv1 , ksize = 2 , strides = 2 , padding = \"SAME\" , name = \"pool1\" , data_format = \"NCHW\" ) conv2 = flow . layers . conv2d ( pool1 , 64 , 5 , padding = \"SAME\" , activation = flow . nn . relu , name = \"conv2\" , kernel_initializer = initializer , ) pool2 = flow . nn . max_pool2d ( conv2 , ksize = 2 , strides = 2 , padding = \"SAME\" , name = \"pool2\" , data_format = \"NCHW\" ) reshape = flow . reshape ( pool2 , [ pool2 . shape [ 0 ], - 1 ]) hidden = flow . layers . dense ( reshape , 512 , activation = flow . nn . relu , kernel_initializer = initializer , name = \"dense1\" , ) if train : hidden = flow . nn . dropout ( hidden , rate = 0.5 , name = \"dropout\" ) return flow . layers . dense ( hidden , 10 , kernel_initializer = initializer , name = \"dense2\" ) Layer is composed by operators. For example: layers.conv2d is composed by conv2d and variable .","title":"Layer"},{"location":"single_client/basics_topics/concept_explanation.html#operator","text":"Operator is the basic calculation unit in OneFlow. The calculation in above example is completed by operators. flow.nn.max_pool2d and flow.reshape are two kinds of operators.","title":"Operator"},{"location":"single_client/basics_topics/concept_explanation.html#5mirrored-view-and-consistent-view","text":"OneFlow use two types of view: Mirrored View and Consistent View . They are used to describe the distribution of data and model under distributed system. Different view is corresponding to different parallelism view. Mirrored View comes from mirrors view of MPI. It is used to describe the mirrored model to multiple devices when using data parallelism. Consistent View regards multi hosts and devices as one in distributed environment. From this view, OneFlow will hide the detailed parallelism strategy and try to find the optimal parallelism plan(data/model/hybrid parallelism). Basically: When we set the mirrored view ( flow.scope.mirrored_view ), it means we can only use data parallelism . For example, when we set one host and four devices in job function, the model will be broadcasted to all devices, the data will be divided into four parts and send to each device. When set consistent view ( flow.scope.consistent_view ), OneFlow can choose data parallelism, model parallelism or hybrid parallelism by its compiler.","title":"5.Mirrored View and Consistent View"},{"location":"single_client/basics_topics/concept_explanation.html#framework-developing","text":"","title":"Framework developing"},{"location":"single_client/basics_topics/concept_explanation.html#1boxing","text":"The module responsible for splitting or merging data blob according the parallelism strategy. We called it Boxing . Such as: When the op of upstream and downstream has different parallelism feature (such as different parallelism number), OneFlow will use Boxing to automatic process the data conversion and transmission.","title":"1.Boxing"},{"location":"single_client/basics_topics/concept_explanation.html#2sbp","text":"All the forward and backward operations in neural network can be calculated by matrix. In block matrix calculation, matrix needs split and broadcast operations at different axises. OneFlow operator has an attribute to describe this, we call it SBP. Of course, the SBP in OneFlow is not only matrix calculation. It also corresponding to divided data into different devices, broadcast and some other operations. SBP is abbreviated for Split, Broadcast, Partial sum.","title":"2.SBP"},{"location":"single_client/basics_topics/concept_explanation.html#split","text":"In parallelism operations, tensor is divided into many sub tensors. Different operators allow tensor to be divided on different axis. Boxing will automatically handle the splitting of tensor on different axis.","title":"Split"},{"location":"single_client/basics_topics/concept_explanation.html#broadcast","text":"In parallelism operator calculation, the tensor will be broadcasted to many devices. This makes the tensor be same on each device.","title":"Broadcast"},{"location":"single_client/basics_topics/concept_explanation.html#partial-sum","text":"If an operator has distributive property, different part of tensor can be simply added.","title":"Partial Sum"},{"location":"single_client/basics_topics/concept_explanation.html#3tensorbuffer-and-tensorlist","text":"Base on static map mechanism, OneFlow can infer the tensor shape of each operator and distribute the memory in advance when compiling. It can achieve zero copies of memory when running programs. But in some specific scenarios, OneFlow needs handle growing data. For example, the shape of the image loaded by DataLoader is unknown when compiling. In order to handle the growing data, OneFlow have two type of data format which is TensorBuffer and TensorList .","title":"3.TensorBuffer and TensorList"},{"location":"single_client/basics_topics/concept_explanation.html#tensorbuffer","text":"TensorBuffer is a flexible data format. When using TensorBuffer. We need to specify the dimension of the instance. OneFlow will generate a corresponding TensorBuffer object for each instance. TensorBuffer will indirectly references memory data and the memory section is dynamic and discontinuous .","title":"TensorBuffer"},{"location":"single_client/basics_topics/concept_explanation.html#tensorlist","text":"Similar with TensorBuffer, TensorList also can store the growing data. The main difference is that the data of TensorList is continuous in memory.","title":"TensorList"},{"location":"single_client/basics_topics/data_input.html","text":"Data Input \u00b6 Machine learning is driven by data. Data loading and preprocessing require both efficiency and scalability. OneFlow supports two methods to load data: One way to do this is to pass a Numpy ndarray object as a parameter to the job function directly. Another approach is to use DataLoader of OneFlow and its related operators. It can load and pre-process datasets of a particular format from the file system. Working directly with Numpy data is easy and convenient but only for small amounts of data. Because when the amount of data is too large, there may be barrier in preparing the Numpy data. Therefore, this approach is more suitable for the initial stages of the project to quickly validate and improve the algorithm. The DataLoader of OneFlow use techniques such as multi-threading and data pipelining which make data loading, data pre-processing more efficient.However, you need to prepare dataset which already supported by Oneflow. Thus we recommend use that in mature projects. Use Numpy as Data Input \u00b6 Example \u00b6 We can directly use Numpy ndarray as data input during training or predicting with OneFlow: # feed_numpy.py import numpy as np from oneflow.compatible import single_client as flow from oneflow.compatible.single_client import typing as tp from typing import Tuple @flow . global_function ( type = \"predict\" ) def test_job ( images : tp . Numpy . Placeholder (( 32 , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( 32 ,), dtype = flow . int32 ), ) -> Tuple [ tp . Numpy , tp . Numpy ]: # do something with images or labels return ( images , labels ) if __name__ == \"__main__\" : images_in = np . random . uniform ( - 10 , 10 , ( 32 , 1 , 28 , 28 )) . astype ( np . float32 ) labels_in = np . random . randint ( - 10 , 10 , ( 32 ,)) . astype ( np . int32 ) images , labels = test_job ( images_in , labels_in ) print ( images . shape , labels . shape ) You can download code from feed_numpy.py and run it by: python3 feed_numpy.py Following output are expected: ( 32 , 1 , 28 , 28 ) ( 32 , ) Code Explanation \u00b6 In the above code, we defined a job function test_job() with images and labels as inputs and annotate (note that the formal parameter is followed by \u201c:\u201d , not \u201c=\u201d) to specifies the shape and data type of the data. Thus, the example generates Numpy data randomly ( images_in and labels_in ) according to the shape and data type requirements of the job function. images_in = np . random . uniform ( - 10 , 10 , ( 32 , 1 , 28 , 28 )) . astype ( np . float32 ) labels_in = np . random . randint ( - 10 , 10 , ( 32 , )) . astype ( np . int32 ) Then directly pass the Numpy data images_in and labels_in as parameters when the job function is called. images , labels = test_job ( images_in , labels_in ) The oneflow.typing.Numpy.Placeholder is the placeholder of Numpy ndarray . There are also various placeholders in OneFlow that can represent more complex forms of Numpy data. More details please refer to The Definition and Call of Job Function . Using DataLoader and Related Operators \u00b6 Under the oneflow.data module, there are DataLoader operators for loading datasets and associated data preprocessing operators.DataLoader is usually named as data.xxx_reader , such as the existing data.ofrecord_reader and data.coco_reader which support OneFlow's native OFRecord format and COCO dataset. In addition, there are other data preprocessing operators that are used to process the data after DataLoader has been loaded. The following code uses data.OFRecordImageDecoderRandomCrop for random image cropping and data.OFRecordRawDecoder for image decoding. You can refer to the API documentation for more details. Examples \u00b6 The following example reads the OFRecord data format file and dealing with images from the ImageNet dataset. The complete code can be downloaded here: of_data_pipeline.py . This script requires an OFRecord dataset and you can make your own one according to this article . Or you can download the part-00000 that we have prepared for you which contains 64 images. Then replace path/to/ImageNet/ofrecord in the script with the directory where the part-00000 file is located and run the script. The following example is running a script with our pre-prepared dataset: wget https://oneflow-public.oss-cn-beijing.aliyuncs.com/online_document/docs/basics_topics/part-00000 sed -i \"s:path/to/ImageNet/ofrecord:./:\" of_data_pipeline.py python3 of_data_pipeline.py The following output are expected: (64, 3, 224, 224) (64,) Code Explanation \u00b6 There are generally two stages in using OneFlow DataLoader: Load Data and Preprocessing Data . flow.data.ofrecord_reader in the script is responsible for loading data from the file system into memory. ofrecord = flow . data . ofrecord_reader ( \"path/to/ImageNet/ofrecord\" , batch_size = batch_size , data_part_num = 1 , part_name_suffix_length = 5 , random_shuffle = True , shuffle_after_epoch = True , ) To specify the directory where the OFRecord file is located and some other parameters please refer to data.ofrecord_reader . If the return value of the DataLoader is a basic data type. Then it can be used directly as an input to the downstream operator. Otherwise the data preprocessing operator needs to be called further for preprocessing. For example, in the script: image = flow . data . OFRecordImageDecoderRandomCrop ( ofrecord , \"encoded\" , color_space = color_space ) label = flow . data . OFRecordRawDecoder ( ofrecord , \"class/label\" , shape = (), dtype = flow . int32 ) rsz = flow . image . Resize ( image , resize_x = 224 , resize_y = 224 , color_space = color_space ) rng = flow . random . CoinFlip ( batch_size = batch_size ) normal = flow . image . CropMirrorNormalize ( rsz , mirror_blob = rng , color_space = color_space , mean = [ 123.68 , 116.779 , 103.939 ], std = [ 58.393 , 57.12 , 57.375 ], output_dtype = flow . float , ) OFRecordImageDecoderRandomCrop is responsible for randomly cropping the image, OFRecordRawDecoder is responsible for decoding the label directly from the ofrecord object. image.Resize resizes the cropped image to 224x224 and CropMirrorNormalize normalizes the image. More Formats Support by DataLoader \u00b6 OneFlow provides a number of DataLoaders and preprocessing operators, refer to oneflow.data for details. These operators will be enriched and optimized in the future.","title":"Data Input"},{"location":"single_client/basics_topics/data_input.html#data-input","text":"Machine learning is driven by data. Data loading and preprocessing require both efficiency and scalability. OneFlow supports two methods to load data: One way to do this is to pass a Numpy ndarray object as a parameter to the job function directly. Another approach is to use DataLoader of OneFlow and its related operators. It can load and pre-process datasets of a particular format from the file system. Working directly with Numpy data is easy and convenient but only for small amounts of data. Because when the amount of data is too large, there may be barrier in preparing the Numpy data. Therefore, this approach is more suitable for the initial stages of the project to quickly validate and improve the algorithm. The DataLoader of OneFlow use techniques such as multi-threading and data pipelining which make data loading, data pre-processing more efficient.However, you need to prepare dataset which already supported by Oneflow. Thus we recommend use that in mature projects.","title":"Data Input"},{"location":"single_client/basics_topics/data_input.html#use-numpy-as-data-input","text":"","title":"Use Numpy as Data Input"},{"location":"single_client/basics_topics/data_input.html#example","text":"We can directly use Numpy ndarray as data input during training or predicting with OneFlow: # feed_numpy.py import numpy as np from oneflow.compatible import single_client as flow from oneflow.compatible.single_client import typing as tp from typing import Tuple @flow . global_function ( type = \"predict\" ) def test_job ( images : tp . Numpy . Placeholder (( 32 , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( 32 ,), dtype = flow . int32 ), ) -> Tuple [ tp . Numpy , tp . Numpy ]: # do something with images or labels return ( images , labels ) if __name__ == \"__main__\" : images_in = np . random . uniform ( - 10 , 10 , ( 32 , 1 , 28 , 28 )) . astype ( np . float32 ) labels_in = np . random . randint ( - 10 , 10 , ( 32 ,)) . astype ( np . int32 ) images , labels = test_job ( images_in , labels_in ) print ( images . shape , labels . shape ) You can download code from feed_numpy.py and run it by: python3 feed_numpy.py Following output are expected: ( 32 , 1 , 28 , 28 ) ( 32 , )","title":"Example"},{"location":"single_client/basics_topics/data_input.html#code-explanation","text":"In the above code, we defined a job function test_job() with images and labels as inputs and annotate (note that the formal parameter is followed by \u201c:\u201d , not \u201c=\u201d) to specifies the shape and data type of the data. Thus, the example generates Numpy data randomly ( images_in and labels_in ) according to the shape and data type requirements of the job function. images_in = np . random . uniform ( - 10 , 10 , ( 32 , 1 , 28 , 28 )) . astype ( np . float32 ) labels_in = np . random . randint ( - 10 , 10 , ( 32 , )) . astype ( np . int32 ) Then directly pass the Numpy data images_in and labels_in as parameters when the job function is called. images , labels = test_job ( images_in , labels_in ) The oneflow.typing.Numpy.Placeholder is the placeholder of Numpy ndarray . There are also various placeholders in OneFlow that can represent more complex forms of Numpy data. More details please refer to The Definition and Call of Job Function .","title":"Code Explanation"},{"location":"single_client/basics_topics/data_input.html#using-dataloader-and-related-operators","text":"Under the oneflow.data module, there are DataLoader operators for loading datasets and associated data preprocessing operators.DataLoader is usually named as data.xxx_reader , such as the existing data.ofrecord_reader and data.coco_reader which support OneFlow's native OFRecord format and COCO dataset. In addition, there are other data preprocessing operators that are used to process the data after DataLoader has been loaded. The following code uses data.OFRecordImageDecoderRandomCrop for random image cropping and data.OFRecordRawDecoder for image decoding. You can refer to the API documentation for more details.","title":"Using DataLoader and Related Operators"},{"location":"single_client/basics_topics/data_input.html#examples","text":"The following example reads the OFRecord data format file and dealing with images from the ImageNet dataset. The complete code can be downloaded here: of_data_pipeline.py . This script requires an OFRecord dataset and you can make your own one according to this article . Or you can download the part-00000 that we have prepared for you which contains 64 images. Then replace path/to/ImageNet/ofrecord in the script with the directory where the part-00000 file is located and run the script. The following example is running a script with our pre-prepared dataset: wget https://oneflow-public.oss-cn-beijing.aliyuncs.com/online_document/docs/basics_topics/part-00000 sed -i \"s:path/to/ImageNet/ofrecord:./:\" of_data_pipeline.py python3 of_data_pipeline.py The following output are expected: (64, 3, 224, 224) (64,)","title":"Examples"},{"location":"single_client/basics_topics/data_input.html#code-explanation_1","text":"There are generally two stages in using OneFlow DataLoader: Load Data and Preprocessing Data . flow.data.ofrecord_reader in the script is responsible for loading data from the file system into memory. ofrecord = flow . data . ofrecord_reader ( \"path/to/ImageNet/ofrecord\" , batch_size = batch_size , data_part_num = 1 , part_name_suffix_length = 5 , random_shuffle = True , shuffle_after_epoch = True , ) To specify the directory where the OFRecord file is located and some other parameters please refer to data.ofrecord_reader . If the return value of the DataLoader is a basic data type. Then it can be used directly as an input to the downstream operator. Otherwise the data preprocessing operator needs to be called further for preprocessing. For example, in the script: image = flow . data . OFRecordImageDecoderRandomCrop ( ofrecord , \"encoded\" , color_space = color_space ) label = flow . data . OFRecordRawDecoder ( ofrecord , \"class/label\" , shape = (), dtype = flow . int32 ) rsz = flow . image . Resize ( image , resize_x = 224 , resize_y = 224 , color_space = color_space ) rng = flow . random . CoinFlip ( batch_size = batch_size ) normal = flow . image . CropMirrorNormalize ( rsz , mirror_blob = rng , color_space = color_space , mean = [ 123.68 , 116.779 , 103.939 ], std = [ 58.393 , 57.12 , 57.375 ], output_dtype = flow . float , ) OFRecordImageDecoderRandomCrop is responsible for randomly cropping the image, OFRecordRawDecoder is responsible for decoding the label directly from the ofrecord object. image.Resize resizes the cropped image to 224x224 and CropMirrorNormalize normalizes the image.","title":"Code Explanation"},{"location":"single_client/basics_topics/data_input.html#more-formats-support-by-dataloader","text":"OneFlow provides a number of DataLoaders and preprocessing operators, refer to oneflow.data for details. These operators will be enriched and optimized in the future.","title":"More Formats Support by DataLoader"},{"location":"single_client/basics_topics/distributed_train.html","text":"Distributed Training \u00b6 In deep learning, more and more scenarios require distributed training. Since distributed systems face problems such as distributed task scheduling and complex resource parallelism in multiple cards machines. Thus distributed training usually has a certain technical threshold for users. In OneFlow, through top-level design and engineering innovation. It is easiest use distribution system . Users can easily use OneFlow for distributed training without making any special changes to the network structure or job logic. This is the most important feature that make OneFlow different from other frameworks. In this article, we will introduce: How to switch a program platform from a single machine to a distributed system. The concept and mission of node in OneFlow. The Distribution Advantage of OneFlow. \u00b6 OneFlow use decentralized streaming architecture. Not like master and worker architecture, it can optimize the communication efficiency of network to the maximum extent. Support consistent view , the whole network only needs one logic input and output. A mirrored view compatible with other frameworks is provided. Users who are familiar with the distributed training of other frameworks can learn to use it quickly. Only a few lines of configuration code are needed to switch a program platform from a single machine to a distributed system. Configuration of the Distributed Training \u00b6 By the distributed training interface of OneFlow, you only need a few configuration to specify the distributed computing nodes IP and the number of devices for performing distributed training network. In another word, it makes a single machine program and a distributed machine program almost the same in terms of complexity of coding. User just need to focus on job logic and structures of model without worrying about distribution execution. Everything related to distribution is handled by OneFlow. Here is an example to change a program run on a single machine to be run on a distributed system with few configurations. Single Machine Program \u00b6 Here is the framework of single machine training program. Because the code of each function will be presented in the distributed program below, it is not listed in detail here. import numpy as np from oneflow.compatible import single_client as flow from oneflow.compatible.single_client import typing as tp BATCH_SIZE = 100 def mlp ( data ): #build network... @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : #achieve job function ... #Optimization method and parameters configuration training if __name__ == '__main__' : #call job function and start training... loss = train_job ( images , labels ) #... Configuration of Ports and Device \u00b6 In oneflow.config , we provide interfaces related to distributed program. We mainly use two of them: oneflow.config.gpu_device_num : set the number of device. This will be applied to all machines. oneflow.config.ctrl_port : set the port number of communications. All the machines will use the same port. In the following demo, we set all machines to use one device and use the port 9988 for communication. User can change the configuration according to their actual situation. #device number flow . config . gpu_device_num ( 1 ) #Port number flow . env . ctrl_port ( 9988 ) To be mentioned that, if we only have one single machine with multiple GPU devices in it, we can still use flow.config.gpu_device_num to change a program from running on a single machine to run on a distributed system. In the code below, we will use two GPU devices in one machine to do the distributed training: flow . config . gpu_device_num ( 2 ) Node Configuration \u00b6 Then we need to config the connection between the machines in network. In OneFlow, the distributed machine called node . The network information of each node is stored as a dict . The key \"addr\" is corresponding with IP of this node. All nodes are stored in a list , which will be informed to Oneflow by flow.env.machine . OneFlow will automatically generate the connection between nodes. nodes = [{ \"addr\" : \"192.168.1.12\" }, { \"addr\" : \"192.168.1.11\" }] flow . env . machine ( nodes ) In the code above, we have two nodes in our distributed system. Their IP is \"192.168.1.12\" and \"192.168.1.11\". It should be noted that the node 0 in list (in the above code is 192.168.1.12) is called master node . After the whole distributed training system starts, it will create the graph while the other nodes are waiting. When construction of graph is finished, all nodes will receive a notice specifying which nodes that they need to contact. Then they will work together in a decentralized way. During the training process, master node will deal with the standard output and store the model. The other nodes are only responsible for calculation. We can wrap the configuration code for distributed training as a function, which is easy to be called: def config_distributed (): print ( \"distributed config\" ) #number of device used in each node flow . config . gpu_device_num ( 1 ) #communication channel flow . env . ctrl_port ( 9988 ) #node configuration nodes = [{ \"addr\" : \"192.168.1.12\" }, { \"addr\" : \"192.168.1.11\" }] flow . env . machine ( n Complete Code of Distributed Training \u00b6 After adding the configurations code, the program becomes a distributed training one. Just follow the same step as we do in a single machine program. Compared with single machine training program , the distributed training program only needs to call one more function named config_distributed . Distribution script: distributed_train.py Running on both 192.168.1.12 and 192.168.1.11 : wget https://docs.oneflow.org/master/code/basics_topics/distributed_train.py python3 distributed_train.py The result of the program will be displayed on 192.168.1.12 . FAQ \u00b6 After running this distribution code, the program waits for a long time and does not display the calculation results\u3002 Please check the ssh configuration to ensure that the two machines can be interconnected with each other ssh-free. Make sure that both machines are using the same version of OneFlow and are running the exact same script program. Make sure the port used for training is unoccupied or replace the port with oneflow.config.ctrl_port . If a proxy is set in an environment variable, make sure the proxy works or disable it. Run training in docker, program waits for a long time and does not show calculation results. In default mode of docker, the machine is isolated from the ports in the container. Then use --net=host (host mode) or use the -p option for port mapping when starting the container. For details information please refer to the docker manual. The communications library was not installed correctly Make sure the version of the communication library (nccl) is the same on each machine during distributed training. Using virtual network cards If there are virtual network cards, you may not be able to communicate with nccl. In this case, you need to specify the communication network cards by export NCCL_SOCKET_IFNAME=device_name . More details please refer to nccl official documentation .","title":"Distributed training"},{"location":"single_client/basics_topics/distributed_train.html#distributed-training","text":"In deep learning, more and more scenarios require distributed training. Since distributed systems face problems such as distributed task scheduling and complex resource parallelism in multiple cards machines. Thus distributed training usually has a certain technical threshold for users. In OneFlow, through top-level design and engineering innovation. It is easiest use distribution system . Users can easily use OneFlow for distributed training without making any special changes to the network structure or job logic. This is the most important feature that make OneFlow different from other frameworks. In this article, we will introduce: How to switch a program platform from a single machine to a distributed system. The concept and mission of node in OneFlow.","title":"Distributed Training"},{"location":"single_client/basics_topics/distributed_train.html#the-distribution-advantage-of-oneflow","text":"OneFlow use decentralized streaming architecture. Not like master and worker architecture, it can optimize the communication efficiency of network to the maximum extent. Support consistent view , the whole network only needs one logic input and output. A mirrored view compatible with other frameworks is provided. Users who are familiar with the distributed training of other frameworks can learn to use it quickly. Only a few lines of configuration code are needed to switch a program platform from a single machine to a distributed system.","title":"The Distribution Advantage of OneFlow."},{"location":"single_client/basics_topics/distributed_train.html#configuration-of-the-distributed-training","text":"By the distributed training interface of OneFlow, you only need a few configuration to specify the distributed computing nodes IP and the number of devices for performing distributed training network. In another word, it makes a single machine program and a distributed machine program almost the same in terms of complexity of coding. User just need to focus on job logic and structures of model without worrying about distribution execution. Everything related to distribution is handled by OneFlow. Here is an example to change a program run on a single machine to be run on a distributed system with few configurations.","title":"Configuration of the Distributed Training"},{"location":"single_client/basics_topics/distributed_train.html#single-machine-program","text":"Here is the framework of single machine training program. Because the code of each function will be presented in the distributed program below, it is not listed in detail here. import numpy as np from oneflow.compatible import single_client as flow from oneflow.compatible.single_client import typing as tp BATCH_SIZE = 100 def mlp ( data ): #build network... @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : #achieve job function ... #Optimization method and parameters configuration training if __name__ == '__main__' : #call job function and start training... loss = train_job ( images , labels ) #...","title":"Single Machine Program"},{"location":"single_client/basics_topics/distributed_train.html#configuration-of-ports-and-device","text":"In oneflow.config , we provide interfaces related to distributed program. We mainly use two of them: oneflow.config.gpu_device_num : set the number of device. This will be applied to all machines. oneflow.config.ctrl_port : set the port number of communications. All the machines will use the same port. In the following demo, we set all machines to use one device and use the port 9988 for communication. User can change the configuration according to their actual situation. #device number flow . config . gpu_device_num ( 1 ) #Port number flow . env . ctrl_port ( 9988 ) To be mentioned that, if we only have one single machine with multiple GPU devices in it, we can still use flow.config.gpu_device_num to change a program from running on a single machine to run on a distributed system. In the code below, we will use two GPU devices in one machine to do the distributed training: flow . config . gpu_device_num ( 2 )","title":"Configuration of Ports and Device"},{"location":"single_client/basics_topics/distributed_train.html#node-configuration","text":"Then we need to config the connection between the machines in network. In OneFlow, the distributed machine called node . The network information of each node is stored as a dict . The key \"addr\" is corresponding with IP of this node. All nodes are stored in a list , which will be informed to Oneflow by flow.env.machine . OneFlow will automatically generate the connection between nodes. nodes = [{ \"addr\" : \"192.168.1.12\" }, { \"addr\" : \"192.168.1.11\" }] flow . env . machine ( nodes ) In the code above, we have two nodes in our distributed system. Their IP is \"192.168.1.12\" and \"192.168.1.11\". It should be noted that the node 0 in list (in the above code is 192.168.1.12) is called master node . After the whole distributed training system starts, it will create the graph while the other nodes are waiting. When construction of graph is finished, all nodes will receive a notice specifying which nodes that they need to contact. Then they will work together in a decentralized way. During the training process, master node will deal with the standard output and store the model. The other nodes are only responsible for calculation. We can wrap the configuration code for distributed training as a function, which is easy to be called: def config_distributed (): print ( \"distributed config\" ) #number of device used in each node flow . config . gpu_device_num ( 1 ) #communication channel flow . env . ctrl_port ( 9988 ) #node configuration nodes = [{ \"addr\" : \"192.168.1.12\" }, { \"addr\" : \"192.168.1.11\" }] flow . env . machine ( n","title":"Node Configuration"},{"location":"single_client/basics_topics/distributed_train.html#complete-code-of-distributed-training","text":"After adding the configurations code, the program becomes a distributed training one. Just follow the same step as we do in a single machine program. Compared with single machine training program , the distributed training program only needs to call one more function named config_distributed . Distribution script: distributed_train.py Running on both 192.168.1.12 and 192.168.1.11 : wget https://docs.oneflow.org/master/code/basics_topics/distributed_train.py python3 distributed_train.py The result of the program will be displayed on 192.168.1.12 .","title":"Complete Code of Distributed Training"},{"location":"single_client/basics_topics/distributed_train.html#faq","text":"After running this distribution code, the program waits for a long time and does not display the calculation results\u3002 Please check the ssh configuration to ensure that the two machines can be interconnected with each other ssh-free. Make sure that both machines are using the same version of OneFlow and are running the exact same script program. Make sure the port used for training is unoccupied or replace the port with oneflow.config.ctrl_port . If a proxy is set in an environment variable, make sure the proxy works or disable it. Run training in docker, program waits for a long time and does not show calculation results. In default mode of docker, the machine is isolated from the ports in the container. Then use --net=host (host mode) or use the -p option for port mapping when starting the container. For details information please refer to the docker manual. The communications library was not installed correctly Make sure the version of the communication library (nccl) is the same on each machine during distributed training. Using virtual network cards If there are virtual network cards, you may not be able to communicate with nccl. In this case, you need to specify the communication network cards by export NCCL_SOCKET_IFNAME=device_name . More details please refer to nccl official documentation .","title":"FAQ"},{"location":"single_client/basics_topics/essentials_of_oneflow.html","text":"OneFlow System Design \u00b6 In this article, we will cover these topics: Motivation OneFlow feature 1: Runtime based on actor system OneFlow feature 2: Compile-time based on a formal description of parallelism Summary Motivation \u00b6 OneFlow is born for performance and horizontal scalability, especially for multi-nodes and multi-devices scenarios. We expect that users can leverage the power of multiple machines and multiple devices in a way as easy as using a single machine with single device, and enjoy the efficiency of linear speedup. Why does OneFlow focus on the performance and user experience in distributed scenarios? With the development of deep learning, the model becomes increasingly large, and the computing power required to train deep learning models will become higher and higher. The computing power and the memory of a single device are far from meeting the needs of deep learning model training, and multiple machines and multiple devices are required for parallelism speedup. If the deep learning framework can make multiple interconnected devices work well together and achieve linear speedup, even if the performance of each device is just so so, it can also meet the computing power needs of any scale. This is the so-called horizontal scalability or scaling out. We do believe this is the solution to the increasing need of computing power\u3002 However, the existing frameworks usually focus on the user experience of a single device, and only handle the multi-machine and multi-devices scenarios that works for data parallelism. That is, mirroring the computation graph on a single device to multiple machines and multiple devices, synchronizing model with Allreduce. For models with a huge amount of parameters such as BERT/GPT-3, users often find it not friendly to use, hard to deploy and not efficient to train models on multiple machines and multiple devices when using existing deep learning frameworks. It is also time-consuming for users to learn how to do distributed training. They also need to care about the synchronization of models between multiple machines and multiple devices. In order to solve the above problems in distributed deep learning, both industry and academia not only improve the deep learning framework itself, but also develop a variety of third-party plugins, such as NCCL, Horovod, BytePS, HugeCTR, Mesh-tensorflow, Gpipe, etc. However, it still can\u2019t meet users' unlimited pursuit to performance. The core motivation of OneFlow is to make multi-machine and multi-devices distributed training efficiently, and at the same time, to make the distributed training experience as simple as using a single device. Let's introduce the two core ideas of OneFlow, and explain how OneFlow views deep learning training in distributed scenarios. Runtime based on actor system \u00b6 Key features: Decentralized scheduling Pipelining Data movement as a first-class citizen Overlapping data movement and computation Overlapping control and data logic OneFlow consists of two stages: Compile-time and Runtime. In the Compile-time, user-defined neural networks and the requested resource are compiled into a static graph execution plan, which is composed of the description of the basic execution unit Actor ; During the runtime , each machine actually creates many Actor instances located to its own machine based on the Actor description in the Plan, and then started the Actor operating system. In the training procedure, the basic unit of OneFlow execution is Actor, which corresponds to a node of the static execution graph. The data produced and consumed between Actors are stored in the form of Registers , and the Actors cooperate through message passing. Decentralized scheduling \u00b6 OneFlow implements decentralized scheduling through the Actor mechanism. In the entire static graph is composed of actors, there is no central scheduler. Each actor only cares about the producer of the data it needs (upstream Actor) and the consumer of the data it produces (downstream Actor). In this way, in the ultra-large-scale distributed training scenario, completely decentralized scheduling can avoid the single-point performance bottleneck with centralized scheduling. Each Actor has an internal state machine, which updates its status according to the messages sent and received by the Actor. It should be noted that Register is a storage block, which stores the data produced by the Actor, and the message is a lightweight data containing the memory address of the Register storage block. It is message instead of Register that is passed between Actors, in this way, OneFlow runtime achieves zero-copy. When an Actor receives a new message and decides whether the Register it needs to consume is ready, and it has free Register to write the produced data. If yes, the Actor executes (Act) once and produces some new data. After action, the Actor sends a message to the consumer Actors who need to consume the produced Register, indicating that \"you can read the data I produced\"; At the same time, the Actor also needs to return the Register it consumes to its producer, indicating that \"I have used up your data and you can recycle it.\" The state machine inside the Actor is shown in Figure 1. Figure 1 Actor state machine inside After the Actor starts, it will switch its two states according to the messages sent and received with other actors: waiting state and execution state . The messages received by an Actor are generally divided into several types: The upstream producer Actor sends a message saying that you can read the data I produce; the downstream consumer Actor sends a message saying that I have used up the data you produced. When this data are used up by all consumers, it can be recycled as a free block and wait for the Actor to produce a new data in next time. Whenever receiving a message, an Actor will try to decides whether its action conditions are met with. There are generally two action conditions: Whether all the data to be read are available; Whether there are free blocks that can be used for production. When the action state is satisfied, the actor starts to launch its internal Kernel to consume incoming data and produce some new data. After action, the Actor will send messages to upstream and downstream: Send a message to the downstream consumer Actor saying that I just produced a piece of data, you can read them; Send a message to the upstream producer Actor saying that I just used up the data you sent me before. Actors only need to care about upstream and downstream messages to decide whether they can act or not. All Actors form a completely decentralized distributed collaborative network through their own internal state machines and messages exchanging mechanism. Pipelining \u00b6 In above, we introduced the internal finite state machine of Actors. Message passing and data movement between Actors are implemented by Register . Whether an Actor can act only relates to two conditions: Whether the Registers consumed by itself are readable; Whether the Registers produced by itself have free blocks to write. For a Register, if we allocate multiple free blocks for it, two adjacent Actors can work simultaneously. In this way, the overlapping of adjacent actors implements pipelining. In an ideal case, the initiation interval of the entire static execution graph is the execution time of the bottleneck actor's each action, the execution time of all the other actors will be hidden through the pipelining. Let's take an example to explain how the pipelining of the Actor system works. Figure 2 is an execution sequence diagram of a computation graph composed of 3 Actors (a, b, c). The green Regst square represents the Register block being occupied, and the white Regst square represents the free block of the same Register. At Time0, Actor a produces a Regst_a_0, and Actor b and Actor c are in waiting state because they have no readable Register. Here we assume that the execution time of each Actor is the same. At Time1, Actor a sends a message to Actor b saying that you can read the Regst_a_0 that I produced. Actor b receives the message and checks whether there is a free block available in the Register b owned by itself, and finds that there is an available Regst_b_0 , so Actor b executes at Time1, reading Regst_a_0 and writing Regst_b_0; at the same time, Actor a will also check whether it has a free block to write, and finds that it has a free block to write, so Actor a will also begin executing at Time1, writing Regst_a_1. (It should be noted here that Regst_a_0 and Regst_a_1 logically belong to the same Register, but they are spatially divided into different free blocks. In deep learning training task, Regst_a_0 and Regst_a_1 store data belonging to different batches produced by a same producer.) So Actor a and Actor b work in parallel. Actor c is still waiting because there is no data to read. At Time2, Actor b has produced Regst_b_0, so it sends a message to the downstream consumer Actor c that you can read the Regst_b_0 I produced, and at the same time sends a message to the upstream producer Actor a that I have consumed your Regst_a_0 . At the same time, Actor a sends a newly produced Regst_a_1 to Actor b . Actor b checks that it still has Regst_b_1 being free, so Actor b starts to read Regst_a_1 and writes Regst_b_1; Actor c receives Regst_b_0 and finds that it has Regst_c_0 being free, so Actor c starts execution, reading Regst_b_0 and writing Regst_c_0; Actor a receives Regst_a_0 that Actor b has used up and returned the ownership, and checks that all consumers of Regst_a_0 are used up, so Regst_a_0 is recycled and marked as a free block, and Actor a can continue to execute and write Regst_a_2. Figure 2 Actor producer-consumer relationship and execution sequence diagram In the above example, at Time2, Actors a , b , and c are all working simultaneously. In typical deep learning training job, Regst_b_0 and Regst_c_0 at Time2 store the data of Batch 0, and Regst_a_1 and Regst_b_1 store the data of Batch 1. Regst_a_2 stores data of Batch 2. By the design of a Register with multiple free blocks, the Actor naturally supports pipelining. Here we raise a further in-depth problem: in OneFlow, the execution of the entire data flow is like a network, and the data flow throught the network and completes the computation. How to slow down the producer's production if it is too fast for the consumer to consume, and how to avoid the case if the producer's production is too slow, and consumers get hungry. This problem involves planning for computing, memory, and transmission bandwidth, so that the bottleneck of the system is as wide as possible. It relates to flow control and resource allocation (For example, how many memory block quotas are allocated to the Register of each Actor). This is a critical problem which has been solved by the OneFlow system. Data movement as a first-class citizen \u00b6 In a distributed environment with multiple machines and multiple devices, the data movement between machines and devices is often the bottleneck affecting the horizontal scalability of the system. Only if the movement cost can be overlapped by the computation, can distributed deep learning training achieve the ideal linear speedup. Compared with other frameworks, OneFlow regards data movement as important as computation, thus proposing the idea of \"data movement is the first-class citizen\" . Most attention of the conventional frameworks is paid to computation in compile-time. The existing frameworks treat the data movement occuring implicitly behind the scenes. Therefore, the arrangement of overlapping computation and movement is ignored while performing the static analysis of the computation graph. OneFlow explicitly expresses the data movement in the computation graph and treat data movement and data computation equally important in static analysis to maximize the overlapping between data movement and computation. In runtime, data movement operations are also carried out by Actors. In addition to actors used for computation on devices, there are also Actors responsible for data movement between host memory and device memory, network Actors for network communication between machines, Actors responsible for data splitting, merging, and replication, Actors responsible for fetching and reading data from disk, and Actors responsible for loading and saving the model, etc. Many other frameworks make data loading, synchronization of model gradients, networks, model loading updates, etc. into a separate module, but in OneFlow, all such complicated functions are implemented in a static execution graph composed of Actors. The design of OneFlow is simple, elegant and efficient. Figure 3 Data movement from one device to another Figure 3 shows that, in the runtime of OneFlow, how the data are moved from the producer to the consumer on another machine if without GPU-direct. Exploit parallelism as much as possible \u00b6 In the design of OneFlow, parallelism is used as much as possible to achieve optimal distributed performance. For example, when considering the distributed training model of gradient synchronization, the transmission bandwidth between device memory and host memory is higher than the network transmission bandwidth between machines. OneFlow will perform two-level scatter and gather operations (local and between each machine) to increase locality and improve overall performance. Give another example, when OneFlow is running, the control part of user program (usually is Python) is executed in parallel with the execution graph. When necessary, OneFlow use mutually exclusive section ensure the correctness of the concurrent execution. Whether the data loader reads data from disk or is fed data from python, OneFlow ensures that it uses parallelism whenever possible, so that the computing device will not be idle due to waiting for data. If existing frameworks want to overlap data movement and computation as much as possible, they usually use multiple nested callback functions. When there are too many nesting levels, the so-called Callback Hell becomes troublesome, and the correctness and readability of code may decrease. However, in OneFlow, the above concurrency is implemented with the simple and clear Actor mechanism, which avoids the Callback Hell problem. In addition, in the multi-machine network communication, the network communication library in OneFlow not only supports the low level epoll implementation, but also naturally supports high-performance communication protocol such as RDMA. However, in most other deep learning frameworks, they use RPC for data movement in the multi-machine network communication. Compile-time based on a formal description of parallelism \u00b6 OneFlow may be the most user-friendly deep learning framework that supports data parallelism, model parallelism, and pipelining parallelism in distributed scenarios. Users only need to create a network model as if it\u2019s on a single device, and tell OneFlow which resource (machines and devices) is available. OneFlow will automatically generate an almost optimal execulation plan for the job, enabling the runtime system use these machines and devices in an efficient way. This stems from a unique design of OneFlow: Consistent View. For multi-machines and multi-devices, OneFlow will abstract it into a single super large device , which we call a logical device. The device memory of this logical device is the sum of the actual device memories of multiple physical devices, and the computing power of this logical device is also the sum of the actual computing power of multiple physical devices. The user only needs to define how the deep learning model is constructed in this logical super device, and doesn\u2019t need to worry about how OneFlow maps from the model to the physical devices. Here are two concepts: \"logical\" and \"physical\". \"Logical\" means that OneFlow abstracts the distributed computation and data into a single super-device, and \"physical\" means that the computation and data are actually deployed on various machines and devices. The deep learning model is a computation graph composed of Ops, and each Op produces and consumes some data in the form of tensor. In a multi-machine and multi-devices environment, a logical Op is mapped to multiple physical Ops. The computation actually performed by each physical Op is a part of the logical Op computation, and a logical Tensor also is mapped to multiple physical Tensors, and each physical Tensor is a part of the logical Tensor. In distributed training defined by other frameworks, each device is viewed as a \"world\", and the data or parameters are synchronized between multiple devices according to the exposed interface; In OneFlow, the involved multiple machines and multiple devices are together viewed as a \"world\". In the following, we introduce a set of Placement+SBP method for overall management of the world. Placement \u00b6 While creating the computation graph, each computation Op can be assigned an attribute called Placement, indicating on which machines and devices the logical Op will be deployed. In general data parallelism, all Ops are deployed on all devices. However, OneFlow also supports user-specified Op Placement. For example, if the network is too large for a single device to accommodate at all, OneFlow allows the first part of the network to be on one device and the second part on the other device. The devices work together like in a \"relay game\", which enables pipelining parallelism. Figure 4 shows an example of a possible Placement. The user defines a network consisting of 3 Ops: Op_0 -> Op_1 -> Op_2. In this example, the Placement of Op_0 and Op_1 is Device 0, and the Placement of Op_2 is Device 1. This is an example of pipelining parallelism. Oneflow will automatically insert the Copy Op needed for data transfer between Op_1 and Op_2. Figure 4 a placement for pipelining parallelism SBP \u00b6 SBP is a unique concept of OneFlow. It is a combination of the initials of three words: Split, Broadcast, PartialSum (taking PartialSum as an example, in fact, it can also be a reduce operation such as PartialMin, PartialMax). The full name of SBP is SbpParallel, which represents a mapping relationship between the logic Tensor and the physical Tensor. Split means that the physical Tensor is obtained by splitting the logical Tensor along a certain dimension. An axis parameter is used to indicate the dimension of the split. If multiple physical Tensors are concatenated along the dimension of Split, the logical Tensor can be restored. Broadcast indicates that each physical Tensor is exactly a copy of the logical Tensor. PartialSum indicates that although the physical Tensor has the same shape as the logical Tensor, the value in the physical Tensor is a part of the value in the corresponding position in the logical Tensor, if you add multiple physical Tensors at the same positions, you can restore the logical Tensor. Figure 5 shows a simple example of SbpParallel. Figure 5 Examples of SbpParallel SbpSignature is a collection of SbpParallels, each of which is an attribute of a specific Op. It depicts how a logical Op is mapped to multiple physical Ops on each device, and how these physical Ops treat the logical and physical mapping of their Input and Output Tensors. An Op may have multiple legal SbpSignatures. A simple legal signature is that the SbpParallel values of both input and output are Broadcast, which means that each physical Op needs the entire logical Tensor. Once the logical computation graph is constructed by the user, OneFlow generates a distributed physical execution graph by the Compiler. Among the feasible Placements of Ops and the list of legal SbpSignature of each Op, the Compile is able to find an optimal SbpSignature (such as with he minimum transmission cost) for each Op, so that the Compiler can generate the most efficient execution plan. Regarding to the list of legal SbpSignatures of an Op, we will give an example of an Op of matrix multiplication (matmul). Definition: Y = matmul(A,B) , A , B , Y are all Tensor , which means Y = AB . Then there are at least two legal SbpSignatures: 1) Y: Split(0) , A: Split(0) , B: Broadcast 2) Y: Split(1) , A: Broadcast , B: Split(1) The diagram of the two legal signatures on the two devices is shown in Figure 6. Assume that the shapes of the logical input and output Tensor of MatMul is: A(64, 10) \u00d7 B(10, 50) -> Y(64, 50) Figure 6 Two leagal SbpSignatures of MatMul , and the Op is distributed on two devices. Under the first SbpSignature, A on device 0 is the first half of logical A, A on device 1 is the second half of logical A (division along the 0 th dimension), and B on both devices is exactly the same as the logical B. The output Y from the two devices is the first half and the second half of the logical Y respectively. The second SbpSignature can also be analyzed in the same way. It should be noted that when A is data and B is model, the first SbpSignature is actually data parallelism , and the second SbpSignature is model parallelism . If there\u2019re two adjacent MatMul ops, the former uses the first SbpSignature and the latter uses the second SbpSignature, the entire network will form the so-called hybrid parallelism . Figure 7 is an example of hybrid parallelism. It defines Y0 = MatMul_0(A0, B0), Y1 = MatMul_1(Y0, B1), a computation graph composed of two ops, where A0, Y0, Y1 are data Tensor, B0 , B1 is the model Tensor. Figure 7 Hybrid parallelism In Figure 7, Y0 produced by MatMul_0 is consumed by MatMul_1, but the two ops view the SBP of the same Tensor differently. MatMul_0 considers Y0 to be a Split (axis=0) segment, but MatMul_1 needs a Broadcast Y0 input. To achieve the mathematical consistency, OneFlow will automatically insert a \"universal\" Boxing Op to do the necessary data splitting, concatenating, handling and summing operations, so that all Ops can efficiently get the data they want in a distributed environment. In data parallelism, if the Tensor in a training forward model is Broadcast, the corresponding gradient computation in the backward direction is PartialSum. When the Optimizer needs all the gradients to update the model, it will trigger the Boxing mechanism to perform efficient gradient synchronization. The most user-friendly distributed framework \u00b6 OneFlow\u2019s Placement + SBP + Boxing mechanisms allow Op and Tensor in user-defined computation graphs to be distributed on various machines and devices in any way. No matter it is data parallelism, model parallelism or pipelining parallelism, for OneFlow, it is just a combination of a specific SbpSignature under a specific Placement, which can be easily configured by the user, or handed over to OneFlow for automatic processing. In addition, before Microsoft launched the ZeRO-2 framework, OneFlow already supported similar features. In the multiple machines and multiple devices scenarios, each model Tensor is only saved on one of the devices, reducing the memory usage in gradient computations. Summary \u00b6 In summary, during the compile time, OneFlow introduces a mathematically rigorous formal system to describe all legal parallel modes, and enable the compiler to automatically search for the optimal parallel mode conveniently. At the runtime, the Actor system supports parallel and concurrent execution in an flexible and efficient way. The core of OneFlow runtime system has the advantages of simplicity, efficiency and high scalability. Based on such mechanisms, OneFlow makes the distributed training extremely efficient, and makes it as easy as training on a single device.","title":"OneFlow System Design"},{"location":"single_client/basics_topics/essentials_of_oneflow.html#oneflow-system-design","text":"In this article, we will cover these topics: Motivation OneFlow feature 1: Runtime based on actor system OneFlow feature 2: Compile-time based on a formal description of parallelism Summary","title":"OneFlow System Design"},{"location":"single_client/basics_topics/essentials_of_oneflow.html#motivation","text":"OneFlow is born for performance and horizontal scalability, especially for multi-nodes and multi-devices scenarios. We expect that users can leverage the power of multiple machines and multiple devices in a way as easy as using a single machine with single device, and enjoy the efficiency of linear speedup. Why does OneFlow focus on the performance and user experience in distributed scenarios? With the development of deep learning, the model becomes increasingly large, and the computing power required to train deep learning models will become higher and higher. The computing power and the memory of a single device are far from meeting the needs of deep learning model training, and multiple machines and multiple devices are required for parallelism speedup. If the deep learning framework can make multiple interconnected devices work well together and achieve linear speedup, even if the performance of each device is just so so, it can also meet the computing power needs of any scale. This is the so-called horizontal scalability or scaling out. We do believe this is the solution to the increasing need of computing power\u3002 However, the existing frameworks usually focus on the user experience of a single device, and only handle the multi-machine and multi-devices scenarios that works for data parallelism. That is, mirroring the computation graph on a single device to multiple machines and multiple devices, synchronizing model with Allreduce. For models with a huge amount of parameters such as BERT/GPT-3, users often find it not friendly to use, hard to deploy and not efficient to train models on multiple machines and multiple devices when using existing deep learning frameworks. It is also time-consuming for users to learn how to do distributed training. They also need to care about the synchronization of models between multiple machines and multiple devices. In order to solve the above problems in distributed deep learning, both industry and academia not only improve the deep learning framework itself, but also develop a variety of third-party plugins, such as NCCL, Horovod, BytePS, HugeCTR, Mesh-tensorflow, Gpipe, etc. However, it still can\u2019t meet users' unlimited pursuit to performance. The core motivation of OneFlow is to make multi-machine and multi-devices distributed training efficiently, and at the same time, to make the distributed training experience as simple as using a single device. Let's introduce the two core ideas of OneFlow, and explain how OneFlow views deep learning training in distributed scenarios.","title":"Motivation"},{"location":"single_client/basics_topics/essentials_of_oneflow.html#runtime-based-on-actor-system","text":"Key features: Decentralized scheduling Pipelining Data movement as a first-class citizen Overlapping data movement and computation Overlapping control and data logic OneFlow consists of two stages: Compile-time and Runtime. In the Compile-time, user-defined neural networks and the requested resource are compiled into a static graph execution plan, which is composed of the description of the basic execution unit Actor ; During the runtime , each machine actually creates many Actor instances located to its own machine based on the Actor description in the Plan, and then started the Actor operating system. In the training procedure, the basic unit of OneFlow execution is Actor, which corresponds to a node of the static execution graph. The data produced and consumed between Actors are stored in the form of Registers , and the Actors cooperate through message passing.","title":"Runtime based on actor system"},{"location":"single_client/basics_topics/essentials_of_oneflow.html#decentralized-scheduling","text":"OneFlow implements decentralized scheduling through the Actor mechanism. In the entire static graph is composed of actors, there is no central scheduler. Each actor only cares about the producer of the data it needs (upstream Actor) and the consumer of the data it produces (downstream Actor). In this way, in the ultra-large-scale distributed training scenario, completely decentralized scheduling can avoid the single-point performance bottleneck with centralized scheduling. Each Actor has an internal state machine, which updates its status according to the messages sent and received by the Actor. It should be noted that Register is a storage block, which stores the data produced by the Actor, and the message is a lightweight data containing the memory address of the Register storage block. It is message instead of Register that is passed between Actors, in this way, OneFlow runtime achieves zero-copy. When an Actor receives a new message and decides whether the Register it needs to consume is ready, and it has free Register to write the produced data. If yes, the Actor executes (Act) once and produces some new data. After action, the Actor sends a message to the consumer Actors who need to consume the produced Register, indicating that \"you can read the data I produced\"; At the same time, the Actor also needs to return the Register it consumes to its producer, indicating that \"I have used up your data and you can recycle it.\" The state machine inside the Actor is shown in Figure 1. Figure 1 Actor state machine inside After the Actor starts, it will switch its two states according to the messages sent and received with other actors: waiting state and execution state . The messages received by an Actor are generally divided into several types: The upstream producer Actor sends a message saying that you can read the data I produce; the downstream consumer Actor sends a message saying that I have used up the data you produced. When this data are used up by all consumers, it can be recycled as a free block and wait for the Actor to produce a new data in next time. Whenever receiving a message, an Actor will try to decides whether its action conditions are met with. There are generally two action conditions: Whether all the data to be read are available; Whether there are free blocks that can be used for production. When the action state is satisfied, the actor starts to launch its internal Kernel to consume incoming data and produce some new data. After action, the Actor will send messages to upstream and downstream: Send a message to the downstream consumer Actor saying that I just produced a piece of data, you can read them; Send a message to the upstream producer Actor saying that I just used up the data you sent me before. Actors only need to care about upstream and downstream messages to decide whether they can act or not. All Actors form a completely decentralized distributed collaborative network through their own internal state machines and messages exchanging mechanism.","title":"Decentralized scheduling"},{"location":"single_client/basics_topics/essentials_of_oneflow.html#pipelining","text":"In above, we introduced the internal finite state machine of Actors. Message passing and data movement between Actors are implemented by Register . Whether an Actor can act only relates to two conditions: Whether the Registers consumed by itself are readable; Whether the Registers produced by itself have free blocks to write. For a Register, if we allocate multiple free blocks for it, two adjacent Actors can work simultaneously. In this way, the overlapping of adjacent actors implements pipelining. In an ideal case, the initiation interval of the entire static execution graph is the execution time of the bottleneck actor's each action, the execution time of all the other actors will be hidden through the pipelining. Let's take an example to explain how the pipelining of the Actor system works. Figure 2 is an execution sequence diagram of a computation graph composed of 3 Actors (a, b, c). The green Regst square represents the Register block being occupied, and the white Regst square represents the free block of the same Register. At Time0, Actor a produces a Regst_a_0, and Actor b and Actor c are in waiting state because they have no readable Register. Here we assume that the execution time of each Actor is the same. At Time1, Actor a sends a message to Actor b saying that you can read the Regst_a_0 that I produced. Actor b receives the message and checks whether there is a free block available in the Register b owned by itself, and finds that there is an available Regst_b_0 , so Actor b executes at Time1, reading Regst_a_0 and writing Regst_b_0; at the same time, Actor a will also check whether it has a free block to write, and finds that it has a free block to write, so Actor a will also begin executing at Time1, writing Regst_a_1. (It should be noted here that Regst_a_0 and Regst_a_1 logically belong to the same Register, but they are spatially divided into different free blocks. In deep learning training task, Regst_a_0 and Regst_a_1 store data belonging to different batches produced by a same producer.) So Actor a and Actor b work in parallel. Actor c is still waiting because there is no data to read. At Time2, Actor b has produced Regst_b_0, so it sends a message to the downstream consumer Actor c that you can read the Regst_b_0 I produced, and at the same time sends a message to the upstream producer Actor a that I have consumed your Regst_a_0 . At the same time, Actor a sends a newly produced Regst_a_1 to Actor b . Actor b checks that it still has Regst_b_1 being free, so Actor b starts to read Regst_a_1 and writes Regst_b_1; Actor c receives Regst_b_0 and finds that it has Regst_c_0 being free, so Actor c starts execution, reading Regst_b_0 and writing Regst_c_0; Actor a receives Regst_a_0 that Actor b has used up and returned the ownership, and checks that all consumers of Regst_a_0 are used up, so Regst_a_0 is recycled and marked as a free block, and Actor a can continue to execute and write Regst_a_2. Figure 2 Actor producer-consumer relationship and execution sequence diagram In the above example, at Time2, Actors a , b , and c are all working simultaneously. In typical deep learning training job, Regst_b_0 and Regst_c_0 at Time2 store the data of Batch 0, and Regst_a_1 and Regst_b_1 store the data of Batch 1. Regst_a_2 stores data of Batch 2. By the design of a Register with multiple free blocks, the Actor naturally supports pipelining. Here we raise a further in-depth problem: in OneFlow, the execution of the entire data flow is like a network, and the data flow throught the network and completes the computation. How to slow down the producer's production if it is too fast for the consumer to consume, and how to avoid the case if the producer's production is too slow, and consumers get hungry. This problem involves planning for computing, memory, and transmission bandwidth, so that the bottleneck of the system is as wide as possible. It relates to flow control and resource allocation (For example, how many memory block quotas are allocated to the Register of each Actor). This is a critical problem which has been solved by the OneFlow system.","title":"Pipelining"},{"location":"single_client/basics_topics/essentials_of_oneflow.html#data-movement-as-a-first-class-citizen","text":"In a distributed environment with multiple machines and multiple devices, the data movement between machines and devices is often the bottleneck affecting the horizontal scalability of the system. Only if the movement cost can be overlapped by the computation, can distributed deep learning training achieve the ideal linear speedup. Compared with other frameworks, OneFlow regards data movement as important as computation, thus proposing the idea of \"data movement is the first-class citizen\" . Most attention of the conventional frameworks is paid to computation in compile-time. The existing frameworks treat the data movement occuring implicitly behind the scenes. Therefore, the arrangement of overlapping computation and movement is ignored while performing the static analysis of the computation graph. OneFlow explicitly expresses the data movement in the computation graph and treat data movement and data computation equally important in static analysis to maximize the overlapping between data movement and computation. In runtime, data movement operations are also carried out by Actors. In addition to actors used for computation on devices, there are also Actors responsible for data movement between host memory and device memory, network Actors for network communication between machines, Actors responsible for data splitting, merging, and replication, Actors responsible for fetching and reading data from disk, and Actors responsible for loading and saving the model, etc. Many other frameworks make data loading, synchronization of model gradients, networks, model loading updates, etc. into a separate module, but in OneFlow, all such complicated functions are implemented in a static execution graph composed of Actors. The design of OneFlow is simple, elegant and efficient. Figure 3 Data movement from one device to another Figure 3 shows that, in the runtime of OneFlow, how the data are moved from the producer to the consumer on another machine if without GPU-direct.","title":"Data movement as a first-class citizen"},{"location":"single_client/basics_topics/essentials_of_oneflow.html#exploit-parallelism-as-much-as-possible","text":"In the design of OneFlow, parallelism is used as much as possible to achieve optimal distributed performance. For example, when considering the distributed training model of gradient synchronization, the transmission bandwidth between device memory and host memory is higher than the network transmission bandwidth between machines. OneFlow will perform two-level scatter and gather operations (local and between each machine) to increase locality and improve overall performance. Give another example, when OneFlow is running, the control part of user program (usually is Python) is executed in parallel with the execution graph. When necessary, OneFlow use mutually exclusive section ensure the correctness of the concurrent execution. Whether the data loader reads data from disk or is fed data from python, OneFlow ensures that it uses parallelism whenever possible, so that the computing device will not be idle due to waiting for data. If existing frameworks want to overlap data movement and computation as much as possible, they usually use multiple nested callback functions. When there are too many nesting levels, the so-called Callback Hell becomes troublesome, and the correctness and readability of code may decrease. However, in OneFlow, the above concurrency is implemented with the simple and clear Actor mechanism, which avoids the Callback Hell problem. In addition, in the multi-machine network communication, the network communication library in OneFlow not only supports the low level epoll implementation, but also naturally supports high-performance communication protocol such as RDMA. However, in most other deep learning frameworks, they use RPC for data movement in the multi-machine network communication.","title":"Exploit parallelism as much as possible"},{"location":"single_client/basics_topics/essentials_of_oneflow.html#compile-time-based-on-a-formal-description-of-parallelism","text":"OneFlow may be the most user-friendly deep learning framework that supports data parallelism, model parallelism, and pipelining parallelism in distributed scenarios. Users only need to create a network model as if it\u2019s on a single device, and tell OneFlow which resource (machines and devices) is available. OneFlow will automatically generate an almost optimal execulation plan for the job, enabling the runtime system use these machines and devices in an efficient way. This stems from a unique design of OneFlow: Consistent View. For multi-machines and multi-devices, OneFlow will abstract it into a single super large device , which we call a logical device. The device memory of this logical device is the sum of the actual device memories of multiple physical devices, and the computing power of this logical device is also the sum of the actual computing power of multiple physical devices. The user only needs to define how the deep learning model is constructed in this logical super device, and doesn\u2019t need to worry about how OneFlow maps from the model to the physical devices. Here are two concepts: \"logical\" and \"physical\". \"Logical\" means that OneFlow abstracts the distributed computation and data into a single super-device, and \"physical\" means that the computation and data are actually deployed on various machines and devices. The deep learning model is a computation graph composed of Ops, and each Op produces and consumes some data in the form of tensor. In a multi-machine and multi-devices environment, a logical Op is mapped to multiple physical Ops. The computation actually performed by each physical Op is a part of the logical Op computation, and a logical Tensor also is mapped to multiple physical Tensors, and each physical Tensor is a part of the logical Tensor. In distributed training defined by other frameworks, each device is viewed as a \"world\", and the data or parameters are synchronized between multiple devices according to the exposed interface; In OneFlow, the involved multiple machines and multiple devices are together viewed as a \"world\". In the following, we introduce a set of Placement+SBP method for overall management of the world.","title":"Compile-time based on a formal description of parallelism"},{"location":"single_client/basics_topics/essentials_of_oneflow.html#placement","text":"While creating the computation graph, each computation Op can be assigned an attribute called Placement, indicating on which machines and devices the logical Op will be deployed. In general data parallelism, all Ops are deployed on all devices. However, OneFlow also supports user-specified Op Placement. For example, if the network is too large for a single device to accommodate at all, OneFlow allows the first part of the network to be on one device and the second part on the other device. The devices work together like in a \"relay game\", which enables pipelining parallelism. Figure 4 shows an example of a possible Placement. The user defines a network consisting of 3 Ops: Op_0 -> Op_1 -> Op_2. In this example, the Placement of Op_0 and Op_1 is Device 0, and the Placement of Op_2 is Device 1. This is an example of pipelining parallelism. Oneflow will automatically insert the Copy Op needed for data transfer between Op_1 and Op_2. Figure 4 a placement for pipelining parallelism","title":"Placement"},{"location":"single_client/basics_topics/essentials_of_oneflow.html#sbp","text":"SBP is a unique concept of OneFlow. It is a combination of the initials of three words: Split, Broadcast, PartialSum (taking PartialSum as an example, in fact, it can also be a reduce operation such as PartialMin, PartialMax). The full name of SBP is SbpParallel, which represents a mapping relationship between the logic Tensor and the physical Tensor. Split means that the physical Tensor is obtained by splitting the logical Tensor along a certain dimension. An axis parameter is used to indicate the dimension of the split. If multiple physical Tensors are concatenated along the dimension of Split, the logical Tensor can be restored. Broadcast indicates that each physical Tensor is exactly a copy of the logical Tensor. PartialSum indicates that although the physical Tensor has the same shape as the logical Tensor, the value in the physical Tensor is a part of the value in the corresponding position in the logical Tensor, if you add multiple physical Tensors at the same positions, you can restore the logical Tensor. Figure 5 shows a simple example of SbpParallel. Figure 5 Examples of SbpParallel SbpSignature is a collection of SbpParallels, each of which is an attribute of a specific Op. It depicts how a logical Op is mapped to multiple physical Ops on each device, and how these physical Ops treat the logical and physical mapping of their Input and Output Tensors. An Op may have multiple legal SbpSignatures. A simple legal signature is that the SbpParallel values of both input and output are Broadcast, which means that each physical Op needs the entire logical Tensor. Once the logical computation graph is constructed by the user, OneFlow generates a distributed physical execution graph by the Compiler. Among the feasible Placements of Ops and the list of legal SbpSignature of each Op, the Compile is able to find an optimal SbpSignature (such as with he minimum transmission cost) for each Op, so that the Compiler can generate the most efficient execution plan. Regarding to the list of legal SbpSignatures of an Op, we will give an example of an Op of matrix multiplication (matmul). Definition: Y = matmul(A,B) , A , B , Y are all Tensor , which means Y = AB . Then there are at least two legal SbpSignatures: 1) Y: Split(0) , A: Split(0) , B: Broadcast 2) Y: Split(1) , A: Broadcast , B: Split(1) The diagram of the two legal signatures on the two devices is shown in Figure 6. Assume that the shapes of the logical input and output Tensor of MatMul is: A(64, 10) \u00d7 B(10, 50) -> Y(64, 50) Figure 6 Two leagal SbpSignatures of MatMul , and the Op is distributed on two devices. Under the first SbpSignature, A on device 0 is the first half of logical A, A on device 1 is the second half of logical A (division along the 0 th dimension), and B on both devices is exactly the same as the logical B. The output Y from the two devices is the first half and the second half of the logical Y respectively. The second SbpSignature can also be analyzed in the same way. It should be noted that when A is data and B is model, the first SbpSignature is actually data parallelism , and the second SbpSignature is model parallelism . If there\u2019re two adjacent MatMul ops, the former uses the first SbpSignature and the latter uses the second SbpSignature, the entire network will form the so-called hybrid parallelism . Figure 7 is an example of hybrid parallelism. It defines Y0 = MatMul_0(A0, B0), Y1 = MatMul_1(Y0, B1), a computation graph composed of two ops, where A0, Y0, Y1 are data Tensor, B0 , B1 is the model Tensor. Figure 7 Hybrid parallelism In Figure 7, Y0 produced by MatMul_0 is consumed by MatMul_1, but the two ops view the SBP of the same Tensor differently. MatMul_0 considers Y0 to be a Split (axis=0) segment, but MatMul_1 needs a Broadcast Y0 input. To achieve the mathematical consistency, OneFlow will automatically insert a \"universal\" Boxing Op to do the necessary data splitting, concatenating, handling and summing operations, so that all Ops can efficiently get the data they want in a distributed environment. In data parallelism, if the Tensor in a training forward model is Broadcast, the corresponding gradient computation in the backward direction is PartialSum. When the Optimizer needs all the gradients to update the model, it will trigger the Boxing mechanism to perform efficient gradient synchronization.","title":"SBP"},{"location":"single_client/basics_topics/essentials_of_oneflow.html#the-most-user-friendly-distributed-framework","text":"OneFlow\u2019s Placement + SBP + Boxing mechanisms allow Op and Tensor in user-defined computation graphs to be distributed on various machines and devices in any way. No matter it is data parallelism, model parallelism or pipelining parallelism, for OneFlow, it is just a combination of a specific SbpSignature under a specific Placement, which can be easily configured by the user, or handed over to OneFlow for automatic processing. In addition, before Microsoft launched the ZeRO-2 framework, OneFlow already supported similar features. In the multiple machines and multiple devices scenarios, each model Tensor is only saved on one of the devices, reducing the memory usage in gradient computations.","title":"The most user-friendly distributed framework"},{"location":"single_client/basics_topics/essentials_of_oneflow.html#summary","text":"In summary, during the compile time, OneFlow introduces a mathematically rigorous formal system to describe all legal parallel modes, and enable the compiler to automatically search for the optimal parallel mode conveniently. At the runtime, the Actor system supports parallel and concurrent execution in an flexible and efficient way. The core of OneFlow runtime system has the advantages of simplicity, efficiency and high scalability. Based on such mechanisms, OneFlow makes the distributed training extremely efficient, and makes it as easy as training on a single device.","title":"Summary"},{"location":"single_client/basics_topics/model_load_save.html","text":"Loading and Saving of Model \u00b6 For loading and saving for model, the common scences is: Save the model that has been trained for a while to facilitate the next training. Save trained model for reproduction(Such as Model Serving). Strictly speaking, we save the untrained model as checkpoint or snapshot . It is different from model saving of a completed model. However in Oneflow, despite the model has been trained or not, we can use the same interface to save model. Thus, like the model \u3001 checkpoint \u3001 snapshot we see in other framework is no difference in OneFlow. In OneFlow, there are interfaces for model saving and loading under the flow.checkpoint . In this article, we will introduce: How to create model parameters How to save and load model Storage structure of OneFlow model How to finetune and extend model Use get_variable to Create/Obtain Model Parameters Object \u00b6 We can use oneflow.get_variable to create or obtain an object and this object can be used to interact with information in global job functions. When we call the interfaces of oneflow.get_all_variables and oneflow.load_variables , we can get or update the value of the object created by get_variable . Because of this feature, the object created by get_variable is used to store model parameters. In fact, there are many high level interface in OneFlow (like oneflow.layers.conv2d ) use get_variable internally to create model parameters internally. Process \u00b6 The get_variable requires a specified name as the identity of the created object. If the name value already existed in the program, then get_variable will get the existed object and return. If the name value doesn't exist in the program, get_variable will create a blob object internally and return. Use get_variable Create Object \u00b6 The signature of oneflow.get_variable is: def get_variable ( name , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , model_name = None , random_seed = None , distribute = distribute_util . broadcast (), ) The following example use get_variable to create parameters and build the network with oneflow.layers.conv2d : #... weight = flow . get_variable ( weight_name if weight_name else name_prefix + \"-weight\" , shape = weight_shape , dtype = inputs . dtype , initializer = kernel_initializer if kernel_initializer is not None else flow . constant_initializer ( 0 ), regularizer = kernel_regularizer , trainable = trainable , model_name = \"weight\" , ) output = flow . nn . conv2d ( inputs , weight , strides , padding , data_format , dilation_rate , groups = groups , name = name ) #... Initializer Setting \u00b6 In the previous sections, when we call get_variable , we specify the method of initializing the parameters by initializer . In OneFlow, we provide many initializers which can be found in oneflow . Under the static graph mechanism, we set the initializer first, and parameter initialization will be done by the OneFlow framework automatically. The initializers currently supported by OneFlow are listed below. Click it to see the details of algorithm: constant_initializer zeros_initializer ones_initializer random_uniform_initializer random_normal_initializer truncated_normal_initializer glorot_uniform_initializer glorot_normal_initializer variance_scaling_initializer kaiming_initializer xavier_normal_initializer xavier_uniform_initializer The Python Interface of OneFlow Models \u00b6 We can use the following interfaces to get or update the value of the variable object created by oneflow.get_variable in job function. oneflow.get_all_variables : Get the variable of all job functions. oneflow.load_variables : Update the variable in job function. oneflow.get_all_variables returns a dictionary whose key is the name specified when creating the variable and the value corresponding to the key is a tensor which has numpy() method to convert itself to a numpy array. For example, creating an object named myblob in job function: @flow . global_function () def job () -> tp . Numpy : ... myblob = flow . get_variable ( \"myblob\" , shape = ( 3 , 3 ), initializer = flow . random_normal_initializer () ) ... If we want to print the value of myblob , we can call: ... for epoch in range ( 20 ): ... job () all_variables = flow . get_all_variables () print ( all_variables [ \"myblob\" ] . numpy ()) ... The flow.get_all_variables gets the dictionary and all_variables[\"myblob\"].numpy() gets the myblob object then converts it to a numpy array. By contrary, we can use oneflow.load_variables to update the values of variable. The signature of oneflow.load_variables is as follows: def load_variables ( value_dict , ignore_mismatch = True ) Before call load_variables , we have to prepare a dictionary whose key is the name specified when creating variable and value is a numpy array. After passing the dictionary to load_variables , load_variables will find the variable object in the job function based on the key and update the value. For example: @flow . global_function ( type = \"predict\" ) def job () -> tp . Numpy : myblob = flow . get_variable ( \"myblob\" , shape = ( 3 , 3 ), initializer = flow . random_normal_initializer () ) return myblob myvardict = { \"myblob\" : np . ones (( 3 , 3 )) . astype ( np . float32 )} flow . load_variables ( myvardict ) print ( flow . get_all_variables ()[ \"myblob\" ] . numpy ()) Although we have chosen the random_normal_initializer initializer, flow.load_variables(myvardict) updates the value of myblob . The final output will be: [[1. 1. 1.] [1. 1. 1.] [1. 1. 1.]] Model Saving and Loading \u00b6 We can save or load the model by methods: oneflow.checkpoint.save : Save the model to the specified path. oneflow.checkpoint.get : Load a model from the specified path. The signature of save is as follows which saves the model to the path specified by path . def save ( path , var_dict = None ) If the optional parameter var_dict is not None , save will save the object specified in var_dict to the specified path . The signature of get is as follows which loads the previously saved model specified by the path . def get ( path ) It will return a dictionary that can be updated into the model using the load_variables . flow . load_variables ( flow . checkpoint . get ( save_dir )) Attention\uff1a The path specified by the save should either be empty or not existed. Otherwise save will report an error (to prevent overwriting the existed saved model) OneFlow models are stored in a specified path in a certain structure. See the storage structure of OneFlow models below for more details. Although there is no limit to the frequency of save in OneFlow. But excessive saving frequency will increase the load on resources such as disk and bandwidth. The Structure of OneFlow Saved Model \u00b6 OneFlow model are the parameters of network. For now there are no meta graph information in OneFlow model. The path to save model have many sub-directories. Each of them is corresponding to the name of job function in model. For example, we define the model in the first place: def lenet ( data , train = False ): initializer = flow . truncated_normal ( 0.1 ) conv1 = flow . layers . conv2d ( data , 32 , 5 , padding = \"SAME\" , activation = flow . nn . relu , name = \"conv1\" , kernel_initializer = initializer , ) pool1 = flow . nn . max_pool2d ( conv1 , ksize = 2 , strides = 2 , padding = \"SAME\" , name = \"pool1\" , data_format = \"NCHW\" ) conv2 = flow . layers . conv2d ( pool1 , 64 , 5 , padding = \"SAME\" , activation = flow . nn . relu , name = \"conv2\" , kernel_initializer = initializer , ) pool2 = flow . nn . max_pool2d ( conv2 , ksize = 2 , strides = 2 , padding = \"SAME\" , name = \"pool2\" , data_format = \"NCHW\" ) reshape = flow . reshape ( pool2 , [ pool2 . shape [ 0 ], - 1 ]) hidden = flow . layers . dense ( reshape , 512 , activation = flow . nn . relu , kernel_initializer = initializer , name = \"dense1\" , ) if train : hidden = flow . nn . dropout ( hidden , rate = 0.5 , name = \"dropout\" ) return flow . layers . dense ( hidden , 10 , kernel_initializer = initializer , name = \"dense2\" ) Assume that in the process of training, we call the following code to save model: flow . checkpoint . save ( './lenet_models_name' ) Then lenet_models_name and the subdirectories are as follows: lenet_models_name/ \u251c\u2500\u2500 conv1-bias \u2502 \u251c\u2500\u2500 meta \u2502 \u2514\u2500\u2500 out \u251c\u2500\u2500 conv1-weight \u2502 \u251c\u2500\u2500 meta \u2502 \u2514\u2500\u2500 out \u251c\u2500\u2500 conv2-bias \u2502 \u251c\u2500\u2500 meta \u2502 \u2514\u2500\u2500 out \u251c\u2500\u2500 conv2-weight \u2502 \u251c\u2500\u2500 meta \u2502 \u2514\u2500\u2500 out \u251c\u2500\u2500 dense1-bias \u2502 \u251c\u2500\u2500 meta \u2502 \u2514\u2500\u2500 out \u251c\u2500\u2500 dense1-weight \u2502 \u251c\u2500\u2500 meta \u2502 \u2514\u2500\u2500 out \u251c\u2500\u2500 dense2-bias \u2502 \u251c\u2500\u2500 meta \u2502 \u2514\u2500\u2500 out \u251c\u2500\u2500 dense2-weight \u2502 \u251c\u2500\u2500 meta \u2502 \u2514\u2500\u2500 out \u251c\u2500\u2500 snapshot_done \u2514\u2500\u2500 System-Train-TrainStep-train_job \u251c\u2500\u2500 meta \u2514\u2500\u2500 out We can see: In the network in job function, each variable is corresponding to a sub-directory. In each of the subdirectories, there are out and meta files where out stores the values of the network parameters in binary form and meta stores the network structure information in text form. Snapshot_done is an empty folder. If it exists, it means that the network training has been finished. Snapshots of the training steps is stored in System-Train-TrainStep-train_job . Model Finetune and Transfer Learning \u00b6 In model finetune and transfer learning, we always need\uff1a Load some of the parameters from original model Initialize the other part of parameters in model We can use oneflow.load_variables to complete the process above. Here is a simple example to illustrate the concept. First we need define a model and save it to ./mlp_models_1 after training: @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : with flow . scope . placement ( \"cpu\" , \"0:0\" ): initializer = flow . truncated_normal ( 0.1 ) reshape = flow . reshape ( images , [ images . shape [ 0 ], - 1 ]) hidden = flow . layers . dense ( reshape , 512 , activation = flow . nn . relu , kernel_initializer = initializer , name = \"dense1\" , ) dense2 = flow . layers . dense ( hidden , 10 , kernel_initializer = initializer , name = \"dense2\" ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , dense2 ) lr_scheduler = flow . optimizer . PiecewiseConstantScheduler ([], [ 0.1 ]) flow . optimizer . SGD ( lr_scheduler , momentum = 0 ) . minimize ( loss ) return loss Then we expand the network and add one more layer ( dense3 ) in above model: @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : with flow . scope . placement ( \"cpu\" , \"0:0\" ): #... original structure dense3 = flow . layers . dense ( dense2 , 10 , kernel_initializer = initializer , name = \"dense3\" ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , dense3 ) #... Finally, load parameters from original model and start training: if __name__ == \"__main__\" : check_point = flow . train . CheckPoint () check_point . load ( \"./mlp_models_1\" ) ( train_images , train_labels ), ( test_images , test_labels ) = flow . data . load_mnist ( BATCH_SIZE , BATCH_SIZE ) for i , ( images , labels ) in enumerate ( zip ( train_images , train_labels )): loss = train_job ( images , labels ) if i % 20 == 0 : print ( loss . mean ()) check_point . save ( \"./mlp_ext_models_1\" ) The parameters of new dense3 layer do not exist in the original model. They are automatically initialized to their values by OneFlow. Codes \u00b6 The following code is from mlp_mnist_origin.py . As the backbone network. Trained model is stored in ./mlp_models_1 . Run: wget https://docs.oneflow.org/master/code/basics_topics/mlp_mnist_origin.py python3 mlp_mnist_origin.py When the training is complete, you will get the mlp_models_1 in the current working directory. The following code is from mlp_mnist_finetune.py . After finetuning (add one more layer dense3 in backbone network), we load ./mlp_models_1 and train it. Run: wget https://docs.oneflow.org/master/code/basics_topics/mlp_mnist_finetune.py python3 mlp_mnist_finetune.py The finetuned models are saved in . /mlp_ext_models_1 .","title":"Loading and saving of model"},{"location":"single_client/basics_topics/model_load_save.html#loading-and-saving-of-model","text":"For loading and saving for model, the common scences is: Save the model that has been trained for a while to facilitate the next training. Save trained model for reproduction(Such as Model Serving). Strictly speaking, we save the untrained model as checkpoint or snapshot . It is different from model saving of a completed model. However in Oneflow, despite the model has been trained or not, we can use the same interface to save model. Thus, like the model \u3001 checkpoint \u3001 snapshot we see in other framework is no difference in OneFlow. In OneFlow, there are interfaces for model saving and loading under the flow.checkpoint . In this article, we will introduce: How to create model parameters How to save and load model Storage structure of OneFlow model How to finetune and extend model","title":"Loading and Saving of Model"},{"location":"single_client/basics_topics/model_load_save.html#use-get_variable-to-createobtain-model-parameters-object","text":"We can use oneflow.get_variable to create or obtain an object and this object can be used to interact with information in global job functions. When we call the interfaces of oneflow.get_all_variables and oneflow.load_variables , we can get or update the value of the object created by get_variable . Because of this feature, the object created by get_variable is used to store model parameters. In fact, there are many high level interface in OneFlow (like oneflow.layers.conv2d ) use get_variable internally to create model parameters internally.","title":"Use get_variable to Create/Obtain Model Parameters Object"},{"location":"single_client/basics_topics/model_load_save.html#process","text":"The get_variable requires a specified name as the identity of the created object. If the name value already existed in the program, then get_variable will get the existed object and return. If the name value doesn't exist in the program, get_variable will create a blob object internally and return.","title":"Process"},{"location":"single_client/basics_topics/model_load_save.html#use-get_variable-create-object","text":"The signature of oneflow.get_variable is: def get_variable ( name , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , model_name = None , random_seed = None , distribute = distribute_util . broadcast (), ) The following example use get_variable to create parameters and build the network with oneflow.layers.conv2d : #... weight = flow . get_variable ( weight_name if weight_name else name_prefix + \"-weight\" , shape = weight_shape , dtype = inputs . dtype , initializer = kernel_initializer if kernel_initializer is not None else flow . constant_initializer ( 0 ), regularizer = kernel_regularizer , trainable = trainable , model_name = \"weight\" , ) output = flow . nn . conv2d ( inputs , weight , strides , padding , data_format , dilation_rate , groups = groups , name = name ) #...","title":"Use get_variable Create Object"},{"location":"single_client/basics_topics/model_load_save.html#initializer-setting","text":"In the previous sections, when we call get_variable , we specify the method of initializing the parameters by initializer . In OneFlow, we provide many initializers which can be found in oneflow . Under the static graph mechanism, we set the initializer first, and parameter initialization will be done by the OneFlow framework automatically. The initializers currently supported by OneFlow are listed below. Click it to see the details of algorithm: constant_initializer zeros_initializer ones_initializer random_uniform_initializer random_normal_initializer truncated_normal_initializer glorot_uniform_initializer glorot_normal_initializer variance_scaling_initializer kaiming_initializer xavier_normal_initializer xavier_uniform_initializer","title":"Initializer Setting"},{"location":"single_client/basics_topics/model_load_save.html#the-python-interface-of-oneflow-models","text":"We can use the following interfaces to get or update the value of the variable object created by oneflow.get_variable in job function. oneflow.get_all_variables : Get the variable of all job functions. oneflow.load_variables : Update the variable in job function. oneflow.get_all_variables returns a dictionary whose key is the name specified when creating the variable and the value corresponding to the key is a tensor which has numpy() method to convert itself to a numpy array. For example, creating an object named myblob in job function: @flow . global_function () def job () -> tp . Numpy : ... myblob = flow . get_variable ( \"myblob\" , shape = ( 3 , 3 ), initializer = flow . random_normal_initializer () ) ... If we want to print the value of myblob , we can call: ... for epoch in range ( 20 ): ... job () all_variables = flow . get_all_variables () print ( all_variables [ \"myblob\" ] . numpy ()) ... The flow.get_all_variables gets the dictionary and all_variables[\"myblob\"].numpy() gets the myblob object then converts it to a numpy array. By contrary, we can use oneflow.load_variables to update the values of variable. The signature of oneflow.load_variables is as follows: def load_variables ( value_dict , ignore_mismatch = True ) Before call load_variables , we have to prepare a dictionary whose key is the name specified when creating variable and value is a numpy array. After passing the dictionary to load_variables , load_variables will find the variable object in the job function based on the key and update the value. For example: @flow . global_function ( type = \"predict\" ) def job () -> tp . Numpy : myblob = flow . get_variable ( \"myblob\" , shape = ( 3 , 3 ), initializer = flow . random_normal_initializer () ) return myblob myvardict = { \"myblob\" : np . ones (( 3 , 3 )) . astype ( np . float32 )} flow . load_variables ( myvardict ) print ( flow . get_all_variables ()[ \"myblob\" ] . numpy ()) Although we have chosen the random_normal_initializer initializer, flow.load_variables(myvardict) updates the value of myblob . The final output will be: [[1. 1. 1.] [1. 1. 1.] [1. 1. 1.]]","title":"The Python Interface of OneFlow Models"},{"location":"single_client/basics_topics/model_load_save.html#model-saving-and-loading","text":"We can save or load the model by methods: oneflow.checkpoint.save : Save the model to the specified path. oneflow.checkpoint.get : Load a model from the specified path. The signature of save is as follows which saves the model to the path specified by path . def save ( path , var_dict = None ) If the optional parameter var_dict is not None , save will save the object specified in var_dict to the specified path . The signature of get is as follows which loads the previously saved model specified by the path . def get ( path ) It will return a dictionary that can be updated into the model using the load_variables . flow . load_variables ( flow . checkpoint . get ( save_dir )) Attention\uff1a The path specified by the save should either be empty or not existed. Otherwise save will report an error (to prevent overwriting the existed saved model) OneFlow models are stored in a specified path in a certain structure. See the storage structure of OneFlow models below for more details. Although there is no limit to the frequency of save in OneFlow. But excessive saving frequency will increase the load on resources such as disk and bandwidth.","title":"Model Saving and Loading"},{"location":"single_client/basics_topics/model_load_save.html#the-structure-of-oneflow-saved-model","text":"OneFlow model are the parameters of network. For now there are no meta graph information in OneFlow model. The path to save model have many sub-directories. Each of them is corresponding to the name of job function in model. For example, we define the model in the first place: def lenet ( data , train = False ): initializer = flow . truncated_normal ( 0.1 ) conv1 = flow . layers . conv2d ( data , 32 , 5 , padding = \"SAME\" , activation = flow . nn . relu , name = \"conv1\" , kernel_initializer = initializer , ) pool1 = flow . nn . max_pool2d ( conv1 , ksize = 2 , strides = 2 , padding = \"SAME\" , name = \"pool1\" , data_format = \"NCHW\" ) conv2 = flow . layers . conv2d ( pool1 , 64 , 5 , padding = \"SAME\" , activation = flow . nn . relu , name = \"conv2\" , kernel_initializer = initializer , ) pool2 = flow . nn . max_pool2d ( conv2 , ksize = 2 , strides = 2 , padding = \"SAME\" , name = \"pool2\" , data_format = \"NCHW\" ) reshape = flow . reshape ( pool2 , [ pool2 . shape [ 0 ], - 1 ]) hidden = flow . layers . dense ( reshape , 512 , activation = flow . nn . relu , kernel_initializer = initializer , name = \"dense1\" , ) if train : hidden = flow . nn . dropout ( hidden , rate = 0.5 , name = \"dropout\" ) return flow . layers . dense ( hidden , 10 , kernel_initializer = initializer , name = \"dense2\" ) Assume that in the process of training, we call the following code to save model: flow . checkpoint . save ( './lenet_models_name' ) Then lenet_models_name and the subdirectories are as follows: lenet_models_name/ \u251c\u2500\u2500 conv1-bias \u2502 \u251c\u2500\u2500 meta \u2502 \u2514\u2500\u2500 out \u251c\u2500\u2500 conv1-weight \u2502 \u251c\u2500\u2500 meta \u2502 \u2514\u2500\u2500 out \u251c\u2500\u2500 conv2-bias \u2502 \u251c\u2500\u2500 meta \u2502 \u2514\u2500\u2500 out \u251c\u2500\u2500 conv2-weight \u2502 \u251c\u2500\u2500 meta \u2502 \u2514\u2500\u2500 out \u251c\u2500\u2500 dense1-bias \u2502 \u251c\u2500\u2500 meta \u2502 \u2514\u2500\u2500 out \u251c\u2500\u2500 dense1-weight \u2502 \u251c\u2500\u2500 meta \u2502 \u2514\u2500\u2500 out \u251c\u2500\u2500 dense2-bias \u2502 \u251c\u2500\u2500 meta \u2502 \u2514\u2500\u2500 out \u251c\u2500\u2500 dense2-weight \u2502 \u251c\u2500\u2500 meta \u2502 \u2514\u2500\u2500 out \u251c\u2500\u2500 snapshot_done \u2514\u2500\u2500 System-Train-TrainStep-train_job \u251c\u2500\u2500 meta \u2514\u2500\u2500 out We can see: In the network in job function, each variable is corresponding to a sub-directory. In each of the subdirectories, there are out and meta files where out stores the values of the network parameters in binary form and meta stores the network structure information in text form. Snapshot_done is an empty folder. If it exists, it means that the network training has been finished. Snapshots of the training steps is stored in System-Train-TrainStep-train_job .","title":"The Structure of OneFlow Saved Model"},{"location":"single_client/basics_topics/model_load_save.html#model-finetune-and-transfer-learning","text":"In model finetune and transfer learning, we always need\uff1a Load some of the parameters from original model Initialize the other part of parameters in model We can use oneflow.load_variables to complete the process above. Here is a simple example to illustrate the concept. First we need define a model and save it to ./mlp_models_1 after training: @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : with flow . scope . placement ( \"cpu\" , \"0:0\" ): initializer = flow . truncated_normal ( 0.1 ) reshape = flow . reshape ( images , [ images . shape [ 0 ], - 1 ]) hidden = flow . layers . dense ( reshape , 512 , activation = flow . nn . relu , kernel_initializer = initializer , name = \"dense1\" , ) dense2 = flow . layers . dense ( hidden , 10 , kernel_initializer = initializer , name = \"dense2\" ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , dense2 ) lr_scheduler = flow . optimizer . PiecewiseConstantScheduler ([], [ 0.1 ]) flow . optimizer . SGD ( lr_scheduler , momentum = 0 ) . minimize ( loss ) return loss Then we expand the network and add one more layer ( dense3 ) in above model: @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : with flow . scope . placement ( \"cpu\" , \"0:0\" ): #... original structure dense3 = flow . layers . dense ( dense2 , 10 , kernel_initializer = initializer , name = \"dense3\" ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , dense3 ) #... Finally, load parameters from original model and start training: if __name__ == \"__main__\" : check_point = flow . train . CheckPoint () check_point . load ( \"./mlp_models_1\" ) ( train_images , train_labels ), ( test_images , test_labels ) = flow . data . load_mnist ( BATCH_SIZE , BATCH_SIZE ) for i , ( images , labels ) in enumerate ( zip ( train_images , train_labels )): loss = train_job ( images , labels ) if i % 20 == 0 : print ( loss . mean ()) check_point . save ( \"./mlp_ext_models_1\" ) The parameters of new dense3 layer do not exist in the original model. They are automatically initialized to their values by OneFlow.","title":"Model Finetune and Transfer Learning"},{"location":"single_client/basics_topics/model_load_save.html#codes","text":"The following code is from mlp_mnist_origin.py . As the backbone network. Trained model is stored in ./mlp_models_1 . Run: wget https://docs.oneflow.org/master/code/basics_topics/mlp_mnist_origin.py python3 mlp_mnist_origin.py When the training is complete, you will get the mlp_models_1 in the current working directory. The following code is from mlp_mnist_finetune.py . After finetuning (add one more layer dense3 in backbone network), we load ./mlp_models_1 and train it. Run: wget https://docs.oneflow.org/master/code/basics_topics/mlp_mnist_finetune.py python3 mlp_mnist_finetune.py The finetuned models are saved in . /mlp_ext_models_1 .","title":"Codes"},{"location":"single_client/basics_topics/optimizer_in_function_config.html","text":"Configuration of Optimization Algorithms and Hyperparameters \u00b6 After a neural network model has been set up, it usually requires training before using for prediction or inference. The training process means to optimize parameters of the nerwork which are usually updated with the back propagation algorithm and a specified optimizer. In this article, we will introduce how to setup optimizers and hyperparameters in OneFlow to users. Key point summary of this article: Configuration examples of job functions for training and prediction. The use of optimizer and learning strategies. Common errors due to misconfiguration and corresponding solutions. Users can directly use the training and inferencing configurations described in Example of configutraion section without knowing the design concept of OneFlow. For more detials please refer to optimizer api Job Function Configuration \u00b6 In [Recognizing MNIST Handwritten Digits] (... /quick_start/lenet_mnist.md#global_function), we have learned about the concept of the oneflow.global_function decorator and the job function. The configuration of this article base on that. The job function can be configured by passing the function_config parameter to the decorator. If you are not familiar with oneflow.global_function , please refer to Recognizing MNIST Handwritten Digits and Job Function Definitions and Calls . Example of Configurations \u00b6 Configuration for prediction/inference \u00b6 Here we define a job function to evaluate the model: eval_job We set up the configurations of eval_job() in get_eval_config fucntion and pass it to @flow.global_function . At the same time, we set the type parameter of the @flow.global_function to \"predict\" for evaluation task. This way, OneFlow does not propagate backwards in this job function. def get_eval_config (): config = flow . function_config () config . default_data_type ( flow . float ) return config @flow . global_function ( type = \"predict\" , get_eval_config ()) def eval_job () -> tp . Numpy : # build neural network here Configuration for training \u00b6 If you specify the type parameter of @flow.global_function to be train , you can get a job function for training. In the following code, train_job is the job function used for training and it is configured with the default function_config (so there is no parameter passed to function_config ). The reason you need to specify the following settings like optimizer, learning rate and other hyperparameters in the job function is because OneFlow will back propagate for train functions. @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : with flow . scope . placement ( \"gpu\" , \"0:0\" ): logits = lenet ( images , train = True ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , logits , name = \"softmax_loss\" ) lr_scheduler = flow . optimizer . PiecewiseConstantScheduler ([], [ 0.1 ]) flow . optimizer . SGD ( lr_scheduler , momentum = 0 ) . minimize ( loss ) return loss In above code: PiecewiseConstantScheduler` sets the learning rate (0.1) and the learning strategy (PiecewiseConstantScheduler, a segment scaling strategy). There are other learning strategies built inside OneFlow. Such as: CosineScheduler \u3001 CustomScheduler \u3001 InverseTimeScheduler and etc. In flow.optimizer.SGD(lr_scheduler, momentum=0).minimize(loss) , set the optimizer to SGD and specify the optimization target as loss . OneFlow contains multiple optimizers such as: SGD \u3001 Adam \u3001 AdamW \u3001 LazyAdam \u3001 LARS \u3001 RMSProp . More information please refer to API documentation. FAQ \u00b6 Error Check failed: job().job_conf().train_conf().has_model_update_conf() If the type of the job function is \"train\" , but optimizer and optimization target are not configured. OneFlow will report an error during back propagation because OneFlow does not know how to update the parameters. Solution: Configure optimizer for the job function and specify the optimization target. Error Check failed: NeedBackwardOp If the type of the job function is \"predict\" but optimizer is incorrectly configured. Then optimizer cannot get the reversed data because OneFlow does not generate a reversed map for the predict job function. Solution: Remove the optimizer statement from the predict function.","title":"Optimization Algorithm and Parameter Configuration"},{"location":"single_client/basics_topics/optimizer_in_function_config.html#configuration-of-optimization-algorithms-and-hyperparameters","text":"After a neural network model has been set up, it usually requires training before using for prediction or inference. The training process means to optimize parameters of the nerwork which are usually updated with the back propagation algorithm and a specified optimizer. In this article, we will introduce how to setup optimizers and hyperparameters in OneFlow to users. Key point summary of this article: Configuration examples of job functions for training and prediction. The use of optimizer and learning strategies. Common errors due to misconfiguration and corresponding solutions. Users can directly use the training and inferencing configurations described in Example of configutraion section without knowing the design concept of OneFlow. For more detials please refer to optimizer api","title":"Configuration of Optimization Algorithms and Hyperparameters"},{"location":"single_client/basics_topics/optimizer_in_function_config.html#job-function-configuration","text":"In [Recognizing MNIST Handwritten Digits] (... /quick_start/lenet_mnist.md#global_function), we have learned about the concept of the oneflow.global_function decorator and the job function. The configuration of this article base on that. The job function can be configured by passing the function_config parameter to the decorator. If you are not familiar with oneflow.global_function , please refer to Recognizing MNIST Handwritten Digits and Job Function Definitions and Calls .","title":"Job Function Configuration"},{"location":"single_client/basics_topics/optimizer_in_function_config.html#example-of-configurations","text":"","title":"Example of Configurations"},{"location":"single_client/basics_topics/optimizer_in_function_config.html#configuration-for-predictioninference","text":"Here we define a job function to evaluate the model: eval_job We set up the configurations of eval_job() in get_eval_config fucntion and pass it to @flow.global_function . At the same time, we set the type parameter of the @flow.global_function to \"predict\" for evaluation task. This way, OneFlow does not propagate backwards in this job function. def get_eval_config (): config = flow . function_config () config . default_data_type ( flow . float ) return config @flow . global_function ( type = \"predict\" , get_eval_config ()) def eval_job () -> tp . Numpy : # build neural network here","title":"Configuration for prediction/inference"},{"location":"single_client/basics_topics/optimizer_in_function_config.html#configuration-for-training","text":"If you specify the type parameter of @flow.global_function to be train , you can get a job function for training. In the following code, train_job is the job function used for training and it is configured with the default function_config (so there is no parameter passed to function_config ). The reason you need to specify the following settings like optimizer, learning rate and other hyperparameters in the job function is because OneFlow will back propagate for train functions. @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : with flow . scope . placement ( \"gpu\" , \"0:0\" ): logits = lenet ( images , train = True ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , logits , name = \"softmax_loss\" ) lr_scheduler = flow . optimizer . PiecewiseConstantScheduler ([], [ 0.1 ]) flow . optimizer . SGD ( lr_scheduler , momentum = 0 ) . minimize ( loss ) return loss In above code: PiecewiseConstantScheduler` sets the learning rate (0.1) and the learning strategy (PiecewiseConstantScheduler, a segment scaling strategy). There are other learning strategies built inside OneFlow. Such as: CosineScheduler \u3001 CustomScheduler \u3001 InverseTimeScheduler and etc. In flow.optimizer.SGD(lr_scheduler, momentum=0).minimize(loss) , set the optimizer to SGD and specify the optimization target as loss . OneFlow contains multiple optimizers such as: SGD \u3001 Adam \u3001 AdamW \u3001 LazyAdam \u3001 LARS \u3001 RMSProp . More information please refer to API documentation.","title":"Configuration for training"},{"location":"single_client/basics_topics/optimizer_in_function_config.html#faq","text":"Error Check failed: job().job_conf().train_conf().has_model_update_conf() If the type of the job function is \"train\" , but optimizer and optimization target are not configured. OneFlow will report an error during back propagation because OneFlow does not know how to update the parameters. Solution: Configure optimizer for the job function and specify the optimization target. Error Check failed: NeedBackwardOp If the type of the job function is \"predict\" but optimizer is incorrectly configured. Then optimizer cannot get the reversed data because OneFlow does not generate a reversed map for the predict job function. Solution: Remove the optimizer statement from the predict function.","title":"FAQ"},{"location":"single_client/extended_topics/consistent_mirrored.html","text":"In distributed training, OneFlow provides two aspects for determining the relationship between data and model. There are consistent view and mirrored view. In this article, we will introduce: The difference and applicable scenario of data parallelism and model parallelism. The characteristics of mirrored view in distributed training. The characteristics of consistent view in distributed training. Data Parallelism and Model Parallelism. \u00b6 In order to better understand consistent and mirrored in OneFlow, we need to understand the difference between data parallelism and model parallelism in distributed training. In order to show the difference more visually, let's look at a simple Op: Matrix multiplication. We assume that during training, there is an input matrix I and the output matrix O is obtained by multiplying matrix I with another matrix W. As shown in the figure, the size of I is (N, C1), the size of W is (C1, C2) and the size of O is (N, C2). In machine learning, we can describe the matrixes as following: Matrix I is the input object, each row is a sample and each column represents the features of the sample. Matrix W represents the parameters of the model. Matrix O is the prediction result. If N in the matrix I is very large, we have large-scale samples. If C2 in matrix W is very large, it means we have a very complex model. If the scale and complexity reach a point, the single machine with a single device will not able to handle the training job. We might consider the distributed training. In a distributed training system, we can choose data parallelism and model parallelism . In order to better understand data parallelism and model parallelism, we use the following figure as the demo of matrix multiplication: The first matrix in grey on the left-hand side of the equation is the input sample. Each row is a sample. The second matrix in blue on the left-hand side of the equation is the parameter(model). In this section, we will see how the operators above are split in different ways in data parallelism and model parallelism. Data Parallelism Diagram \u00b6 In data parallelism , the sample data are divided into small parts. The divided data will send to each training node and calculate with the complete models . Finally, we combine the information in each node. As shown in the figure below: Model Parallelism Diagram \u00b6 In model parallelism , the model will be divided. Complete data will be sent to each node and calculate with the divided model . Finally, we combine the model in each node. As shown in the figure below: In conclusion: In data parallelism, each node uses the same model to train and the data is divided. In model parallelism, each node receives the same data and the model is divided. We will introduce two parallelism strategies in OneFlow ( mirrored and consistent ) and learn how to choose different parallelism methods in different strategies. Two Types of Placeholder \u00b6 In use OneFlow build neural network and The Definition and Call of Job Function , we have already introduced the concept of Placeholder and Blob . Actually, in the view of parallelism, the Placeholder of OneFlow can be divided into two types: Use oneflow.typing.Numpy.Placeholder and oneflow.typing.ListNumpy.Placeholder to construct the placeholder, which is corresponding to Consistent and Mirrored . We will explain them in detail in the examples below. Using Mirrored View in OneFlow \u00b6 Other frameworks like TensorFlow or Pytorch support mirrored view . The mirrored view of OneFlow is similar to them. In mirrored view, the model are copied in each GPU, the graph building on each node is the same, thus we can only use data parallelism . In OneFlow, the default strategy is consistent view, so you should use default_logical_view of flow.function_config() to define: func_config = flow . function_config () func_config . default_logical_view ( flow . scope . mirrored_view ()) In mirrored_view , we can only use data parallelism . When we call the job function, we need to divide the data evenly according to the amount of the devices and put the data after dividing it into a list . Every element in the list is the data to send to each device . The return value type of job function is oneflow.typing.ListNumpy . Every element in the list is corresponding to the results of each device. Concat all elements in the list can make a complete batch. Code \u00b6 In the following code, we use mirrored_view with two devices to train. Complete Code: mirrored_strategy.py We will explain the key part of the code in detail in the following \"code explanation\" section. Code explanation \u00b6 In the above code: Use flow.config.gpu_device_num to set device amount as 2. flow . config . gpu_device_num ( 2 ) oneflow.typing.ListNumpy.Placeholder defined the sample amount which is divided. And the relationship between BATCH_SIZE_PER_GPU and BATCH_SIZE is BATCH_SIZE=BATCH_SIZE_PER_GPU\u00d7GPU_NUM . def train_job ( images : tp . ListNumpy . Placeholder (( BATCH_SIZE_PER_GPU , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . ListNumpy . Placeholder (( BATCH_SIZE_PER_GPU ,), dtype = flow . int32 ), ) -> tp . ListNumpy : The data after dividing need to be stored in the list and passed to training functions. The number of elements in list need to be same as the amount of devices in the training process . The i-th element in list will be sent to the i-th device: images1 = images [: BATCH_SIZE_PER_GPU ] images2 = images [ BATCH_SIZE_PER_GPU :] labels1 = labels [: BATCH_SIZE_PER_GPU ] labels2 = labels [ BATCH_SIZE_PER_GPU :] imgs_list = [ images1 , images2 ] labels_list = [ labels1 , labels2 ] loss = train_job ( imgs_list , labels_list ) The return result loss is a list , the number of elements in this list need to be same as the amount of devices in the training process . Then we concat them and print the total_loss . total_loss = np . array ([ * loss [ 0 ], * loss [ 1 ]]) if i % 20 == 0 : print ( total_loss . mean ()) Use consistent view in OneFlow \u00b6 We have already learned about the mirrored view, where samples will be distributed evenly while the models are the same in every device, and the results of each node need to be assembled to get the complete batch. In addition to mirrored view, OneFlow also provides consistent view. Consistent view is one of the features of OneFlow. Compared with mirrored view, it has a great advantage. OneFlow will use consistent view as default. We can declare it explicitly as the following code. config = flow . function_config () config . default_distribute_strategy ( flow . scope . consistent_view ()) The reason why consistent view is the main feature of OneFlow is that in OneFlow design, if we use consistent_view , multiple devices in a distributed system can get consistently in logic level from user's point of view. We use matrix multiplication as an example in the beginning of article, we only need focus on matrix multiplication itself on mathematics level. But in project, the issue of how to config and use model parallelism or data parallelism can be easily done in OneFlow. OneFlow will handle The data division in data parallelism , model division in model parallelism and serial logic issue quickly and efficiently. In consistent view of OneFlow, we can choose model parallelism, data parallelism, or hybrid parallelism freely. Code Example \u00b6 In the following code, we use consistent view and use two devices to train. The default parallelism method is data parallelism in consistent view. The issue of how to set model parallelism and hybrid parallelism in consistent view will be discussed in parallels features of OneFlow . Complete code: consistent_strategy.py Code explanation \u00b6 In above code: Use flow.config.gpu_device_num to set the amount of devices: flow . config . gpu_device_num ( 2 ) Use tp.Numpy.Placeholder to define the placeholder in consistent view. Because the blob of Numpy.Placeholder represent the op and placeholder in logic. Thus. the BATCH_SIZE is the sum of samples, without artificial split or combination @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : The job function is called directly to obtain the training results. The splitting and concatenating in the distributed training are completed automatically by OneFlow. In the consistent view, there are few differences between the single-machine training program and the distributed training program. for i , ( images , labels ) in enumerate ( zip ( train_images , train_labels )): loss = train_job ( images , labels ) if i % 20 == 0 : print ( loss . mean ()) More extension \u00b6 With the development of machine learning theory and practice, there are many models unable to train in a single device or by data parallelism only. Adopting consistent view in OneFlow, the above problems can be solved well through free selection and combination of parallel methods. We will introduce in parallel features of OneFlow .","title":"Consistent & Mirrored View"},{"location":"single_client/extended_topics/consistent_mirrored.html#data-parallelism-and-model-parallelism","text":"In order to better understand consistent and mirrored in OneFlow, we need to understand the difference between data parallelism and model parallelism in distributed training. In order to show the difference more visually, let's look at a simple Op: Matrix multiplication. We assume that during training, there is an input matrix I and the output matrix O is obtained by multiplying matrix I with another matrix W. As shown in the figure, the size of I is (N, C1), the size of W is (C1, C2) and the size of O is (N, C2). In machine learning, we can describe the matrixes as following: Matrix I is the input object, each row is a sample and each column represents the features of the sample. Matrix W represents the parameters of the model. Matrix O is the prediction result. If N in the matrix I is very large, we have large-scale samples. If C2 in matrix W is very large, it means we have a very complex model. If the scale and complexity reach a point, the single machine with a single device will not able to handle the training job. We might consider the distributed training. In a distributed training system, we can choose data parallelism and model parallelism . In order to better understand data parallelism and model parallelism, we use the following figure as the demo of matrix multiplication: The first matrix in grey on the left-hand side of the equation is the input sample. Each row is a sample. The second matrix in blue on the left-hand side of the equation is the parameter(model). In this section, we will see how the operators above are split in different ways in data parallelism and model parallelism.","title":"Data Parallelism and Model Parallelism."},{"location":"single_client/extended_topics/consistent_mirrored.html#data-parallelism-diagram","text":"In data parallelism , the sample data are divided into small parts. The divided data will send to each training node and calculate with the complete models . Finally, we combine the information in each node. As shown in the figure below:","title":"Data Parallelism Diagram"},{"location":"single_client/extended_topics/consistent_mirrored.html#model-parallelism-diagram","text":"In model parallelism , the model will be divided. Complete data will be sent to each node and calculate with the divided model . Finally, we combine the model in each node. As shown in the figure below: In conclusion: In data parallelism, each node uses the same model to train and the data is divided. In model parallelism, each node receives the same data and the model is divided. We will introduce two parallelism strategies in OneFlow ( mirrored and consistent ) and learn how to choose different parallelism methods in different strategies.","title":"Model Parallelism Diagram"},{"location":"single_client/extended_topics/consistent_mirrored.html#two-types-of-placeholder","text":"In use OneFlow build neural network and The Definition and Call of Job Function , we have already introduced the concept of Placeholder and Blob . Actually, in the view of parallelism, the Placeholder of OneFlow can be divided into two types: Use oneflow.typing.Numpy.Placeholder and oneflow.typing.ListNumpy.Placeholder to construct the placeholder, which is corresponding to Consistent and Mirrored . We will explain them in detail in the examples below.","title":"Two Types of Placeholder"},{"location":"single_client/extended_topics/consistent_mirrored.html#using-mirrored-view-in-oneflow","text":"Other frameworks like TensorFlow or Pytorch support mirrored view . The mirrored view of OneFlow is similar to them. In mirrored view, the model are copied in each GPU, the graph building on each node is the same, thus we can only use data parallelism . In OneFlow, the default strategy is consistent view, so you should use default_logical_view of flow.function_config() to define: func_config = flow . function_config () func_config . default_logical_view ( flow . scope . mirrored_view ()) In mirrored_view , we can only use data parallelism . When we call the job function, we need to divide the data evenly according to the amount of the devices and put the data after dividing it into a list . Every element in the list is the data to send to each device . The return value type of job function is oneflow.typing.ListNumpy . Every element in the list is corresponding to the results of each device. Concat all elements in the list can make a complete batch.","title":"Using Mirrored View in OneFlow"},{"location":"single_client/extended_topics/consistent_mirrored.html#code","text":"In the following code, we use mirrored_view with two devices to train. Complete Code: mirrored_strategy.py We will explain the key part of the code in detail in the following \"code explanation\" section.","title":"Code"},{"location":"single_client/extended_topics/consistent_mirrored.html#code-explanation","text":"In the above code: Use flow.config.gpu_device_num to set device amount as 2. flow . config . gpu_device_num ( 2 ) oneflow.typing.ListNumpy.Placeholder defined the sample amount which is divided. And the relationship between BATCH_SIZE_PER_GPU and BATCH_SIZE is BATCH_SIZE=BATCH_SIZE_PER_GPU\u00d7GPU_NUM . def train_job ( images : tp . ListNumpy . Placeholder (( BATCH_SIZE_PER_GPU , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . ListNumpy . Placeholder (( BATCH_SIZE_PER_GPU ,), dtype = flow . int32 ), ) -> tp . ListNumpy : The data after dividing need to be stored in the list and passed to training functions. The number of elements in list need to be same as the amount of devices in the training process . The i-th element in list will be sent to the i-th device: images1 = images [: BATCH_SIZE_PER_GPU ] images2 = images [ BATCH_SIZE_PER_GPU :] labels1 = labels [: BATCH_SIZE_PER_GPU ] labels2 = labels [ BATCH_SIZE_PER_GPU :] imgs_list = [ images1 , images2 ] labels_list = [ labels1 , labels2 ] loss = train_job ( imgs_list , labels_list ) The return result loss is a list , the number of elements in this list need to be same as the amount of devices in the training process . Then we concat them and print the total_loss . total_loss = np . array ([ * loss [ 0 ], * loss [ 1 ]]) if i % 20 == 0 : print ( total_loss . mean ())","title":"Code explanation"},{"location":"single_client/extended_topics/consistent_mirrored.html#use-consistent-view-in-oneflow","text":"We have already learned about the mirrored view, where samples will be distributed evenly while the models are the same in every device, and the results of each node need to be assembled to get the complete batch. In addition to mirrored view, OneFlow also provides consistent view. Consistent view is one of the features of OneFlow. Compared with mirrored view, it has a great advantage. OneFlow will use consistent view as default. We can declare it explicitly as the following code. config = flow . function_config () config . default_distribute_strategy ( flow . scope . consistent_view ()) The reason why consistent view is the main feature of OneFlow is that in OneFlow design, if we use consistent_view , multiple devices in a distributed system can get consistently in logic level from user's point of view. We use matrix multiplication as an example in the beginning of article, we only need focus on matrix multiplication itself on mathematics level. But in project, the issue of how to config and use model parallelism or data parallelism can be easily done in OneFlow. OneFlow will handle The data division in data parallelism , model division in model parallelism and serial logic issue quickly and efficiently. In consistent view of OneFlow, we can choose model parallelism, data parallelism, or hybrid parallelism freely.","title":"Use consistent view in OneFlow"},{"location":"single_client/extended_topics/consistent_mirrored.html#code-example","text":"In the following code, we use consistent view and use two devices to train. The default parallelism method is data parallelism in consistent view. The issue of how to set model parallelism and hybrid parallelism in consistent view will be discussed in parallels features of OneFlow . Complete code: consistent_strategy.py","title":"Code Example"},{"location":"single_client/extended_topics/consistent_mirrored.html#code-explanation_1","text":"In above code: Use flow.config.gpu_device_num to set the amount of devices: flow . config . gpu_device_num ( 2 ) Use tp.Numpy.Placeholder to define the placeholder in consistent view. Because the blob of Numpy.Placeholder represent the op and placeholder in logic. Thus. the BATCH_SIZE is the sum of samples, without artificial split or combination @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : The job function is called directly to obtain the training results. The splitting and concatenating in the distributed training are completed automatically by OneFlow. In the consistent view, there are few differences between the single-machine training program and the distributed training program. for i , ( images , labels ) in enumerate ( zip ( train_images , train_labels )): loss = train_job ( images , labels ) if i % 20 == 0 : print ( loss . mean ())","title":"Code explanation"},{"location":"single_client/extended_topics/consistent_mirrored.html#more-extension","text":"With the development of machine learning theory and practice, there are many models unable to train in a single device or by data parallelism only. Adopting consistent view in OneFlow, the above problems can be solved well through free selection and combination of parallel methods. We will introduce in parallel features of OneFlow .","title":"More extension"},{"location":"single_client/extended_topics/how_to_convert_image_to_ofrecord.html","text":"Convert Image Files to OFRecord Datasets \u00b6 In OFRecord Data Format and Loading and Preparing OFRecord Dataset , we learned how to convert other dataset formats to OFRecord separately and how to load OFRecord datasets. In this article, we will explain how to make image files into OFRecord datasets. Also we provide relevant script for users to use directly or make modification base on that, which includes: Make OFRecord datasets based on MNIST dataset. How OFRecord Reader is encoded. Training on OFRecord dataset. Make OFRecord Datasets Based on Image Files \u00b6 We use MNIST Handwritten Digits dataset to produce an OFRecord format file. we only take 50 pictures for demonstration. Please refer to img2ofrecord for relevant script and dataset. img2ofrecord . Download and unzip the relevant zip file $ wget https://oneflow-static.oss-cn-beijing.aliyuncs.com/oneflow-tutorial-attachments/img2ofrecord.zip $ unzip img2ofrecord.zip Change directory to corresponding path and run OFRecord production script img2ofrecord.py $ cd ./img_to_ofrecord $ python img2ofrecord.py --part_num=5 --save_dir=./dataset/ --img_format=.png --image_root=./images/train_set/ The following output will display as the script runs. The image root is: ./images/train_set/ The amount of OFRecord data part is: 5 The directory of Labels is: ./images/train_label/label.txt The image format is: .png The OFRecord save directory is: ./dataset/ Start Processing...... ./images/train_set/00000030_3.png feature saved ./images/train_set/00000034_0.png feature saved ./images/train_set/00000026_4.png feature saved ./images/train_set/00000043_9.png feature saved ...... Process image successfully !!! Thus far, we have created the OFRecord file and saved it under ./dataset . Code Explanation \u00b6 The hierarchy of code directory is: img_to_ofrecord \u251c\u2500\u2500 images \u251c\u2500\u2500 train_set \u251c\u2500\u2500 00000000_5.png \u251c\u2500\u2500 00000001_0.png \u251c\u2500\u2500 00000002_4.png ...... \u251c\u2500\u2500 train_label \u251c\u2500\u2500 label.txt \u251c\u2500\u2500 img2ofrecord.py \u251c\u2500\u2500 lenet_train.py images directory holds the original training dataset and label file. The label file is stored as json here in following format\uff1a {\"00000030_3.png\": 3} {\"00000034_0.png\": 0} {\"00000026_4.png\": 4} {\"00000043_9.png\": 9} {\"00000047_5.png\": 5} {\"00000003_1.png\": 1} ...... img2ofrecord.py is the script which converts image files in train_set to OFRecord dataset. lenet_train.py is the script loading OFRecord we just made for training. The command options of img2ofrecord.py are: - image_root specify the root directory of the image. - part_num specify the number of OFRecord files to generate. An error is reported if the number is greater than the total number of images. - label_dir specify the directory of the label. - img_format specify the format of the image. - save_dir specify the directory where the OFRecord file will be saved. How OFRecord Reader is Encoded \u00b6 The code associated with the encoding of OFRecord files is in img2ofrecord.py . The encoding process is as follows\uff1a First, encoding the incoming image data. def encode_img_file ( filename , ext = \".jpg\" ): img = cv2 . imread ( filename ) encoded_data = cv2 . imencode ( ext , img )[ 1 ] return encoded_data . tostring () The ext is the image encoding format. Currently, The format supported by ONEFLOW image encoding and decoding is consistent with that of OpenCV, which can be refered in cv::ImwriteFlags for details. JPEG, one of the most common lossy code formats. Please refer to JPEG . PNG, a common lossless bitmap encoding format. Please refer to Portable Network Graphics . TIFF, a extensible compressed encoding format. Please refer to Tagged Image File Format . Second, data is converted to the form of Feature, serialized, and the data length is written to the file. def ndarray2ofrecords ( dsfile , dataname , encoded_data , labelname , encoded_label ): topack = { dataname : bytes_feature ( encoded_data ), labelname : int32_feature ( encoded_label )} ofrecord_features = ofrecord . OFRecord ( feature = topack ) serilizedBytes = ofrecord_features . SerializeToString () length = ofrecord_features . ByteSize () dsfile . write ( struct . pack ( \"q\" , length )) dsfile . write ( serilizedBytes ) Training on OFRecord Dataset \u00b6 We run lenet_train.py . It will read the OFRecord dataset that we have just created and train it on the LeNet model. The outputs of training script should like below: [6.778578] [2.0212684] [1.3814741] [0.47514156] [0.13277876] [0.16388433] [0.03788032] [0.01225162] ...... At this point, we have successfully completed the whole process of dataset production, reading and training.","title":"Convert Image Files to OFRecord Datasets"},{"location":"single_client/extended_topics/how_to_convert_image_to_ofrecord.html#convert-image-files-to-ofrecord-datasets","text":"In OFRecord Data Format and Loading and Preparing OFRecord Dataset , we learned how to convert other dataset formats to OFRecord separately and how to load OFRecord datasets. In this article, we will explain how to make image files into OFRecord datasets. Also we provide relevant script for users to use directly or make modification base on that, which includes: Make OFRecord datasets based on MNIST dataset. How OFRecord Reader is encoded. Training on OFRecord dataset.","title":"Convert Image Files to OFRecord Datasets"},{"location":"single_client/extended_topics/how_to_convert_image_to_ofrecord.html#make-ofrecord-datasets-based-on-image-files","text":"We use MNIST Handwritten Digits dataset to produce an OFRecord format file. we only take 50 pictures for demonstration. Please refer to img2ofrecord for relevant script and dataset. img2ofrecord . Download and unzip the relevant zip file $ wget https://oneflow-static.oss-cn-beijing.aliyuncs.com/oneflow-tutorial-attachments/img2ofrecord.zip $ unzip img2ofrecord.zip Change directory to corresponding path and run OFRecord production script img2ofrecord.py $ cd ./img_to_ofrecord $ python img2ofrecord.py --part_num=5 --save_dir=./dataset/ --img_format=.png --image_root=./images/train_set/ The following output will display as the script runs. The image root is: ./images/train_set/ The amount of OFRecord data part is: 5 The directory of Labels is: ./images/train_label/label.txt The image format is: .png The OFRecord save directory is: ./dataset/ Start Processing...... ./images/train_set/00000030_3.png feature saved ./images/train_set/00000034_0.png feature saved ./images/train_set/00000026_4.png feature saved ./images/train_set/00000043_9.png feature saved ...... Process image successfully !!! Thus far, we have created the OFRecord file and saved it under ./dataset .","title":"Make OFRecord Datasets Based on Image Files"},{"location":"single_client/extended_topics/how_to_convert_image_to_ofrecord.html#code-explanation","text":"The hierarchy of code directory is: img_to_ofrecord \u251c\u2500\u2500 images \u251c\u2500\u2500 train_set \u251c\u2500\u2500 00000000_5.png \u251c\u2500\u2500 00000001_0.png \u251c\u2500\u2500 00000002_4.png ...... \u251c\u2500\u2500 train_label \u251c\u2500\u2500 label.txt \u251c\u2500\u2500 img2ofrecord.py \u251c\u2500\u2500 lenet_train.py images directory holds the original training dataset and label file. The label file is stored as json here in following format\uff1a {\"00000030_3.png\": 3} {\"00000034_0.png\": 0} {\"00000026_4.png\": 4} {\"00000043_9.png\": 9} {\"00000047_5.png\": 5} {\"00000003_1.png\": 1} ...... img2ofrecord.py is the script which converts image files in train_set to OFRecord dataset. lenet_train.py is the script loading OFRecord we just made for training. The command options of img2ofrecord.py are: - image_root specify the root directory of the image. - part_num specify the number of OFRecord files to generate. An error is reported if the number is greater than the total number of images. - label_dir specify the directory of the label. - img_format specify the format of the image. - save_dir specify the directory where the OFRecord file will be saved.","title":"Code Explanation"},{"location":"single_client/extended_topics/how_to_convert_image_to_ofrecord.html#how-ofrecord-reader-is-encoded","text":"The code associated with the encoding of OFRecord files is in img2ofrecord.py . The encoding process is as follows\uff1a First, encoding the incoming image data. def encode_img_file ( filename , ext = \".jpg\" ): img = cv2 . imread ( filename ) encoded_data = cv2 . imencode ( ext , img )[ 1 ] return encoded_data . tostring () The ext is the image encoding format. Currently, The format supported by ONEFLOW image encoding and decoding is consistent with that of OpenCV, which can be refered in cv::ImwriteFlags for details. JPEG, one of the most common lossy code formats. Please refer to JPEG . PNG, a common lossless bitmap encoding format. Please refer to Portable Network Graphics . TIFF, a extensible compressed encoding format. Please refer to Tagged Image File Format . Second, data is converted to the form of Feature, serialized, and the data length is written to the file. def ndarray2ofrecords ( dsfile , dataname , encoded_data , labelname , encoded_label ): topack = { dataname : bytes_feature ( encoded_data ), labelname : int32_feature ( encoded_label )} ofrecord_features = ofrecord . OFRecord ( feature = topack ) serilizedBytes = ofrecord_features . SerializeToString () length = ofrecord_features . ByteSize () dsfile . write ( struct . pack ( \"q\" , length )) dsfile . write ( serilizedBytes )","title":"How OFRecord Reader is Encoded"},{"location":"single_client/extended_topics/how_to_convert_image_to_ofrecord.html#training-on-ofrecord-dataset","text":"We run lenet_train.py . It will read the OFRecord dataset that we have just created and train it on the LeNet model. The outputs of training script should like below: [6.778578] [2.0212684] [1.3814741] [0.47514156] [0.13277876] [0.16388433] [0.03788032] [0.01225162] ...... At this point, we have successfully completed the whole process of dataset production, reading and training.","title":"Training on OFRecord Dataset"},{"location":"single_client/extended_topics/how_to_make_ofdataset.html","text":"In [data input] (... /basics_topics/data_input.md) we learned that it is usually more efficient to load data using DataLoader and related operators. Also, we learned how to use DataLoader and related operators. In article OFRecord , we learn about the storage format of OFRecord files. In this article, we will focus on the loading and generating of OneFlow's OFRecord dataset, which mainly includes: The hierarchy of OFRecord dataset Multiple ways of loading OFRecord dataset The transition between OFRecord dataset and other data formats What is OFRecord Dataset \u00b6 In article OFRecord , we introduce what OFRecord file is and the storage format of OFRecord file . OFRecord dataset is the collection of OFRecord files . The collection of mutiple files that named by OneFlow convention, and that stored in the same directory, is an OFRecord dataset. By default, The files in OFRecord dataset directory are uniformly named in the way of part-xxx , where \"xxx\" is the file id starting from zero, and there can be choices about padding or non-padding. These are the examples of using non-padding name style: mnist_kaggle/train/ \u251c\u2500\u2500 part-0 \u251c\u2500\u2500 part-1 \u251c\u2500\u2500 part-10 \u251c\u2500\u2500 part-11 \u251c\u2500\u2500 part-12 \u251c\u2500\u2500 part-13 \u251c\u2500\u2500 part-14 \u251c\u2500\u2500 part-15 \u251c\u2500\u2500 part-2 \u251c\u2500\u2500 part-3 \u251c\u2500\u2500 part-4 \u251c\u2500\u2500 part-5 \u251c\u2500\u2500 part-6 \u251c\u2500\u2500 part-7 \u251c\u2500\u2500 part-8 \u2514\u2500\u2500 part-9 These are the examples of using padding name style: mnist_kaggle/train/ \u251c\u2500\u2500 part-00000 \u251c\u2500\u2500 part-00001 \u251c\u2500\u2500 part-00002 \u251c\u2500\u2500 part-00003 \u251c\u2500\u2500 part-00004 \u251c\u2500\u2500 part-00005 \u251c\u2500\u2500 part-00006 \u251c\u2500\u2500 part-00007 \u251c\u2500\u2500 part-00008 \u251c\u2500\u2500 part-00009 \u251c\u2500\u2500 part-00010 \u251c\u2500\u2500 part-00011 \u251c\u2500\u2500 part-00012 \u251c\u2500\u2500 part-00013 \u251c\u2500\u2500 part-00014 \u251c\u2500\u2500 part-00015 OneFlow adopts this convention, which is consistent with the default storage filename in spark , so it is convenient to prepare OFRecord data by spark. Actually, we can specify the filename prefix part- , whether we pad the filename id and how many bits to pad. We just need to keep the same parameters when loading dataset, which will be described below. OneFlow provides the API interface to load OFRecord dataset by specifying the path of dataset directory, so that we can have the multi-threading, pipelining and some other advantages brought by OneFlow framework. The Method to Load OFRecord Dataset \u00b6 We use ofrecord_reader to load and preprocess dataset. In article Data Input , we have shown how to use ofrecord_reader API to load OFRecord data and preprocess it. Code: of_data_pipeline.py The prototype of ofrecord_reader is as follows\uff1a def ofrecord_reader ( ofrecord_dir , batch_size = 1 , data_part_num = 1 , part_name_prefix = \"part-\" , part_name_suffix_length =- 1 , random_shuffle = False , shuffle_buffer_size = 1024 , shuffle_after_epoch = False , name = None , ) ofrecord_dir is the directory which stored the dataset batchsize assign the batch size in each epoch data_part_num assign the number of ofrecord data format file in the directory which stored the dataset. It will raise an error if the parameter is greater than the number of the existed files part_name_prefix assign the filename prefix of ofrecord files. Oneflow locates the ofrecord files according to the prefix + index in the dataset directory part_name_suffix_length assigns the padding of ofrecord file index, -1 represents no padding random_shuffle assign whether shuffle the sample order randomly when reading data shuffle_buffer_size assign the buffer size when reading data shuffle_after_epoch assign whether shuffle the sample order after each epoch The benefit of using ofrecord_reader is that ofrecord_reader acts as a normal operator which participates in OneFlow composition optimization and enjoys OneFlow pipeline acceleration. For flexibility and extensibility of the code, we can define a preprocessing OP for ofrecord_reader to deal with specific data formats which are coupled with operational logic (e.g. decoding, decompression and etc.). For more information on DataLoader and related operator usage refer to Data input . The transition between other data format data and OFRecord dataset \u00b6 According to the storage format of OFRecord file in article OFRecord and the filename format convention of OFRecord dataset introduced at the beginning, we can prepare OFRecord dataset by ourselves. To prepare dataset easier, we provide jar package from Spark, which is convenient to the interconversion between OFRecord and common data formats (such as TFRecord and JSON). The installation and launch of Spark \u00b6 At first, we should download Spark and Spark-oneflow-connector\uff1a Download the spark-2.4.7-bin-hadoop2.7.tgz from the official website of Spark Download jar package here , which is needed by Spark to support the ofrecord file format Then unzip the spark-2.4.7-bin-hadoop2.7.tgz and configure the environment variable SPARK_HOME : export SPARK_HOME=path/to/spark-2.4.7-bin-hadoop2.7 export PATH=$SPARK_HOME/bin:$PATH We can launch the pyspark shell with the following command\uff1a pyspark --master \"local[*]\"\\ --jars spark-oneflow-connector-assembly-0.1.0_int64.jar\\ --packages org.tensorflow:spark-tensorflow-connector_2.11:1.13.1 Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 2.4.7 /_/ Using Python version 3.6.10 (default, Mar 25 2020 18:53:43) SparkSession available as 'spark'. We can complete the data conversion between OFRecord dataset and other formats in pyspark shell. Use Spark to view OFRecord dataset \u00b6 We can view OFRecord data with following code\uff1a spark.read.format(\"ofrecord\").load(\"file:///path/to/ofrecord_file\").show() The first 20 rows are displayed by default: +--------------------+------+ | images|labels| +--------------------+------+ |[0.33967614, 0.87...| 2| |[0.266905, 0.9730...| 3| |[0.66661334, 0.67...| 1| |[0.91943026, 0.89...| 6| |[0.014844197, 0.0...| 6| |[0.5366513, 0.748...| 4| |[0.055148937, 0.7...| 7| |[0.7814437, 0.228...| 4| |[0.31193638, 0.55...| 3| |[0.20034336, 0.24...| 4| |[0.09441255, 0.07...| 3| |[0.5177533, 0.397...| 0| |[0.23703437, 0.44...| 9| |[0.9425567, 0.859...| 9| |[0.017339867, 0.0...| 3| |[0.827106, 0.3122...| 0| |[0.8641392, 0.194...| 2| |[0.95585227, 0.29...| 3| |[0.7508129, 0.464...| 4| |[0.035597708, 0.3...| 9| +--------------------+------+ only showing top 20 rows The interconversion with TFRecord dataset \u00b6 we can convert TFRecord to OFRecord with the following command\uff1a reader = spark . read . format ( \"tfrecords\" ) dataframe = reader . load ( \"file:///path/to/tfrecord_file\" ) writer = dataframe . write . format ( \"ofrecord\" ) writer . save ( \"file:///path/to/outputdir\" ) In the above code, the outputdir directory will be created automatically, and ofrecord files will be saved into this directory. Make sure that the \"outputdir\" directory does not exist before executing the command. In addition, we can use the following command to split data into multiple ofrecord files. reader = spark . read . format ( \"tfrecords\" ) dataframe = reader . load ( \"file:///path/to/tfrecord_file\" ) writer = dataframe . repartition ( 10 ) . write . format ( \"ofrecord\" ) writer . save ( \"file://path/to/outputdir\" ) After executing the above commands, 10 ofrecord files of part-xxx format will be generated in \"outputdir\" directory. The process of converting OFRecord file to TFRecord file is similar. we just need to change the format of read/write side: reader = spark . read . format ( \"ofrecord\" ) dataframe = reader . load ( \"file:///path/to/ofrecord_file\" ) writer = dataframe . write . format ( \"tfrecords\" ) writer . save ( \"file:///path/to/outputdir\" ) The interconversion with JSON format \u00b6 We can convert JSON to OFRecord with the following command\uff1a dataframe = spark . read . json ( \"file:///path/to/json_file\" ) writer = dataframe . write . format ( \"ofrecord\" ) writer . save ( \"file:///path/to/outputdir\" ) The following command will convert OFRecord data to JSON files\uff1a reader = spark . read . format ( \"ofrecord\" ) dataframe = reader . load ( \"file:///path/to/ofrecord_file\" ) dataframe . write . json ( \"file://path/to/outputdir\" )","title":"Loading and Preparing OFRecord Dataset"},{"location":"single_client/extended_topics/how_to_make_ofdataset.html#what-is-ofrecord-dataset","text":"In article OFRecord , we introduce what OFRecord file is and the storage format of OFRecord file . OFRecord dataset is the collection of OFRecord files . The collection of mutiple files that named by OneFlow convention, and that stored in the same directory, is an OFRecord dataset. By default, The files in OFRecord dataset directory are uniformly named in the way of part-xxx , where \"xxx\" is the file id starting from zero, and there can be choices about padding or non-padding. These are the examples of using non-padding name style: mnist_kaggle/train/ \u251c\u2500\u2500 part-0 \u251c\u2500\u2500 part-1 \u251c\u2500\u2500 part-10 \u251c\u2500\u2500 part-11 \u251c\u2500\u2500 part-12 \u251c\u2500\u2500 part-13 \u251c\u2500\u2500 part-14 \u251c\u2500\u2500 part-15 \u251c\u2500\u2500 part-2 \u251c\u2500\u2500 part-3 \u251c\u2500\u2500 part-4 \u251c\u2500\u2500 part-5 \u251c\u2500\u2500 part-6 \u251c\u2500\u2500 part-7 \u251c\u2500\u2500 part-8 \u2514\u2500\u2500 part-9 These are the examples of using padding name style: mnist_kaggle/train/ \u251c\u2500\u2500 part-00000 \u251c\u2500\u2500 part-00001 \u251c\u2500\u2500 part-00002 \u251c\u2500\u2500 part-00003 \u251c\u2500\u2500 part-00004 \u251c\u2500\u2500 part-00005 \u251c\u2500\u2500 part-00006 \u251c\u2500\u2500 part-00007 \u251c\u2500\u2500 part-00008 \u251c\u2500\u2500 part-00009 \u251c\u2500\u2500 part-00010 \u251c\u2500\u2500 part-00011 \u251c\u2500\u2500 part-00012 \u251c\u2500\u2500 part-00013 \u251c\u2500\u2500 part-00014 \u251c\u2500\u2500 part-00015 OneFlow adopts this convention, which is consistent with the default storage filename in spark , so it is convenient to prepare OFRecord data by spark. Actually, we can specify the filename prefix part- , whether we pad the filename id and how many bits to pad. We just need to keep the same parameters when loading dataset, which will be described below. OneFlow provides the API interface to load OFRecord dataset by specifying the path of dataset directory, so that we can have the multi-threading, pipelining and some other advantages brought by OneFlow framework.","title":"What is OFRecord Dataset"},{"location":"single_client/extended_topics/how_to_make_ofdataset.html#the-method-to-load-ofrecord-dataset","text":"We use ofrecord_reader to load and preprocess dataset. In article Data Input , we have shown how to use ofrecord_reader API to load OFRecord data and preprocess it. Code: of_data_pipeline.py The prototype of ofrecord_reader is as follows\uff1a def ofrecord_reader ( ofrecord_dir , batch_size = 1 , data_part_num = 1 , part_name_prefix = \"part-\" , part_name_suffix_length =- 1 , random_shuffle = False , shuffle_buffer_size = 1024 , shuffle_after_epoch = False , name = None , ) ofrecord_dir is the directory which stored the dataset batchsize assign the batch size in each epoch data_part_num assign the number of ofrecord data format file in the directory which stored the dataset. It will raise an error if the parameter is greater than the number of the existed files part_name_prefix assign the filename prefix of ofrecord files. Oneflow locates the ofrecord files according to the prefix + index in the dataset directory part_name_suffix_length assigns the padding of ofrecord file index, -1 represents no padding random_shuffle assign whether shuffle the sample order randomly when reading data shuffle_buffer_size assign the buffer size when reading data shuffle_after_epoch assign whether shuffle the sample order after each epoch The benefit of using ofrecord_reader is that ofrecord_reader acts as a normal operator which participates in OneFlow composition optimization and enjoys OneFlow pipeline acceleration. For flexibility and extensibility of the code, we can define a preprocessing OP for ofrecord_reader to deal with specific data formats which are coupled with operational logic (e.g. decoding, decompression and etc.). For more information on DataLoader and related operator usage refer to Data input .","title":"The Method to Load OFRecord Dataset"},{"location":"single_client/extended_topics/how_to_make_ofdataset.html#the-transition-between-other-data-format-data-and-ofrecord-dataset","text":"According to the storage format of OFRecord file in article OFRecord and the filename format convention of OFRecord dataset introduced at the beginning, we can prepare OFRecord dataset by ourselves. To prepare dataset easier, we provide jar package from Spark, which is convenient to the interconversion between OFRecord and common data formats (such as TFRecord and JSON).","title":"The transition between other data format data and OFRecord dataset"},{"location":"single_client/extended_topics/how_to_make_ofdataset.html#the-installation-and-launch-of-spark","text":"At first, we should download Spark and Spark-oneflow-connector\uff1a Download the spark-2.4.7-bin-hadoop2.7.tgz from the official website of Spark Download jar package here , which is needed by Spark to support the ofrecord file format Then unzip the spark-2.4.7-bin-hadoop2.7.tgz and configure the environment variable SPARK_HOME : export SPARK_HOME=path/to/spark-2.4.7-bin-hadoop2.7 export PATH=$SPARK_HOME/bin:$PATH We can launch the pyspark shell with the following command\uff1a pyspark --master \"local[*]\"\\ --jars spark-oneflow-connector-assembly-0.1.0_int64.jar\\ --packages org.tensorflow:spark-tensorflow-connector_2.11:1.13.1 Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 2.4.7 /_/ Using Python version 3.6.10 (default, Mar 25 2020 18:53:43) SparkSession available as 'spark'. We can complete the data conversion between OFRecord dataset and other formats in pyspark shell.","title":"The installation and launch of Spark"},{"location":"single_client/extended_topics/how_to_make_ofdataset.html#use-spark-to-view-ofrecord-dataset","text":"We can view OFRecord data with following code\uff1a spark.read.format(\"ofrecord\").load(\"file:///path/to/ofrecord_file\").show() The first 20 rows are displayed by default: +--------------------+------+ | images|labels| +--------------------+------+ |[0.33967614, 0.87...| 2| |[0.266905, 0.9730...| 3| |[0.66661334, 0.67...| 1| |[0.91943026, 0.89...| 6| |[0.014844197, 0.0...| 6| |[0.5366513, 0.748...| 4| |[0.055148937, 0.7...| 7| |[0.7814437, 0.228...| 4| |[0.31193638, 0.55...| 3| |[0.20034336, 0.24...| 4| |[0.09441255, 0.07...| 3| |[0.5177533, 0.397...| 0| |[0.23703437, 0.44...| 9| |[0.9425567, 0.859...| 9| |[0.017339867, 0.0...| 3| |[0.827106, 0.3122...| 0| |[0.8641392, 0.194...| 2| |[0.95585227, 0.29...| 3| |[0.7508129, 0.464...| 4| |[0.035597708, 0.3...| 9| +--------------------+------+ only showing top 20 rows","title":"Use Spark to view OFRecord dataset"},{"location":"single_client/extended_topics/how_to_make_ofdataset.html#the-interconversion-with-tfrecord-dataset","text":"we can convert TFRecord to OFRecord with the following command\uff1a reader = spark . read . format ( \"tfrecords\" ) dataframe = reader . load ( \"file:///path/to/tfrecord_file\" ) writer = dataframe . write . format ( \"ofrecord\" ) writer . save ( \"file:///path/to/outputdir\" ) In the above code, the outputdir directory will be created automatically, and ofrecord files will be saved into this directory. Make sure that the \"outputdir\" directory does not exist before executing the command. In addition, we can use the following command to split data into multiple ofrecord files. reader = spark . read . format ( \"tfrecords\" ) dataframe = reader . load ( \"file:///path/to/tfrecord_file\" ) writer = dataframe . repartition ( 10 ) . write . format ( \"ofrecord\" ) writer . save ( \"file://path/to/outputdir\" ) After executing the above commands, 10 ofrecord files of part-xxx format will be generated in \"outputdir\" directory. The process of converting OFRecord file to TFRecord file is similar. we just need to change the format of read/write side: reader = spark . read . format ( \"ofrecord\" ) dataframe = reader . load ( \"file:///path/to/ofrecord_file\" ) writer = dataframe . write . format ( \"tfrecords\" ) writer . save ( \"file:///path/to/outputdir\" )","title":"The interconversion with TFRecord dataset"},{"location":"single_client/extended_topics/how_to_make_ofdataset.html#the-interconversion-with-json-format","text":"We can convert JSON to OFRecord with the following command\uff1a dataframe = spark . read . json ( \"file:///path/to/json_file\" ) writer = dataframe . write . format ( \"ofrecord\" ) writer . save ( \"file:///path/to/outputdir\" ) The following command will convert OFRecord data to JSON files\uff1a reader = spark . read . format ( \"ofrecord\" ) dataframe = reader . load ( \"file:///path/to/ofrecord_file\" ) dataframe . write . json ( \"file://path/to/outputdir\" )","title":"The interconversion with JSON format"},{"location":"single_client/extended_topics/job_function_define_call.html","text":"The Definition and Call of Job Function \u00b6 In OneFlow, we encapsulate the training, inference and some other tasks into a \"job function\". The job function is used to connect the user's business logic and the computing resource managed by OneFlow. In OneFlow, the function decorated by @oneflow.global_function decorator is the OneFlow's job function We mainly define the structure of the model and choose the optimization target in job function. In addition, we can also pass some hyperparameters about training and some configuration of the environment to the job function (like the following example: get_train_config() ), OneFlow will manage the memory, GPU and other computing resource according to our configuration. In this article, we will specifically learn about: how to define and call the job function how to get the return value of job function The Relationship Between Job Function and Running Process of OneFlow \u00b6 The job function is divided into two phases: definition and call. It's related to OneFlow's operating mechanism. Briefly, the OneFlow Python layer API simply describes the configuration and the training environment of the model. These information will pass to the C++ backend. After compilation, graph building and so on, the computation graph is obtained. Finally, the job function will be executed in OneFlow runtime. The job function describes the model and the training environment. In this phase there's no data. We can only define the shape and data type of the nodes (as known as PlaceHolder ) for creating and compiling the computation graph of OneFlow. The job function will be called after the OneFlow runtime starts. We can pass the data by calling job function and get the results. We will introduce the definition and calling method of job functions in detail as below. The Definition of Job Function \u00b6 We encapsulate the model in Python and use oneflow.global_function to decorate. Then the definition is completed. The job function mainly describes two things: The structure of model The optimizing target in training phase In the following code, we build a Multi-Layer Perceptron model and use flow.nn.sparse_softmax_cross_entropy_with_logits to compute the cross-entropy loss as our optimizing target. @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Callback [ tp . Numpy ]: # mlp initializer = flow . truncated_normal ( 0.1 ) reshape = flow . reshape ( images , [ images . shape [ 0 ], - 1 ]) hidden = flow . layers . dense ( reshape , 512 , activation = flow . nn . relu , kernel_initializer = initializer , name = \"hidden\" , ) logits = flow . layers . dense ( hidden , 10 , kernel_initializer = initializer , name = \"output\" ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , logits , name = \"softmax_loss\" ) lr_scheduler = flow . optimizer . PiecewiseConstantScheduler ([], [ 0.1 ]) flow . optimizer . SGD ( lr_scheduler , momentum = 0 ) . minimize ( loss ) return loss The Parameters of oneflow.global_function \u00b6 oneflow.global_function decorator accepts two parameters, type and function_config . The parameter type accepts a string, which can only set as train or predict . When we define a training model, we set it as train . We set is as predict when we define a model for testing or inferencing. The parameter function_config accepts an object which is constructed by oneflow.function_config() . In function_config object, we can use its method or attribute to config. As the following code. def get_train_config (): config = flow . function_config () config . default_data_type ( flow . float ) return config We set the default data type, then, we can pass the function_config object to the global_function decorator. @flow . global_function ( type = \"train\" , function_config = get_train_config ()) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : For the complete code, you can refer to Consistent and Mirrored 's mixed_parallel_mlp.py PlaceHolder \u00b6 Noted that the images \u3001 logits \u3001 labels \u3001 loss and some other objects have no data in our definition of the job function. They are used to describe the shape and attribute of data , which is called PlaceHolder . For the parameters of the job function, we use Numpy.Placeholder , ListNumpy.Placeholder , ListListNumpy.Placeholder in the oneflow.typing package to annotate the data type of them as numpy.ndarray , Sequence[numpy.ndarray] and Sequence[Sequence[numpy.ndarray]] respectively. Besides the types of oneflow.typing , the variables returned from OneFlow operators or layers in the job function, like the reshape \u3001 hidden \u3001 logits \u3001 loss in the code above, are also PlaceHolder. All the variables mentioned above inherit the base class BlobDef directly or indirectly. We call this object type as Blob in OneFlow. The Blob has no data when defining the job function. It only plays the role of data placeholder for building the graph. The Return Value of the Job Function \u00b6 The concept of the data placeholder Blob is emphasized above because the return value of the job function cannot be arbitrarily specified. It must be Blob type object or a container which only contains the Blob object. For example, the loss returned in the above code is a Blob object The return values of job function should be annotated. As an example, -> tp.Numpy in above code means the function returns a Blob object. As another example, we can annotate the return value type as -> Tuple[tp.Numpy, tp.Numpy] .It means the function returns a tuple which contains two Blob object You can refer to Get the result of the job function for specific examples. The Call of Job Function \u00b6 OneFlow uses decorator to convert Python function into OneFlow's job function. It is transparent to user. We can call the job function just like we call a Python function. Every time we call the job function, OneFlow will complete the forward propagation, back propagation, parameter updates, and more in framework. In the code below. When we get the data, we will pass parameters and call the train_job function to print loss . ( train_images , train_labels ), ( test_images , test_labels ) = flow . data . load_mnist ( BATCH_SIZE ) for epoch in range ( 3 ): for i , ( images , labels ) in enumerate ( zip ( train_images , train_labels )): loss = train_job ( images , labels ) if i % 20 == 0 : print ( loss . mean ()) As you can see, by calling the job function train_job , the numpy data is directly returned. The method shown above is synchronous. OneFlow also supports asynchronous invocation. For more details you can refer to the article Get the result of the job function .","title":"The Definition and Call of Job Function"},{"location":"single_client/extended_topics/job_function_define_call.html#the-definition-and-call-of-job-function","text":"In OneFlow, we encapsulate the training, inference and some other tasks into a \"job function\". The job function is used to connect the user's business logic and the computing resource managed by OneFlow. In OneFlow, the function decorated by @oneflow.global_function decorator is the OneFlow's job function We mainly define the structure of the model and choose the optimization target in job function. In addition, we can also pass some hyperparameters about training and some configuration of the environment to the job function (like the following example: get_train_config() ), OneFlow will manage the memory, GPU and other computing resource according to our configuration. In this article, we will specifically learn about: how to define and call the job function how to get the return value of job function","title":"The Definition and Call of Job Function"},{"location":"single_client/extended_topics/job_function_define_call.html#the-relationship-between-job-function-and-running-process-of-oneflow","text":"The job function is divided into two phases: definition and call. It's related to OneFlow's operating mechanism. Briefly, the OneFlow Python layer API simply describes the configuration and the training environment of the model. These information will pass to the C++ backend. After compilation, graph building and so on, the computation graph is obtained. Finally, the job function will be executed in OneFlow runtime. The job function describes the model and the training environment. In this phase there's no data. We can only define the shape and data type of the nodes (as known as PlaceHolder ) for creating and compiling the computation graph of OneFlow. The job function will be called after the OneFlow runtime starts. We can pass the data by calling job function and get the results. We will introduce the definition and calling method of job functions in detail as below.","title":"The Relationship Between Job Function and Running Process of OneFlow"},{"location":"single_client/extended_topics/job_function_define_call.html#the-definition-of-job-function","text":"We encapsulate the model in Python and use oneflow.global_function to decorate. Then the definition is completed. The job function mainly describes two things: The structure of model The optimizing target in training phase In the following code, we build a Multi-Layer Perceptron model and use flow.nn.sparse_softmax_cross_entropy_with_logits to compute the cross-entropy loss as our optimizing target. @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Callback [ tp . Numpy ]: # mlp initializer = flow . truncated_normal ( 0.1 ) reshape = flow . reshape ( images , [ images . shape [ 0 ], - 1 ]) hidden = flow . layers . dense ( reshape , 512 , activation = flow . nn . relu , kernel_initializer = initializer , name = \"hidden\" , ) logits = flow . layers . dense ( hidden , 10 , kernel_initializer = initializer , name = \"output\" ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , logits , name = \"softmax_loss\" ) lr_scheduler = flow . optimizer . PiecewiseConstantScheduler ([], [ 0.1 ]) flow . optimizer . SGD ( lr_scheduler , momentum = 0 ) . minimize ( loss ) return loss","title":"The Definition of Job Function"},{"location":"single_client/extended_topics/job_function_define_call.html#the-parameters-of-oneflowglobal_function","text":"oneflow.global_function decorator accepts two parameters, type and function_config . The parameter type accepts a string, which can only set as train or predict . When we define a training model, we set it as train . We set is as predict when we define a model for testing or inferencing. The parameter function_config accepts an object which is constructed by oneflow.function_config() . In function_config object, we can use its method or attribute to config. As the following code. def get_train_config (): config = flow . function_config () config . default_data_type ( flow . float ) return config We set the default data type, then, we can pass the function_config object to the global_function decorator. @flow . global_function ( type = \"train\" , function_config = get_train_config ()) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : For the complete code, you can refer to Consistent and Mirrored 's mixed_parallel_mlp.py","title":"The Parameters of oneflow.global_function"},{"location":"single_client/extended_topics/job_function_define_call.html#placeholder","text":"Noted that the images \u3001 logits \u3001 labels \u3001 loss and some other objects have no data in our definition of the job function. They are used to describe the shape and attribute of data , which is called PlaceHolder . For the parameters of the job function, we use Numpy.Placeholder , ListNumpy.Placeholder , ListListNumpy.Placeholder in the oneflow.typing package to annotate the data type of them as numpy.ndarray , Sequence[numpy.ndarray] and Sequence[Sequence[numpy.ndarray]] respectively. Besides the types of oneflow.typing , the variables returned from OneFlow operators or layers in the job function, like the reshape \u3001 hidden \u3001 logits \u3001 loss in the code above, are also PlaceHolder. All the variables mentioned above inherit the base class BlobDef directly or indirectly. We call this object type as Blob in OneFlow. The Blob has no data when defining the job function. It only plays the role of data placeholder for building the graph.","title":"PlaceHolder"},{"location":"single_client/extended_topics/job_function_define_call.html#the-return-value-of-the-job-function","text":"The concept of the data placeholder Blob is emphasized above because the return value of the job function cannot be arbitrarily specified. It must be Blob type object or a container which only contains the Blob object. For example, the loss returned in the above code is a Blob object The return values of job function should be annotated. As an example, -> tp.Numpy in above code means the function returns a Blob object. As another example, we can annotate the return value type as -> Tuple[tp.Numpy, tp.Numpy] .It means the function returns a tuple which contains two Blob object You can refer to Get the result of the job function for specific examples.","title":"The Return Value of the Job Function"},{"location":"single_client/extended_topics/job_function_define_call.html#the-call-of-job-function","text":"OneFlow uses decorator to convert Python function into OneFlow's job function. It is transparent to user. We can call the job function just like we call a Python function. Every time we call the job function, OneFlow will complete the forward propagation, back propagation, parameter updates, and more in framework. In the code below. When we get the data, we will pass parameters and call the train_job function to print loss . ( train_images , train_labels ), ( test_images , test_labels ) = flow . data . load_mnist ( BATCH_SIZE ) for epoch in range ( 3 ): for i , ( images , labels ) in enumerate ( zip ( train_images , train_labels )): loss = train_job ( images , labels ) if i % 20 == 0 : print ( loss . mean ()) As you can see, by calling the job function train_job , the numpy data is directly returned. The method shown above is synchronous. OneFlow also supports asynchronous invocation. For more details you can refer to the article Get the result of the job function .","title":"The Call of Job Function"},{"location":"single_client/extended_topics/model_mixed_parallel.html","text":"Features of Parallelism in OneFlow \u00b6 In Consistent and Mirrored view , we have already known OneFlow provides two types of view: mirrored and consistent view and we learned about the consistent view in OneFlow have some special features. Because in consistent_view , OneFlow provides a logically consistent view. During distributed training, users can freely choose to use data parallelism, model parallelism or hybrid parallelism. In this article, we will keep going through the special consistent view in OneFlow. We will learn about: Data parallelism in consistent_view flow chart. Hybrid parallelism in consistent_view flow chart. The advantages of hybrid parallelism and the applicable scenario. Example of hybrid parallelism. Network Logical Diagram in Model Training \u00b6 We need to set up a simple multi-layer network first and use this network to discuss parallelism methods. The structure like the figure shows: In each layer, we have samples (in grey), models (in blue) and operators (circles) which operating on both of them. To simplify our discussion, we can limit the sample and model as a matrix . The operator applying on them we call it matrix multiplication . Compare the figure above, we can easily get the logic of the network: The input of layer 0 is Data 0 matrix and Model 0 matrix. We apply op (matrix multiplication) and get output Data 1 . The input of layer 1 is Data 1 matrix and Model 1 matrix. We apply op and get output . The layer 2 is output layer and Data 2 is the output of network. Of course, it can play as input in a deeper network. In consistent view, OneFlow supports the data parallelism, model parallelism and hybrid parallelism. We will introduce them in order but hybrid parallelism is the key point. The Features of Parallelism in Consistent View \u00b6 Data Parallelism \u00b6 We have already known that in consistent view. The default parallelism method is data parallelism. If we choose mirrored view, we can only use data parallelism. If you pass numpy data directly when you call the job function (instead of using OneFlow's [DataLoader and related operators] (... /basics_topics/data_input.md#dataloader)), the difference between them are: In mirrored view, when we use data parallelism. We need to split and reorganize data according to the number of device and use list to pass and receive data. But in consistent view we have the consistency on logic. Splitting data and reorganizing data will be completed by OneFlow framework. The following figure is in consistent view, using data parallelism to achieve original logical network process: In data parallelism, we use two devices for training. As we use data parallelism , we can see that for each original logical layer, the sample is divided in average to each device. We have a complete training model in each device. The data after splitting are processed by op . Finally we combine the data in each device and get the complete data. Model parallelism \u00b6 In consistent view, we can choose model parallelism (the configuration details we will talk about it later). The flow diagram is as follows: In model parallelism example, we still use two devices for training. In each layer of original logic model is processed by op on part of model and complete data . Then they are combined and we get the complete results. One thing we need to mention is in above figure. The output from each device on layer 0 cannot use as the input in layer 1: Because in model parallelism, in order to complete the operation. We need partial model and complete data. To solve this problem, OneFlow use boxing mechanism. boxing will count the data in each node in distributed training and divide or assemble data properly then send to corresponding GPU. Besides the model assembling in model parallelism, boxing is also used for reverse gradient synchronization in data parallelism. The algorithm in boxing is complex. But it is transparent to users. The Illustration of boxing is just to prevent users from being confused. In this article, we only need to remember that OneFlow will automatically solve the data distribution issue. Choose the optimal parallelism method \u00b6 The difference between data parallelism and model parallelism is not constant. The sample, model size and model structure decide the performance in distributed training. We need to analyze the data to choose the optimal one. To be concluded: In data parallelism case, the information needed to be synchronized is gradient in backpropagation. Thus, we need to make sure that synchronization of information between different nodes is faster than calculation inside nodes. For example, the Convolution Layer has few parameters, but it needs large scale of calculation. Therefore, it is suitable for data parallelism. In model parallelism, we divide the logical model equally and send them to each device , which will solve the oversize model problem. Thus it is suitable for the neural network with massive parameters (like fully connected layer) to use model parallelism. In fact, we can use hybrid parallelism , it means OneFlow uses different parallelism in different parts of training process. For example, at the beginning of the neural network, it has few parameters and a lot of calculation, which makes it better to use data parallelism. For the layer with a lot of parameters, such as fully connected layer, we should use model parallelism. The following figure is the demonstration for the neural network which use hybrid parallelism . Currently, other popular frameworks either do not support mixed parallelism or require detailed customization. But in OneFlow, the hybrid parallelism distributed training can be configured through simple settings, and the distributed system can also be deeply optimized with the ultra-high degree of freedom pipelining mode. Hybrid Parallelism Example: \u00b6 Code \u00b6 In consistent view, we use hybrid parallelism to MLP model: the input layer and hidden layer use data parallelism, output layer use model parallelism. Complete Code: hybrid_parallelism_mlp.py More explanations can be seen in \"code explanations\" Code explanation \u00b6 The above code is modified from the demo in 3 min quick start . Compare two versions of code, we can see it is easy to configure the parallelism method in consistent_view with few codes. The crucial parts are: Use oneflow.config.gpu_device_num to set the device number in training: flow . config . gpu_device_num ( 2 ) reshape and hidden using data parallelism as default. The output layer can set model_distribute as flow.distribute.split(axis=0) to change to model parallelism: def mlp ( data ): initializer = flow . truncated_normal ( 0.1 ) reshape = flow . reshape ( data , [ data . shape [ 0 ], - 1 ]) hidden = flow . layers . dense ( reshape , 512 , activation = flow . nn . relu , kernel_initializer = initializer , name = \"dense1\" , ) return flow . layers . dense ( hidden , 10 , kernel_initializer = initializer , # dense for column storage with split(0) slicing. model_distribute = flow . distribute . split ( axis = 0 ), name = \"dense2\" , ) You may be curious about why split(axis=0) is column cutting. To be explained, dense is column-oriented storage in OneFlow. Thus the flow.distribute.split(axis=0) in above code is split by column. In addition, flow.layers.dense use model_distribute to set parallelism mode, it use the common get_variable to create blob in basic level from inner, and internally calls the more general interface get_variable to create blob . The get_variable interface uses a parameter named distribute to set the parallelism mode. As you can see, we can change the single machine training program to a distributed, hybrid parallel program with few modifications, which is one of the features that distinguishes OneFlow from other frameworks. Pipelining Example \u00b6 Besides the model parallelism, OneFlow also provides a more flexible parallelism method called pipelining, it allow user use scope.placement to specify the device of the operator. In pipelining, some parts of layers of the whole network are on one device and some are on other devices. They work consecutively as relay, switch between devices in different phases. In the following example, we change a few codes in \"Using consistent view in OneFlow\" of Consistent and Mirrored view and demonstrate pipelining. Code \u00b6 Complete Code: hybrid_parallelism_lenet.py Please refer to code explanation later for more details. Code Explanation \u00b6 There are only two important lines of code and they have similar effect: Use oneflow.scope.placement to specify the operator run on device 0 in hidden layer. with flow . scope . placement ( \"gpu\" , \"0:0\" ): hidden = flow . layers . dense ( reshape , 512 , activation = flow . nn . relu , kernel_initializer = initializer , name = \"hidden\" , Use oneflow.scope.placement to specify the operator in output layer run on device 1. with flow . scope . placement ( \"gpu\" , \"0:1\" ): output = flow . layers . dense ( hidden , 10 , kernel_initializer = initializer , name = \"outlayer\" ) More details of scope.placement can be found in the API documentation . Pipelining can allow user to specify which device to be used for each op. It is very useful for user who master the distributed training to optimize deeply . In addition, OneFlow also provides API oneflow.unpack , oneflow.pack . Combined with the own features of task scheduling in OneFlow, they make the pipelining easier to be used and more efficient. We will introduce them in other article.","title":"Features of Parallelism in OneFlow"},{"location":"single_client/extended_topics/model_mixed_parallel.html#features-of-parallelism-in-oneflow","text":"In Consistent and Mirrored view , we have already known OneFlow provides two types of view: mirrored and consistent view and we learned about the consistent view in OneFlow have some special features. Because in consistent_view , OneFlow provides a logically consistent view. During distributed training, users can freely choose to use data parallelism, model parallelism or hybrid parallelism. In this article, we will keep going through the special consistent view in OneFlow. We will learn about: Data parallelism in consistent_view flow chart. Hybrid parallelism in consistent_view flow chart. The advantages of hybrid parallelism and the applicable scenario. Example of hybrid parallelism.","title":"Features of Parallelism in OneFlow"},{"location":"single_client/extended_topics/model_mixed_parallel.html#network-logical-diagram-in-model-training","text":"We need to set up a simple multi-layer network first and use this network to discuss parallelism methods. The structure like the figure shows: In each layer, we have samples (in grey), models (in blue) and operators (circles) which operating on both of them. To simplify our discussion, we can limit the sample and model as a matrix . The operator applying on them we call it matrix multiplication . Compare the figure above, we can easily get the logic of the network: The input of layer 0 is Data 0 matrix and Model 0 matrix. We apply op (matrix multiplication) and get output Data 1 . The input of layer 1 is Data 1 matrix and Model 1 matrix. We apply op and get output . The layer 2 is output layer and Data 2 is the output of network. Of course, it can play as input in a deeper network. In consistent view, OneFlow supports the data parallelism, model parallelism and hybrid parallelism. We will introduce them in order but hybrid parallelism is the key point.","title":"Network Logical Diagram in Model Training"},{"location":"single_client/extended_topics/model_mixed_parallel.html#the-features-of-parallelism-in-consistent-view","text":"","title":"The Features of Parallelism in Consistent View"},{"location":"single_client/extended_topics/model_mixed_parallel.html#data-parallelism","text":"We have already known that in consistent view. The default parallelism method is data parallelism. If we choose mirrored view, we can only use data parallelism. If you pass numpy data directly when you call the job function (instead of using OneFlow's [DataLoader and related operators] (... /basics_topics/data_input.md#dataloader)), the difference between them are: In mirrored view, when we use data parallelism. We need to split and reorganize data according to the number of device and use list to pass and receive data. But in consistent view we have the consistency on logic. Splitting data and reorganizing data will be completed by OneFlow framework. The following figure is in consistent view, using data parallelism to achieve original logical network process: In data parallelism, we use two devices for training. As we use data parallelism , we can see that for each original logical layer, the sample is divided in average to each device. We have a complete training model in each device. The data after splitting are processed by op . Finally we combine the data in each device and get the complete data.","title":"Data Parallelism"},{"location":"single_client/extended_topics/model_mixed_parallel.html#model-parallelism","text":"In consistent view, we can choose model parallelism (the configuration details we will talk about it later). The flow diagram is as follows: In model parallelism example, we still use two devices for training. In each layer of original logic model is processed by op on part of model and complete data . Then they are combined and we get the complete results. One thing we need to mention is in above figure. The output from each device on layer 0 cannot use as the input in layer 1: Because in model parallelism, in order to complete the operation. We need partial model and complete data. To solve this problem, OneFlow use boxing mechanism. boxing will count the data in each node in distributed training and divide or assemble data properly then send to corresponding GPU. Besides the model assembling in model parallelism, boxing is also used for reverse gradient synchronization in data parallelism. The algorithm in boxing is complex. But it is transparent to users. The Illustration of boxing is just to prevent users from being confused. In this article, we only need to remember that OneFlow will automatically solve the data distribution issue.","title":"Model parallelism"},{"location":"single_client/extended_topics/model_mixed_parallel.html#choose-the-optimal-parallelism-method","text":"The difference between data parallelism and model parallelism is not constant. The sample, model size and model structure decide the performance in distributed training. We need to analyze the data to choose the optimal one. To be concluded: In data parallelism case, the information needed to be synchronized is gradient in backpropagation. Thus, we need to make sure that synchronization of information between different nodes is faster than calculation inside nodes. For example, the Convolution Layer has few parameters, but it needs large scale of calculation. Therefore, it is suitable for data parallelism. In model parallelism, we divide the logical model equally and send them to each device , which will solve the oversize model problem. Thus it is suitable for the neural network with massive parameters (like fully connected layer) to use model parallelism. In fact, we can use hybrid parallelism , it means OneFlow uses different parallelism in different parts of training process. For example, at the beginning of the neural network, it has few parameters and a lot of calculation, which makes it better to use data parallelism. For the layer with a lot of parameters, such as fully connected layer, we should use model parallelism. The following figure is the demonstration for the neural network which use hybrid parallelism . Currently, other popular frameworks either do not support mixed parallelism or require detailed customization. But in OneFlow, the hybrid parallelism distributed training can be configured through simple settings, and the distributed system can also be deeply optimized with the ultra-high degree of freedom pipelining mode.","title":"Choose the optimal parallelism method"},{"location":"single_client/extended_topics/model_mixed_parallel.html#hybrid-parallelism-example","text":"","title":"Hybrid Parallelism Example:"},{"location":"single_client/extended_topics/model_mixed_parallel.html#code","text":"In consistent view, we use hybrid parallelism to MLP model: the input layer and hidden layer use data parallelism, output layer use model parallelism. Complete Code: hybrid_parallelism_mlp.py More explanations can be seen in \"code explanations\"","title":"Code"},{"location":"single_client/extended_topics/model_mixed_parallel.html#code-explanation","text":"The above code is modified from the demo in 3 min quick start . Compare two versions of code, we can see it is easy to configure the parallelism method in consistent_view with few codes. The crucial parts are: Use oneflow.config.gpu_device_num to set the device number in training: flow . config . gpu_device_num ( 2 ) reshape and hidden using data parallelism as default. The output layer can set model_distribute as flow.distribute.split(axis=0) to change to model parallelism: def mlp ( data ): initializer = flow . truncated_normal ( 0.1 ) reshape = flow . reshape ( data , [ data . shape [ 0 ], - 1 ]) hidden = flow . layers . dense ( reshape , 512 , activation = flow . nn . relu , kernel_initializer = initializer , name = \"dense1\" , ) return flow . layers . dense ( hidden , 10 , kernel_initializer = initializer , # dense for column storage with split(0) slicing. model_distribute = flow . distribute . split ( axis = 0 ), name = \"dense2\" , ) You may be curious about why split(axis=0) is column cutting. To be explained, dense is column-oriented storage in OneFlow. Thus the flow.distribute.split(axis=0) in above code is split by column. In addition, flow.layers.dense use model_distribute to set parallelism mode, it use the common get_variable to create blob in basic level from inner, and internally calls the more general interface get_variable to create blob . The get_variable interface uses a parameter named distribute to set the parallelism mode. As you can see, we can change the single machine training program to a distributed, hybrid parallel program with few modifications, which is one of the features that distinguishes OneFlow from other frameworks.","title":"Code explanation"},{"location":"single_client/extended_topics/model_mixed_parallel.html#pipelining-example","text":"Besides the model parallelism, OneFlow also provides a more flexible parallelism method called pipelining, it allow user use scope.placement to specify the device of the operator. In pipelining, some parts of layers of the whole network are on one device and some are on other devices. They work consecutively as relay, switch between devices in different phases. In the following example, we change a few codes in \"Using consistent view in OneFlow\" of Consistent and Mirrored view and demonstrate pipelining.","title":"Pipelining Example"},{"location":"single_client/extended_topics/model_mixed_parallel.html#code_1","text":"Complete Code: hybrid_parallelism_lenet.py Please refer to code explanation later for more details.","title":"Code"},{"location":"single_client/extended_topics/model_mixed_parallel.html#code-explanation_1","text":"There are only two important lines of code and they have similar effect: Use oneflow.scope.placement to specify the operator run on device 0 in hidden layer. with flow . scope . placement ( \"gpu\" , \"0:0\" ): hidden = flow . layers . dense ( reshape , 512 , activation = flow . nn . relu , kernel_initializer = initializer , name = \"hidden\" , Use oneflow.scope.placement to specify the operator in output layer run on device 1. with flow . scope . placement ( \"gpu\" , \"0:1\" ): output = flow . layers . dense ( hidden , 10 , kernel_initializer = initializer , name = \"outlayer\" ) More details of scope.placement can be found in the API documentation . Pipelining can allow user to specify which device to be used for each op. It is very useful for user who master the distributed training to optimize deeply . In addition, OneFlow also provides API oneflow.unpack , oneflow.pack . Combined with the own features of task scheduling in OneFlow, they make the pipelining easier to be used and more efficient. We will introduce them in other article.","title":"Code Explanation"},{"location":"single_client/extended_topics/ofrecord.html","text":"Deep Learning applications need complex multi-stage data preprocessing pipeline, the first step of data pipeline is data loading. OneFlow supports multiple data formats in data loading, among which OFRecord format is the native data format of OneFlow. The data format definition of OFRecord is similar to TFRecord of Tensorflow. Users familiar with TFRecord can start with OneFlow's OFRecord quickly. Key points of this article\uff1a The data type used in OFRecord How to convert data to OFRecord object and serialize it The file format of OFRecord It should be helpful for users to learn how to make ofdataset after learning the above contents. Data Types of OFRecord \u00b6 Internally, OneFlow use Protocol Buffers to describe the serialization format of OFRecord. The related definitions can be found in the oneflow/core/record/record.proto file\uff1a syntax = \"proto2\"; package oneflow; message BytesList { repeated bytes value = 1; } message FloatList { repeated float value = 1 [packed = true]; } message DoubleList { repeated double value = 1 [packed = true]; } message Int32List { repeated int32 value = 1 [packed = true]; } message Int64List { repeated int64 value = 1 [packed = true]; } message Feature { oneof kind { BytesList bytes_list = 1; FloatList float_list = 2; DoubleList double_list = 3; Int32List int32_list = 4; Int64List int64_list = 5; } } message OFRecord { map<string, Feature> feature = 1; } Firstly let's explain the above important data types in details\uff1a OFRecord: the instantiated object of OFRecord, which can be used to store all data that need to be serialized. It is composed of many key-value pairs of string->Feature; Feature: can store one of the data type including BytesList, FloatList, DoubleList, Int32List, Int64List; The corresponding interfaces with the same name of OFRecord, Feature, XXXList and other data types will be generated by Protocol Buffers , making it possible to build corresponding objects at the Python level. Convert Data into Feature Format \u00b6 Users can convert data to Feature format with the invocation of ofrecord.xxxList and ofrecord.Feature . We also encapsulate the interface generated by protocol buffers to make it more convenient for users. import oneflow.core.record.record_pb2 as ofrecord def int32_feature ( value ): if not isinstance ( value , ( list , tuple )): value = [ value ] return ofrecord . Feature ( int32_list = ofrecord . Int32List ( value = value )) def int64_feature ( value ): if not isinstance ( value , ( list , tuple )): value = [ value ] return ofrecord . Feature ( int64_list = ofrecord . Int64List ( value = value )) def float_feature ( value ): if not isinstance ( value , ( list , tuple )): value = [ value ] return ofrecord . Feature ( float_list = ofrecord . FloatList ( value = value )) def double_feature ( value ): if not isinstance ( value , ( list , tuple )): value = [ value ] return ofrecord . Feature ( double_list = ofrecord . DoubleList ( value = value )) def bytes_feature ( value ): if not isinstance ( value , ( list , tuple )): value = [ value ] if not six . PY2 : if isinstance ( value [ 0 ], str ): value = [ x . encode () for x in value ] return ofrecord . Feature ( bytes_list = ofrecord . BytesList ( value = value )) Creating and Serializing OFRecord Object \u00b6 In the following example, we will create an OFRecord object which contains two features then serialize with its SerializeToString method obserations = 28 * 28 f = open ( \"./dataset/part-0\" , \"wb\" ) for loop in range ( 0 , 3 ): image = [ random . random () for x in range ( 0 , obserations )] label = [ random . randint ( 0 , 9 )] topack = { \"images\" : float_feature ( image ), \"labels\" : int64_feature ( label ), } ofrecord_features = ofrecord . OFRecord ( feature = topack ) serilizedBytes = ofrecord_features . SerializeToString () With the above example, we can summarize the steps for serializing data\uff1a First, users can convert data which needs to be serialized to a Feature object with the invocation of ofrecord.Feature and ofrecord.XXXList Second, store the Feature objects obtained in the previous step as string->Feature key-value format in Python dict Third, create OFRecord object with the invocation of ofrecord.OFRecord Last, get the serialized result of OFRecord object with its SerializeToString method The serialized result can be saved as a file with ofrecord format. OFRecord Format File \u00b6 According to the format convention of OneFlow, users can get OFRecord file after serializing the OFRecord object. Multiple OFRecord objects can be stored in one OFRecord file which can be used in OneFlow data-pipeline . The specific operations can be seen at how to make ofrecord dataset . According to the OneFlow convention, each OFRecord object is stored in the following format. uint64 length byte data[length] The length of the data are stored in the first eight bytes and then followed by the serialized data. length = ofrecord_features . ByteSize () f . write ( struct . pack ( \"q\" , length )) f . write ( serilizedBytes ) Code \u00b6 The following complete code shows how to generate an OFRecord file and then manually read datas by calling the OFRecord interface generated by protobuf . Actually, OneFlow provides flow.data.decode_ofrecord and other interfaces, which can more easily extract the contents of OFRecord files(dataset). See how to make ofrecord dataset for details. Write OFRecord Object to File \u00b6 In the following code, we randomly generate 3 samples and their corresponding labels, each sample is a 28*28 picture. After these three samples are converted into OFRecord objects, they are stored in the file according to the OneFlow convention format. Complete code\uff1a ofrecord_to_string.py Read data from OFRecord file \u00b6 The code below shows how to parse and read data from OFRecord file generated in the above example. Getting the OFRecord object by calling the FromString method to deserialize the file contents then display the data\uff1a The complete code\uff1a ofrecord_from_string.py","title":"The OFRecord Data Format"},{"location":"single_client/extended_topics/ofrecord.html#data-types-of-ofrecord","text":"Internally, OneFlow use Protocol Buffers to describe the serialization format of OFRecord. The related definitions can be found in the oneflow/core/record/record.proto file\uff1a syntax = \"proto2\"; package oneflow; message BytesList { repeated bytes value = 1; } message FloatList { repeated float value = 1 [packed = true]; } message DoubleList { repeated double value = 1 [packed = true]; } message Int32List { repeated int32 value = 1 [packed = true]; } message Int64List { repeated int64 value = 1 [packed = true]; } message Feature { oneof kind { BytesList bytes_list = 1; FloatList float_list = 2; DoubleList double_list = 3; Int32List int32_list = 4; Int64List int64_list = 5; } } message OFRecord { map<string, Feature> feature = 1; } Firstly let's explain the above important data types in details\uff1a OFRecord: the instantiated object of OFRecord, which can be used to store all data that need to be serialized. It is composed of many key-value pairs of string->Feature; Feature: can store one of the data type including BytesList, FloatList, DoubleList, Int32List, Int64List; The corresponding interfaces with the same name of OFRecord, Feature, XXXList and other data types will be generated by Protocol Buffers , making it possible to build corresponding objects at the Python level.","title":"Data Types of OFRecord"},{"location":"single_client/extended_topics/ofrecord.html#convert-data-into-feature-format","text":"Users can convert data to Feature format with the invocation of ofrecord.xxxList and ofrecord.Feature . We also encapsulate the interface generated by protocol buffers to make it more convenient for users. import oneflow.core.record.record_pb2 as ofrecord def int32_feature ( value ): if not isinstance ( value , ( list , tuple )): value = [ value ] return ofrecord . Feature ( int32_list = ofrecord . Int32List ( value = value )) def int64_feature ( value ): if not isinstance ( value , ( list , tuple )): value = [ value ] return ofrecord . Feature ( int64_list = ofrecord . Int64List ( value = value )) def float_feature ( value ): if not isinstance ( value , ( list , tuple )): value = [ value ] return ofrecord . Feature ( float_list = ofrecord . FloatList ( value = value )) def double_feature ( value ): if not isinstance ( value , ( list , tuple )): value = [ value ] return ofrecord . Feature ( double_list = ofrecord . DoubleList ( value = value )) def bytes_feature ( value ): if not isinstance ( value , ( list , tuple )): value = [ value ] if not six . PY2 : if isinstance ( value [ 0 ], str ): value = [ x . encode () for x in value ] return ofrecord . Feature ( bytes_list = ofrecord . BytesList ( value = value ))","title":"Convert Data into Feature Format"},{"location":"single_client/extended_topics/ofrecord.html#creating-and-serializing-ofrecord-object","text":"In the following example, we will create an OFRecord object which contains two features then serialize with its SerializeToString method obserations = 28 * 28 f = open ( \"./dataset/part-0\" , \"wb\" ) for loop in range ( 0 , 3 ): image = [ random . random () for x in range ( 0 , obserations )] label = [ random . randint ( 0 , 9 )] topack = { \"images\" : float_feature ( image ), \"labels\" : int64_feature ( label ), } ofrecord_features = ofrecord . OFRecord ( feature = topack ) serilizedBytes = ofrecord_features . SerializeToString () With the above example, we can summarize the steps for serializing data\uff1a First, users can convert data which needs to be serialized to a Feature object with the invocation of ofrecord.Feature and ofrecord.XXXList Second, store the Feature objects obtained in the previous step as string->Feature key-value format in Python dict Third, create OFRecord object with the invocation of ofrecord.OFRecord Last, get the serialized result of OFRecord object with its SerializeToString method The serialized result can be saved as a file with ofrecord format.","title":"Creating and Serializing OFRecord Object"},{"location":"single_client/extended_topics/ofrecord.html#ofrecord-format-file","text":"According to the format convention of OneFlow, users can get OFRecord file after serializing the OFRecord object. Multiple OFRecord objects can be stored in one OFRecord file which can be used in OneFlow data-pipeline . The specific operations can be seen at how to make ofrecord dataset . According to the OneFlow convention, each OFRecord object is stored in the following format. uint64 length byte data[length] The length of the data are stored in the first eight bytes and then followed by the serialized data. length = ofrecord_features . ByteSize () f . write ( struct . pack ( \"q\" , length )) f . write ( serilizedBytes )","title":"OFRecord Format File"},{"location":"single_client/extended_topics/ofrecord.html#code","text":"The following complete code shows how to generate an OFRecord file and then manually read datas by calling the OFRecord interface generated by protobuf . Actually, OneFlow provides flow.data.decode_ofrecord and other interfaces, which can more easily extract the contents of OFRecord files(dataset). See how to make ofrecord dataset for details.","title":"Code"},{"location":"single_client/extended_topics/ofrecord.html#write-ofrecord-object-to-file","text":"In the following code, we randomly generate 3 samples and their corresponding labels, each sample is a 28*28 picture. After these three samples are converted into OFRecord objects, they are stored in the file according to the OneFlow convention format. Complete code\uff1a ofrecord_to_string.py","title":"Write OFRecord Object to File"},{"location":"single_client/extended_topics/ofrecord.html#read-data-from-ofrecord-file","text":"The code below shows how to parse and read data from OFRecord file generated in the above example. Getting the OFRecord object by calling the FromString method to deserialize the file contents then display the data\uff1a The complete code\uff1a ofrecord_from_string.py","title":"Read data from OFRecord file"},{"location":"single_client/extended_topics/oneflow_convert_tools.html","text":"oneflow_convert_tools \u00b6 oneflow_onnx \u00b6 Introduction \u00b6 oneflow_onnx tool package includes two major functions: one is to export OneFlow out of ONNX, while the other is to transform ONNX models, which are obtained from other training frameworks, into Oneflow models. This tool package has already been adapted to TensorFlow/Pytorch/PaddlePaddle pre-trained models. The process of oneflow_onnx extracting ONNX and transforming it into OneFlow's format is called X2OneFlow (X representing TensorFlow/Pytorch/PaddlePaddle). OneFlow2ONNX models are supported. Specifically, OneFlow's lazy mode model can be transfomed into ONNX's format. Transformable OneFlow model can be obtained by using the method explained on flow.checkpoint.save . For more information, please refer to OneFlow2ONNX Model List . X2OneFlow models are supported. TensorFlow/Pytorch/PaddlePaddle model can be transformed into OneFlow's format through ONNX. OneFlow2ONNX operators are supported. Currently, oneflow_onnx is fully capable of exporting ONNX Opset10, and parts of OneFlow operator can transform ONNX Opsets that are in lower order. Please refer to OneFlow2ONNX Operator Lists for more information. X2OneFlow operators are supported. Currently, oneflow_onnx is fully capable of supporting most CV operators in TensorFlow/Pytorch/PaddlePaddle. Please refer to X2OneFlow Operator Lists for more information. Code generation is also supported. oneflow_onnx is able to generate OneFlow code and transforming models simultaneously . Please refer to X2OneFlow Code Generation List for more information. To sum up, OneFlow2ONNX can support over 80 ONNX OP X2OneFlow can support 80 ONNX OP, 50+ TensorFlow OP, 80+ Pytorch OP, and 50+ PaddlePaddle OP which covers most operations when doing CV model classifications. Since the OPs and models we support are all in eager mode API, users are required to install versions of PaddlePaddle >= 2.0.0, TensorFlow >= 2.0.0, and there is no specific requirements for Pytorch. Until now, X2OneFlow has successfully transformed 50+ official models from TensorFlow/Pytorch/PaddlePaddle, and you're always welcomed to experience our product. Environment Dependencies \u00b6 User's Environment Configuration \u00b6 python> = 3 .5 onnx> = 1 .8.0 onnx-simplifier> = 0 .3.3 onnxoptimizer> = 0 .2.5 onnxruntime> = 1 .6.0 oneflow ( https://github.com/Oneflow-Inc/oneflow#install-with-pip-package ) If you'd llike to use X2OneFlow, the following versions of deep learning frameworks are needed: pytorch> = 1 .7.0 paddlepaddle> = 2 .0.0 paddle2onnx> = 0 .6 tensorflow> = 2 .0.0 tf2onnx> = 1 .8.4 Installation \u00b6 Method 1 \u00b6 pip install oneflow_onn Method 2 git clone https://github.com/Oneflow-Inc/oneflow_convert_tools cd oneflow_onnx python3 setup.py install Usage \u00b6 Please refer to Examples Related Documents \u00b6 OneFlow2ONNX Model List X2OneFlow Model List OneFlow2ONNX Operator List X2OneFlow Operator List Examples nchw2nhwc_tool \u00b6 Introduction \u00b6 This tool is to transform NCHW, which is trained through OneFlow, into NHWC Format. Please click here for more information save_serving_tool \u00b6 Introduction \u00b6 This tool is to transform OneFlow models into models that can be used on the Serving end. Please click here for more information","title":"OneFlow And ONNX Convert"},{"location":"single_client/extended_topics/oneflow_convert_tools.html#oneflow_convert_tools","text":"","title":"oneflow_convert_tools"},{"location":"single_client/extended_topics/oneflow_convert_tools.html#oneflow_onnx","text":"","title":"oneflow_onnx"},{"location":"single_client/extended_topics/oneflow_convert_tools.html#introduction","text":"oneflow_onnx tool package includes two major functions: one is to export OneFlow out of ONNX, while the other is to transform ONNX models, which are obtained from other training frameworks, into Oneflow models. This tool package has already been adapted to TensorFlow/Pytorch/PaddlePaddle pre-trained models. The process of oneflow_onnx extracting ONNX and transforming it into OneFlow's format is called X2OneFlow (X representing TensorFlow/Pytorch/PaddlePaddle). OneFlow2ONNX models are supported. Specifically, OneFlow's lazy mode model can be transfomed into ONNX's format. Transformable OneFlow model can be obtained by using the method explained on flow.checkpoint.save . For more information, please refer to OneFlow2ONNX Model List . X2OneFlow models are supported. TensorFlow/Pytorch/PaddlePaddle model can be transformed into OneFlow's format through ONNX. OneFlow2ONNX operators are supported. Currently, oneflow_onnx is fully capable of exporting ONNX Opset10, and parts of OneFlow operator can transform ONNX Opsets that are in lower order. Please refer to OneFlow2ONNX Operator Lists for more information. X2OneFlow operators are supported. Currently, oneflow_onnx is fully capable of supporting most CV operators in TensorFlow/Pytorch/PaddlePaddle. Please refer to X2OneFlow Operator Lists for more information. Code generation is also supported. oneflow_onnx is able to generate OneFlow code and transforming models simultaneously . Please refer to X2OneFlow Code Generation List for more information. To sum up, OneFlow2ONNX can support over 80 ONNX OP X2OneFlow can support 80 ONNX OP, 50+ TensorFlow OP, 80+ Pytorch OP, and 50+ PaddlePaddle OP which covers most operations when doing CV model classifications. Since the OPs and models we support are all in eager mode API, users are required to install versions of PaddlePaddle >= 2.0.0, TensorFlow >= 2.0.0, and there is no specific requirements for Pytorch. Until now, X2OneFlow has successfully transformed 50+ official models from TensorFlow/Pytorch/PaddlePaddle, and you're always welcomed to experience our product.","title":"Introduction"},{"location":"single_client/extended_topics/oneflow_convert_tools.html#environment-dependencies","text":"","title":"Environment Dependencies"},{"location":"single_client/extended_topics/oneflow_convert_tools.html#users-environment-configuration","text":"python> = 3 .5 onnx> = 1 .8.0 onnx-simplifier> = 0 .3.3 onnxoptimizer> = 0 .2.5 onnxruntime> = 1 .6.0 oneflow ( https://github.com/Oneflow-Inc/oneflow#install-with-pip-package ) If you'd llike to use X2OneFlow, the following versions of deep learning frameworks are needed: pytorch> = 1 .7.0 paddlepaddle> = 2 .0.0 paddle2onnx> = 0 .6 tensorflow> = 2 .0.0 tf2onnx> = 1 .8.4","title":"User's Environment Configuration"},{"location":"single_client/extended_topics/oneflow_convert_tools.html#installation","text":"","title":"Installation"},{"location":"single_client/extended_topics/oneflow_convert_tools.html#method-1","text":"pip install oneflow_onn Method 2 git clone https://github.com/Oneflow-Inc/oneflow_convert_tools cd oneflow_onnx python3 setup.py install","title":"Method 1"},{"location":"single_client/extended_topics/oneflow_convert_tools.html#usage","text":"Please refer to Examples","title":"Usage"},{"location":"single_client/extended_topics/oneflow_convert_tools.html#related-documents","text":"OneFlow2ONNX Model List X2OneFlow Model List OneFlow2ONNX Operator List X2OneFlow Operator List Examples","title":"Related Documents"},{"location":"single_client/extended_topics/oneflow_convert_tools.html#nchw2nhwc_tool","text":"","title":"nchw2nhwc_tool"},{"location":"single_client/extended_topics/oneflow_convert_tools.html#introduction_1","text":"This tool is to transform NCHW, which is trained through OneFlow, into NHWC Format. Please click here for more information","title":"Introduction"},{"location":"single_client/extended_topics/oneflow_convert_tools.html#save_serving_tool","text":"","title":"save_serving_tool"},{"location":"single_client/extended_topics/oneflow_convert_tools.html#introduction_2","text":"This tool is to transform OneFlow models into models that can be used on the Serving end. Please click here for more information","title":"Introduction"},{"location":"single_client/extended_topics/watch_watch_diff.html","text":"How to Obtain Runtime Data \u00b6 OneFlow support oneflow.watch and oneflow.watch_diff . We can use them to register a callback function to get data and gradient tensor in job functions at runtime. Using Guidance \u00b6 To get data or gradient tensor in job function, we need to follow these steps: Write a callback function and the parameters of the callback function should be annotated to indicate the data type. The logic of the callback function need to be set up by user themselves. When defining the job functions, we use oneflow.watch or oneflow.watch_diff to register callback function. We obtain data tensor from the former one and their corresponding gradient from the latter one. At the appropriate time when the job function is running, OneFlow will call the previous callback function which was registered earlier and pass the monitored data to the callback function then execute the logic in the callback function. Take oneflow.watch as example: def my_watch ( x : T ): #process x @global_function () def foo () -> T : #define network ... oneflow . watch ( x , my_watch ) #... The T in the code above is the data type in oneflow.typing . Like oneflow.typing.Numpy . Please refer to this article . We will use the following examples to demonstrate how to use watch and watch_diff . Use watch to Obtain the Data when Running \u00b6 The following is an example to demonstrate how to use oneflow.watch to obtain the data from middle layer in OneFlow. Code: test_watch.py Run above code: python3 test_watch.py We can get results like the followings: in: [ 0.15727027 0.45887455 0.10939325 0.66666406 -0.62354755] out: [0.15727027 0.45887455 0.10939325 0.66666406 0. ] Code Explanation \u00b6 In the example, we focus on y in ReluJob . Thus, we call flow.watch(y, watch_handler) to monitor y . The function oneflow.watch needs two parameters: The first parameter is y which we focus on. The second parameter is a callback function. When OneFlow use device resources to execute ReluJob , it will send y as a parameter to callback function. We define our callback function watch_handler to print out its parameters. User can use customized callback function to process the data from OneFlow according to their own requirements. Use watch_diff to Obtain Gradient when Running \u00b6 The following is an example to demonstrate how to use oneflow.watch_diff to obtain the gradient at runtime. Code: test_watch_diff.py Run above code: python3 test_watch.py We should have the following results: [ ... [ 1.39966095e-03 3.49164731e-03 3.31605263e-02 4.50417027e-03 7.73609674e-04 4.89911772e-02 2.47627571e-02 7.65468649e-05 -1.18361652e-01 1.20161276e-03]] (100, 10) float32 Code Explanation \u00b6 In the example above, we use oneflow.watch_diff to obtain the gradient. The processe is the same as the example which using oneflow.watch to obtain data tensor. First, we define the callback function: def watch_diff_handler ( blob : tp . Numpy ): print ( \"watch_diff_handler:\" , blob , blob . shape , blob . dtype ) Then we use oneflow.watch_diff to register the callback function in job function: flow . watch_diff ( logits , watch_diff_handler ) When running, OneFlow framework will call watch_diff_handler and send the gradient corresponding with logits to watch_diff_handler .","title":"Obtain Runtime Data"},{"location":"single_client/extended_topics/watch_watch_diff.html#how-to-obtain-runtime-data","text":"OneFlow support oneflow.watch and oneflow.watch_diff . We can use them to register a callback function to get data and gradient tensor in job functions at runtime.","title":"How to Obtain Runtime Data"},{"location":"single_client/extended_topics/watch_watch_diff.html#using-guidance","text":"To get data or gradient tensor in job function, we need to follow these steps: Write a callback function and the parameters of the callback function should be annotated to indicate the data type. The logic of the callback function need to be set up by user themselves. When defining the job functions, we use oneflow.watch or oneflow.watch_diff to register callback function. We obtain data tensor from the former one and their corresponding gradient from the latter one. At the appropriate time when the job function is running, OneFlow will call the previous callback function which was registered earlier and pass the monitored data to the callback function then execute the logic in the callback function. Take oneflow.watch as example: def my_watch ( x : T ): #process x @global_function () def foo () -> T : #define network ... oneflow . watch ( x , my_watch ) #... The T in the code above is the data type in oneflow.typing . Like oneflow.typing.Numpy . Please refer to this article . We will use the following examples to demonstrate how to use watch and watch_diff .","title":"Using Guidance"},{"location":"single_client/extended_topics/watch_watch_diff.html#use-watch-to-obtain-the-data-when-running","text":"The following is an example to demonstrate how to use oneflow.watch to obtain the data from middle layer in OneFlow. Code: test_watch.py Run above code: python3 test_watch.py We can get results like the followings: in: [ 0.15727027 0.45887455 0.10939325 0.66666406 -0.62354755] out: [0.15727027 0.45887455 0.10939325 0.66666406 0. ]","title":"Use watch to Obtain the Data when Running"},{"location":"single_client/extended_topics/watch_watch_diff.html#code-explanation","text":"In the example, we focus on y in ReluJob . Thus, we call flow.watch(y, watch_handler) to monitor y . The function oneflow.watch needs two parameters: The first parameter is y which we focus on. The second parameter is a callback function. When OneFlow use device resources to execute ReluJob , it will send y as a parameter to callback function. We define our callback function watch_handler to print out its parameters. User can use customized callback function to process the data from OneFlow according to their own requirements.","title":"Code Explanation"},{"location":"single_client/extended_topics/watch_watch_diff.html#use-watch_diff-to-obtain-gradient-when-running","text":"The following is an example to demonstrate how to use oneflow.watch_diff to obtain the gradient at runtime. Code: test_watch_diff.py Run above code: python3 test_watch.py We should have the following results: [ ... [ 1.39966095e-03 3.49164731e-03 3.31605263e-02 4.50417027e-03 7.73609674e-04 4.89911772e-02 2.47627571e-02 7.65468649e-05 -1.18361652e-01 1.20161276e-03]] (100, 10) float32","title":"Use watch_diff to Obtain Gradient when Running"},{"location":"single_client/extended_topics/watch_watch_diff.html#code-explanation_1","text":"In the example above, we use oneflow.watch_diff to obtain the gradient. The processe is the same as the example which using oneflow.watch to obtain data tensor. First, we define the callback function: def watch_diff_handler ( blob : tp . Numpy ): print ( \"watch_diff_handler:\" , blob , blob . shape , blob . dtype ) Then we use oneflow.watch_diff to register the callback function in job function: flow . watch_diff ( logits , watch_diff_handler ) When running, OneFlow framework will call watch_diff_handler and send the gradient corresponding with logits to watch_diff_handler .","title":"Code Explanation"},{"location":"single_client/quick_start/introduce.html","text":"Compatible Interface \u00b6 This topic is only reserved for compatibility with the old interface. If you haven't used 'v0.4.0' or earlier versions, please ignore all the contents of this topic directly . By version 0.4.0 and earlier, the interface of OneFlow was non object-oriented. Now OneFlow provides object-oriented interfaces. In order to take care of the code using the old interface, OneFlow moves the old interface to oneflow.compatible.single_client . That code only need to replace the import statements: import oneflow as flow import oneflow.typing as tp with: from oneflow.compatible import single_client as flow from oneflow.compatible.single_client import typing as tp","title":"Introduce"},{"location":"single_client/quick_start/introduce.html#compatible-interface","text":"This topic is only reserved for compatibility with the old interface. If you haven't used 'v0.4.0' or earlier versions, please ignore all the contents of this topic directly . By version 0.4.0 and earlier, the interface of OneFlow was non object-oriented. Now OneFlow provides object-oriented interfaces. In order to take care of the code using the old interface, OneFlow moves the old interface to oneflow.compatible.single_client . That code only need to replace the import statements: import oneflow as flow import oneflow.typing as tp with: from oneflow.compatible import single_client as flow from oneflow.compatible.single_client import typing as tp","title":"Compatible Interface"},{"location":"single_client/quick_start/lenet_mnist.html","text":"This article covers topics below: Configuring the hardware and software environment using the OneFlow interface Define models using OneFlow's interface Model training with train type How to save/load model Use the predict type for model evaluation Using predict type for image recognition This article demonstrates the key steps of how to train a LeNet model with MNIST dataset using OneFlow. The full example code is attached at the end of article. You can see the effects of each script by running the following commands (The script operation rely on the default GPU No.0 on your machine. If you install the CPU version of OneFlow, the script will automatically call the CPU for training/evaluation). First of all, clone the documentation repository and switch to the corresponding path: git clone https://github.com/Oneflow-Inc/oneflow-documentation.git cd oneflow-documentation/en/docs/code/quick_start/ Training model python lenet_train.py The commands above will train a model with MNIST dataset and save it. Output\uff1a File mnist.npz already exist, path: ./mnist.npz 5.9947124 1.0865117 0.5317516 0.20937675 0.26428983 0.21764673 0.23443426 ... A trained model is the prerequisite of lenet_eval.py and lenet_test.py . We can directly download a trained model to skip the training progress: #change directory to: en/docs/code/quick_start/ wget https://oneflow-public.oss-cn-beijing.aliyuncs.com/online_document/docs/quick_start/lenet_models_1.zip unzip lenet_models_1.zip Evaluation python lenet_eval.py The command above uses the MNIST's testing set to evaluate the trained model and print out the accuracy. Output\uff1a File mnist.npz already exist, path: ./mnist.npz accuracy: 99.4% Image recognition python lenet_test.py ./9.png # Output\uff1aprediction: 9 The above command will use the trained model to predict the content of file \"9.png\". We can also download and verify more from prepared images . Introduction of MNIST Dataset \u00b6 MNIST is a handwritten digits database including training set and testing set. Training set includes 60000 pictures and their corresponding label. Yann LeCun and others have normalized all the images and packed them into a single binary file for downloading. http://yann.lecun.com/exdb/mnist/ Define Training Model \u00b6 Modules oneflow.nn and oneflow.layers provide the operators to construct the model. def lenet ( data , train = False ): initializer = flow . truncated_normal ( 0.1 ) conv1 = flow . layers . conv2d ( data , 32 , 5 , padding = \"SAME\" , activation = flow . nn . relu , name = \"conv1\" , kernel_initializer = initializer , ) pool1 = flow . nn . max_pool2d ( conv1 , ksize = 2 , strides = 2 , padding = \"SAME\" , name = \"pool1\" , data_format = \"NCHW\" ) conv2 = flow . layers . conv2d ( pool1 , 64 , 5 , padding = \"SAME\" , activation = flow . nn . relu , name = \"conv2\" , kernel_initializer = initializer , ) pool2 = flow . nn . max_pool2d ( conv2 , ksize = 2 , strides = 2 , padding = \"SAME\" , name = \"pool2\" , data_format = \"NCHW\" ) reshape = flow . reshape ( pool2 , [ pool2 . shape [ 0 ], - 1 ]) hidden = flow . layers . dense ( reshape , 512 , activation = flow . nn . relu , kernel_initializer = initializer , name = \"dense1\" , ) if train : hidden = flow . nn . dropout ( hidden , rate = 0.5 , name = \"dropout\" ) return flow . layers . dense ( hidden , 10 , kernel_initializer = initializer , name = \"dense2\" ) As the code showing above, we build up a LeNet network model. Implementation of Job Function for Training \u00b6 OneFlow provides a decorator named oneflow.global_function by which we can covert a Python function to a OneFlow Job Function . global_function Decorator \u00b6 oneflow.function_config decorator takes a type parameter to specify the type of job function. The type=\"tranining\" means that the job function is for traning and type=\"predict\" is for predicting. There is also a function_config parameter taken by oneflow.global_function decorator. The function_config contains configuration about training. @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 )) -> tp . Numpy : # Implementation of netwrok ... The tp.Numpy.Placeholder is a placeholder. The annotation tp.Numpy on return type means that the job function will return a numpy object. Setup Optimizer \u00b6 We can use oneflow.optimizer to specify the parameters needed by optimization. By this way, in the process of each iteration during training, OneFlow will take the specified object as optimization goal. @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : with flow . scope . placement ( \"gpu\" , \"0:0\" ): logits = lenet ( images , train = True ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , logits , name = \"softmax_loss\" ) lr_scheduler = flow . optimizer . PiecewiseConstantScheduler ([], [ 0.1 ]) flow . optimizer . SGD ( lr_scheduler , momentum = 0 ) . minimize ( loss ) return loss So Far, we use flow.nn.sparse_softmax_cross_entropy_with_logits to calculate the loss and specify it as optimization goal. lr_scheduler sets the learning rate schedule, and [0.1] means learning rate is 0.1. flow.optimizer.SGD means SGD is specified as the optimizer. The loss is the goal of minimization to the optimizer and the return type (not requried). Calling the Job Function and Get Results \u00b6 We can start training by invoking the job function. The return value we get when we call the job function is defined by the annotation of return value type in job function. We can get one or multiple results after each call of job function. Example on Single Return Value \u00b6 The job function in lenet_train.py : @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : with flow . scope . placement ( \"gpu\" , \"0:0\" ): logits = lenet ( images , train = True ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , logits , name = \"softmax_loss\" ) lr_scheduler = flow . optimizer . PiecewiseConstantScheduler ([], [ 0.1 ]) flow . optimizer . SGD ( lr_scheduler , momentum = 0 ) . minimize ( loss ) return loss The return value in job function is a tp.Numpy . When calling job function, we will get a numpy object: for epoch in range ( 20 ): for i , ( images , labels ) in enumerate ( zip ( train_images , train_labels )): loss = train_job ( images , labels ) if i % 20 == 0 : print ( loss . mean ()) We call the train_job and print the loss every 20 iterations. Example on Multiple Return Values \u00b6 In script lenet_eval.py , we define the job function below: @flow . global_function ( type = \"predict\" ) def eval_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> Tuple [ tp . Numpy , tp . Numpy ]: with flow . scope . placement ( \"gpu\" , \"0:0\" ): logits = lenet ( images , train = False ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , logits , name = \"softmax_loss\" ) return ( labels , logits ) The return value type of this job function is Tuple[tp.Numpy, tp.Numpy] . When we call the job function, we will get a tuple container. There are two numpy objects in it: for i , ( images , labels ) in enumerate ( zip ( test_images , test_labels )): labels , logits = eval_job ( images , labels ) acc ( labels , logits ) We call the job function and get labels and logits then use them to evaluate the model. Synchronous and Asynchronous Call \u00b6 All code in this article only call synchronously to get results from job function. In fact, OneFlow can call job function asynchronously. For more details, please refer to Obtain results from job function . Model Initialization, Saving and Loading \u00b6 Model Initialization and Saving \u00b6 The example of model saved by the flow.checkpoint.save : if __name__ == '__main__' : #data loading and training ... flow . checkpoint . save ( \"./lenet_models_1\" ) When the model is saved, we will get a folder called \"lenet_models_1\". This folder contains directories and files corresponding with the model parameters. Model Loading \u00b6 During the prediction process, we can load the parameter from the file to memory by flow.checkpoint.get and then update the parameter to the model by flow.load_variables . For example: if __name__ == '__main__' : flow . load_variables ( flow . checkpoint . get ( \"./lenet_models_1\" )) #evaluation process ... Evaluation of Model \u00b6 The job function for evaluation is basically same as job function for training. The small difference is that the model we use is already saved in evaluation process. Thus, initialization and update of model during iteration are not needed. Job Function for Evaluation \u00b6 @flow . global_function ( type = \"predict\" ) def eval_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> Tuple [ tp . Numpy , tp . Numpy ]: with flow . scope . placement ( \"gpu\" , \"0:0\" ): logits = lenet ( images , train = False ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , logits , name = \"softmax_loss\" ) return ( labels , logits ) Code above is the implementation of job function for evaluation and its return type is declared as Tuple[tp.Numpy, tp.Numpy] . Tuple have two numpy in it. We will call the job function and calculate the accuracy according to the return values. Process of Evaluation \u00b6 The acc function is used to count the total number of samples and the number of correct prediction results. We will call the job function to get paramters labels and logits : g_total = 0 g_correct = 0 def acc ( labels , logits ): global g_total global g_correct predictions = np . argmax ( logits , 1 ) right_count = np . sum ( predictions == labels ) g_total += labels . shape [ 0 ] g_correct += right_count Call the job function for evaluation: if __name__ == \"__main__\" : flow . load_variables ( flow . checkpoint . get ( \"./lenet_models_1\" )) ( train_images , train_labels ), ( test_images , test_labels ) = flow . data . load_mnist ( BATCH_SIZE , BATCH_SIZE ) for epoch in range ( 1 ): for i , ( images , labels ) in enumerate ( zip ( test_images , test_labels )): labels , logits = eval_job ( images , labels ) acc ( labels , logits ) print ( \"accuracy: {0:.1f} %\" . format ( g_correct * 100 / g_total )) So far, we call the job function for evaluation looply and print the accuracy of evaluation result on MNIST testing set. Image Prediction \u00b6 After making a few changes to the code above, it will take the data from the raw images rather than existing dataset. Then we can get a model to predict the content from the images. def load_image ( file ): im = Image . open ( file ) . convert ( \"L\" ) im = im . resize (( 28 , 28 ), Image . ANTIALIAS ) im = np . array ( im ) . reshape ( 1 , 1 , 28 , 28 ) . astype ( np . float32 ) im = ( im - 128.0 ) / 255.0 im . reshape (( - 1 , 1 , 1 , im . shape [ 1 ], im . shape [ 2 ])) return im def main (): if len ( sys . argv ) != 2 : usage () return flow . load_variables ( flow . checkpoint . get ( \"./lenet_models_1\" )) image = load_image ( sys . argv [ 1 ]) logits = test_job ( image ) prediction = np . argmax ( logits , 1 ) print ( \"prediction: {} \" . format ( prediction [ 0 ])) if __name__ == \"__main__\" : main () Code \u00b6 Model training \u00b6 Script: lenet_train.py Model evaluation \u00b6 Script: lenet_eval.py Saved model: lenet_models_1.zip Digits prediction \u00b6 Script: lenet_test.py Saved model: lenet_models_1.zip MNIST image dataset mnist_raw_images.zip","title":"Recognition of MNIST Handwritten Digits"},{"location":"single_client/quick_start/lenet_mnist.html#introduction-of-mnist-dataset","text":"MNIST is a handwritten digits database including training set and testing set. Training set includes 60000 pictures and their corresponding label. Yann LeCun and others have normalized all the images and packed them into a single binary file for downloading. http://yann.lecun.com/exdb/mnist/","title":"Introduction of MNIST Dataset"},{"location":"single_client/quick_start/lenet_mnist.html#define-training-model","text":"Modules oneflow.nn and oneflow.layers provide the operators to construct the model. def lenet ( data , train = False ): initializer = flow . truncated_normal ( 0.1 ) conv1 = flow . layers . conv2d ( data , 32 , 5 , padding = \"SAME\" , activation = flow . nn . relu , name = \"conv1\" , kernel_initializer = initializer , ) pool1 = flow . nn . max_pool2d ( conv1 , ksize = 2 , strides = 2 , padding = \"SAME\" , name = \"pool1\" , data_format = \"NCHW\" ) conv2 = flow . layers . conv2d ( pool1 , 64 , 5 , padding = \"SAME\" , activation = flow . nn . relu , name = \"conv2\" , kernel_initializer = initializer , ) pool2 = flow . nn . max_pool2d ( conv2 , ksize = 2 , strides = 2 , padding = \"SAME\" , name = \"pool2\" , data_format = \"NCHW\" ) reshape = flow . reshape ( pool2 , [ pool2 . shape [ 0 ], - 1 ]) hidden = flow . layers . dense ( reshape , 512 , activation = flow . nn . relu , kernel_initializer = initializer , name = \"dense1\" , ) if train : hidden = flow . nn . dropout ( hidden , rate = 0.5 , name = \"dropout\" ) return flow . layers . dense ( hidden , 10 , kernel_initializer = initializer , name = \"dense2\" ) As the code showing above, we build up a LeNet network model.","title":"Define Training Model"},{"location":"single_client/quick_start/lenet_mnist.html#implementation-of-job-function-for-training","text":"OneFlow provides a decorator named oneflow.global_function by which we can covert a Python function to a OneFlow Job Function .","title":"Implementation of Job Function for Training"},{"location":"single_client/quick_start/lenet_mnist.html#global_function-decorator","text":"oneflow.function_config decorator takes a type parameter to specify the type of job function. The type=\"tranining\" means that the job function is for traning and type=\"predict\" is for predicting. There is also a function_config parameter taken by oneflow.global_function decorator. The function_config contains configuration about training. @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 )) -> tp . Numpy : # Implementation of netwrok ... The tp.Numpy.Placeholder is a placeholder. The annotation tp.Numpy on return type means that the job function will return a numpy object.","title":"global_function Decorator"},{"location":"single_client/quick_start/lenet_mnist.html#setup-optimizer","text":"We can use oneflow.optimizer to specify the parameters needed by optimization. By this way, in the process of each iteration during training, OneFlow will take the specified object as optimization goal. @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : with flow . scope . placement ( \"gpu\" , \"0:0\" ): logits = lenet ( images , train = True ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , logits , name = \"softmax_loss\" ) lr_scheduler = flow . optimizer . PiecewiseConstantScheduler ([], [ 0.1 ]) flow . optimizer . SGD ( lr_scheduler , momentum = 0 ) . minimize ( loss ) return loss So Far, we use flow.nn.sparse_softmax_cross_entropy_with_logits to calculate the loss and specify it as optimization goal. lr_scheduler sets the learning rate schedule, and [0.1] means learning rate is 0.1. flow.optimizer.SGD means SGD is specified as the optimizer. The loss is the goal of minimization to the optimizer and the return type (not requried).","title":"Setup Optimizer"},{"location":"single_client/quick_start/lenet_mnist.html#calling-the-job-function-and-get-results","text":"We can start training by invoking the job function. The return value we get when we call the job function is defined by the annotation of return value type in job function. We can get one or multiple results after each call of job function.","title":"Calling the Job Function and Get Results"},{"location":"single_client/quick_start/lenet_mnist.html#example-on-single-return-value","text":"The job function in lenet_train.py : @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : with flow . scope . placement ( \"gpu\" , \"0:0\" ): logits = lenet ( images , train = True ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , logits , name = \"softmax_loss\" ) lr_scheduler = flow . optimizer . PiecewiseConstantScheduler ([], [ 0.1 ]) flow . optimizer . SGD ( lr_scheduler , momentum = 0 ) . minimize ( loss ) return loss The return value in job function is a tp.Numpy . When calling job function, we will get a numpy object: for epoch in range ( 20 ): for i , ( images , labels ) in enumerate ( zip ( train_images , train_labels )): loss = train_job ( images , labels ) if i % 20 == 0 : print ( loss . mean ()) We call the train_job and print the loss every 20 iterations.","title":"Example on Single Return Value"},{"location":"single_client/quick_start/lenet_mnist.html#example-on-multiple-return-values","text":"In script lenet_eval.py , we define the job function below: @flow . global_function ( type = \"predict\" ) def eval_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> Tuple [ tp . Numpy , tp . Numpy ]: with flow . scope . placement ( \"gpu\" , \"0:0\" ): logits = lenet ( images , train = False ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , logits , name = \"softmax_loss\" ) return ( labels , logits ) The return value type of this job function is Tuple[tp.Numpy, tp.Numpy] . When we call the job function, we will get a tuple container. There are two numpy objects in it: for i , ( images , labels ) in enumerate ( zip ( test_images , test_labels )): labels , logits = eval_job ( images , labels ) acc ( labels , logits ) We call the job function and get labels and logits then use them to evaluate the model.","title":"Example on Multiple Return Values"},{"location":"single_client/quick_start/lenet_mnist.html#synchronous-and-asynchronous-call","text":"All code in this article only call synchronously to get results from job function. In fact, OneFlow can call job function asynchronously. For more details, please refer to Obtain results from job function .","title":"Synchronous and Asynchronous Call"},{"location":"single_client/quick_start/lenet_mnist.html#model-initialization-saving-and-loading","text":"","title":"Model Initialization, Saving and Loading"},{"location":"single_client/quick_start/lenet_mnist.html#model-initialization-and-saving","text":"The example of model saved by the flow.checkpoint.save : if __name__ == '__main__' : #data loading and training ... flow . checkpoint . save ( \"./lenet_models_1\" ) When the model is saved, we will get a folder called \"lenet_models_1\". This folder contains directories and files corresponding with the model parameters.","title":"Model Initialization and Saving"},{"location":"single_client/quick_start/lenet_mnist.html#model-loading","text":"During the prediction process, we can load the parameter from the file to memory by flow.checkpoint.get and then update the parameter to the model by flow.load_variables . For example: if __name__ == '__main__' : flow . load_variables ( flow . checkpoint . get ( \"./lenet_models_1\" )) #evaluation process ...","title":"Model Loading"},{"location":"single_client/quick_start/lenet_mnist.html#evaluation-of-model","text":"The job function for evaluation is basically same as job function for training. The small difference is that the model we use is already saved in evaluation process. Thus, initialization and update of model during iteration are not needed.","title":"Evaluation of Model"},{"location":"single_client/quick_start/lenet_mnist.html#job-function-for-evaluation","text":"@flow . global_function ( type = \"predict\" ) def eval_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> Tuple [ tp . Numpy , tp . Numpy ]: with flow . scope . placement ( \"gpu\" , \"0:0\" ): logits = lenet ( images , train = False ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , logits , name = \"softmax_loss\" ) return ( labels , logits ) Code above is the implementation of job function for evaluation and its return type is declared as Tuple[tp.Numpy, tp.Numpy] . Tuple have two numpy in it. We will call the job function and calculate the accuracy according to the return values.","title":"Job Function for Evaluation"},{"location":"single_client/quick_start/lenet_mnist.html#process-of-evaluation","text":"The acc function is used to count the total number of samples and the number of correct prediction results. We will call the job function to get paramters labels and logits : g_total = 0 g_correct = 0 def acc ( labels , logits ): global g_total global g_correct predictions = np . argmax ( logits , 1 ) right_count = np . sum ( predictions == labels ) g_total += labels . shape [ 0 ] g_correct += right_count Call the job function for evaluation: if __name__ == \"__main__\" : flow . load_variables ( flow . checkpoint . get ( \"./lenet_models_1\" )) ( train_images , train_labels ), ( test_images , test_labels ) = flow . data . load_mnist ( BATCH_SIZE , BATCH_SIZE ) for epoch in range ( 1 ): for i , ( images , labels ) in enumerate ( zip ( test_images , test_labels )): labels , logits = eval_job ( images , labels ) acc ( labels , logits ) print ( \"accuracy: {0:.1f} %\" . format ( g_correct * 100 / g_total )) So far, we call the job function for evaluation looply and print the accuracy of evaluation result on MNIST testing set.","title":"Process of Evaluation"},{"location":"single_client/quick_start/lenet_mnist.html#image-prediction","text":"After making a few changes to the code above, it will take the data from the raw images rather than existing dataset. Then we can get a model to predict the content from the images. def load_image ( file ): im = Image . open ( file ) . convert ( \"L\" ) im = im . resize (( 28 , 28 ), Image . ANTIALIAS ) im = np . array ( im ) . reshape ( 1 , 1 , 28 , 28 ) . astype ( np . float32 ) im = ( im - 128.0 ) / 255.0 im . reshape (( - 1 , 1 , 1 , im . shape [ 1 ], im . shape [ 2 ])) return im def main (): if len ( sys . argv ) != 2 : usage () return flow . load_variables ( flow . checkpoint . get ( \"./lenet_models_1\" )) image = load_image ( sys . argv [ 1 ]) logits = test_job ( image ) prediction = np . argmax ( logits , 1 ) print ( \"prediction: {} \" . format ( prediction [ 0 ])) if __name__ == \"__main__\" : main ()","title":"Image Prediction"},{"location":"single_client/quick_start/lenet_mnist.html#code","text":"","title":"Code"},{"location":"single_client/quick_start/lenet_mnist.html#model-training","text":"Script: lenet_train.py","title":"Model training"},{"location":"single_client/quick_start/lenet_mnist.html#model-evaluation","text":"Script: lenet_eval.py Saved model: lenet_models_1.zip","title":"Model evaluation"},{"location":"single_client/quick_start/lenet_mnist.html#digits-prediction","text":"Script: lenet_test.py Saved model: lenet_models_1.zip MNIST image dataset mnist_raw_images.zip","title":"Digits prediction"},{"location":"single_client/quick_start/quickstart_in_3_min.html","text":"This article introduces how to quickly get start with OneFlow. We can complete a full neural network training process just in 3 minutes. Example \u00b6 With OneFlow installed, you can run the following command to download mlp_mnist.py python script from repository and run it. wget https://docs.oneflow.org/en/master/code/quick_start/mlp_mnist.py python3 mlp_mnist.py The output looks like below: Epoch [1/20], Loss: 2.3155 Epoch [1/20], Loss: 0.7955 Epoch [1/20], Loss: 0.4653 Epoch [1/20], Loss: 0.2064 Epoch [1/20], Loss: 0.2683 Epoch [1/20], Loss: 0.3167 ... The output is a series of numbers representing the loss values while training. The goal of training is to make the loss value as small as possible. So far, you have completed a full neural network training by using OneFlow. Code Explanation \u00b6 The following is the full code. # mlp_mnist.py import oneflow as flow import oneflow.typing as tp import numpy as np BATCH_SIZE = 100 @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : with flow . scope . placement ( \"cpu\" , \"0:0\" ): reshape = flow . reshape ( images , [ images . shape [ 0 ], - 1 ]) initializer1 = flow . random_uniform_initializer ( - 1 / 28.0 , 1 / 28.0 ) hidden = flow . layers . dense ( reshape , 500 , activation = flow . nn . relu , kernel_initializer = initializer1 , bias_initializer = initializer1 , name = \"dense1\" , ) initializer2 = flow . random_uniform_initializer ( - np . sqrt ( 1 / 500.0 ), np . sqrt ( 1 / 500.0 )) logits = flow . layers . dense ( hidden , 10 , kernel_initializer = initializer2 , bias_initializer = initializer2 , name = \"dense2\" ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , logits ) lr_scheduler = flow . optimizer . PiecewiseConstantScheduler ([], [ 0.001 ]) flow . optimizer . Adam ( lr_scheduler ) . minimize ( loss ) return loss if __name__ == \"__main__\" : ( train_images , train_labels ), ( test_images , test_labels ) = flow . data . load_mnist ( BATCH_SIZE , BATCH_SIZE ) for epoch in range ( 20 ): for i , ( images , labels ) in enumerate ( zip ( train_images , train_labels )): loss = train_job ( images , labels ) if i % 20 == 0 : print ( 'Epoch [ {} / {} ], Loss: {:.4f} ' . format ( epoch + 1 , 20 , loss . mean ())) The next section is a brief description of this code. A special feature of OneFlow compares to other deep learning frameworks is: @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : train_job function which decorated by @flow.global_function is called \"job function\". Unless functions are decorated by @flow.global_function , or they can not be recognized by OneFlow. The parameter type is used to specify the type of job: type=\"train\" means it's a training job and type=\"predict\" means evaluation or prediction job. In OneFlow, a neural network training or prediction task needs two pieces of information: One part is the structure of neural network and its related parameters. These are defined in the job function which mentioned above. The other part is the configuration of training to the network. For example, learning rate and type of model optimizer. These are defined by code as below: lr_scheduler = flow . optimizer . PiecewiseConstantScheduler ([], [ 0.001 ]) flow . optimizer . Adam ( lr_scheduler ) . minimize ( loss ) Besides the job function definition and configuration which mentioned above, code in this script contains all the points of how to train a neural network. flow.data.load_mnist(BATCH_SIZE,BATCH_SIZE) : Prepare and load training data. train_job(images, labels) : return the loss value for each iteration. print(..., loss.mean()) : print loss values for every 20 iterations. This page is just a simple example on neural network. A more comprehensive and detailed introduction of OneFlow can be found in Convolution Neural Network for Handwriting Recognition . In addition, you can refer to Basic topics to learn more about how to use OneFlow for deep learning. Benchmarks and related scripts for some prevalent networks are also provided in repository OneFlow-Benchmark . FAQ \u00b6 Getting stuck when running this script It may be that the incorrect proxy is set in the environment. You can cancel the proxy by first running the command unset http_proxy unset https_proxy unset HTTP_PROXY unset HTTPS_PROXY Then try again My computer can't connect to the Internet and keeps getting stuck when I run the script. This script will automatically download the required data file from the network. If your computer is not connected to the Internet, you will need to download it manually by clicking here and placing it in the script mlp_ mnist.py in the same path and then try again.","title":"Quick Start in 3 Minutes"},{"location":"single_client/quick_start/quickstart_in_3_min.html#example","text":"With OneFlow installed, you can run the following command to download mlp_mnist.py python script from repository and run it. wget https://docs.oneflow.org/en/master/code/quick_start/mlp_mnist.py python3 mlp_mnist.py The output looks like below: Epoch [1/20], Loss: 2.3155 Epoch [1/20], Loss: 0.7955 Epoch [1/20], Loss: 0.4653 Epoch [1/20], Loss: 0.2064 Epoch [1/20], Loss: 0.2683 Epoch [1/20], Loss: 0.3167 ... The output is a series of numbers representing the loss values while training. The goal of training is to make the loss value as small as possible. So far, you have completed a full neural network training by using OneFlow.","title":"Example"},{"location":"single_client/quick_start/quickstart_in_3_min.html#code-explanation","text":"The following is the full code. # mlp_mnist.py import oneflow as flow import oneflow.typing as tp import numpy as np BATCH_SIZE = 100 @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : with flow . scope . placement ( \"cpu\" , \"0:0\" ): reshape = flow . reshape ( images , [ images . shape [ 0 ], - 1 ]) initializer1 = flow . random_uniform_initializer ( - 1 / 28.0 , 1 / 28.0 ) hidden = flow . layers . dense ( reshape , 500 , activation = flow . nn . relu , kernel_initializer = initializer1 , bias_initializer = initializer1 , name = \"dense1\" , ) initializer2 = flow . random_uniform_initializer ( - np . sqrt ( 1 / 500.0 ), np . sqrt ( 1 / 500.0 )) logits = flow . layers . dense ( hidden , 10 , kernel_initializer = initializer2 , bias_initializer = initializer2 , name = \"dense2\" ) loss = flow . nn . sparse_softmax_cross_entropy_with_logits ( labels , logits ) lr_scheduler = flow . optimizer . PiecewiseConstantScheduler ([], [ 0.001 ]) flow . optimizer . Adam ( lr_scheduler ) . minimize ( loss ) return loss if __name__ == \"__main__\" : ( train_images , train_labels ), ( test_images , test_labels ) = flow . data . load_mnist ( BATCH_SIZE , BATCH_SIZE ) for epoch in range ( 20 ): for i , ( images , labels ) in enumerate ( zip ( train_images , train_labels )): loss = train_job ( images , labels ) if i % 20 == 0 : print ( 'Epoch [ {} / {} ], Loss: {:.4f} ' . format ( epoch + 1 , 20 , loss . mean ())) The next section is a brief description of this code. A special feature of OneFlow compares to other deep learning frameworks is: @flow . global_function ( type = \"train\" ) def train_job ( images : tp . Numpy . Placeholder (( BATCH_SIZE , 1 , 28 , 28 ), dtype = flow . float ), labels : tp . Numpy . Placeholder (( BATCH_SIZE ,), dtype = flow . int32 ), ) -> tp . Numpy : train_job function which decorated by @flow.global_function is called \"job function\". Unless functions are decorated by @flow.global_function , or they can not be recognized by OneFlow. The parameter type is used to specify the type of job: type=\"train\" means it's a training job and type=\"predict\" means evaluation or prediction job. In OneFlow, a neural network training or prediction task needs two pieces of information: One part is the structure of neural network and its related parameters. These are defined in the job function which mentioned above. The other part is the configuration of training to the network. For example, learning rate and type of model optimizer. These are defined by code as below: lr_scheduler = flow . optimizer . PiecewiseConstantScheduler ([], [ 0.001 ]) flow . optimizer . Adam ( lr_scheduler ) . minimize ( loss ) Besides the job function definition and configuration which mentioned above, code in this script contains all the points of how to train a neural network. flow.data.load_mnist(BATCH_SIZE,BATCH_SIZE) : Prepare and load training data. train_job(images, labels) : return the loss value for each iteration. print(..., loss.mean()) : print loss values for every 20 iterations. This page is just a simple example on neural network. A more comprehensive and detailed introduction of OneFlow can be found in Convolution Neural Network for Handwriting Recognition . In addition, you can refer to Basic topics to learn more about how to use OneFlow for deep learning. Benchmarks and related scripts for some prevalent networks are also provided in repository OneFlow-Benchmark .","title":"Code Explanation"},{"location":"single_client/quick_start/quickstart_in_3_min.html#faq","text":"Getting stuck when running this script It may be that the incorrect proxy is set in the environment. You can cancel the proxy by first running the command unset http_proxy unset https_proxy unset HTTP_PROXY unset HTTPS_PROXY Then try again My computer can't connect to the Internet and keeps getting stuck when I run the script. This script will automatically download the required data file from the network. If your computer is not connected to the Internet, you will need to download it manually by clicking here and placing it in the script mlp_ mnist.py in the same path and then try again.","title":"FAQ"}]}